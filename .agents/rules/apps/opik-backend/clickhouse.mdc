---
description: ClickHouse SQL query guidelines for opik-backend
globs: apps/opik-backend/**/*
alwaysApply: false
---
# ClickHouse SQL Query Guidelines

Comprehensive guidelines for writing and maintaining ClickHouse SQL queries embedded as Java String literals in the Opik backend.

## Core Principles

### **Schema-First Development**

- **Never invent schema**: Always verify table names, column names, and types from migration files
- **Check migrations first**: Review `src/main/resources/liquibase/db-app-analytics/migrations/` before writing queries
- **Find similar queries**: Search for existing queries that access the same tables before implementing new ones
- **Reuse patterns**: Match established query patterns for consistency and correctness

### **Query Development Workflow**

1. **Find similar query**: Search for existing queries accessing the same tables
2. **Verify schema**: Check migration files for table structure and column types
3. **Implement change**: Write query following established patterns
4. **Update mapping**: Ensure Java row mappers match query output aliases
5. **Test query**: Verify query works with actual data

## Schema Verification

### **Migration File Locations**

- **ClickHouse migrations**: `src/main/resources/liquibase/db-app-analytics/migrations/`
- **Migration naming**: `000001_description.sql`, `000002_description.sql`, etc.
- **Schema source of truth**: Migration files define all tables, columns, and types

### **Common ClickHouse Tables**

**Note**: The schema examples below are for reference only. Always verify the actual schema from migration files in `src/main/resources/liquibase/db-app-analytics/migrations/` as they are the source of truth. Key tables include:

- `traces` - Main trace entities (ReplacingMergeTree)
- `spans` - Span entities within traces (ReplacingMergeTree)
- `feedback_scores` - Feedback scores for entities (ReplacingMergeTree)
- `experiments` - Experiment entities (ReplacingMergeTree)
- `automation_rule_evaluator_logs` - Audit logs (ReplicatedMergeTree - no deduplication needed)
- `alert_logs` - Alert audit logs (ReplicatedMergeTree - no deduplication needed)

**Important enum values to note**:
- `spans.type`: `Enum8('unknown' = 0, 'general' = 1, 'tool' = 2, 'llm' = 3, 'guardrail' = 4)` (migration 000016)
- `feedback_scores.source`: `Enum8('sdk' = 1, 'ui' = 2, 'online_scoring' = 3)` (migration 000017)
- `traces.visibility_mode`: `Enum8('unknown' = 0, 'default' = 1, 'hidden' = 2)` (migration 000027)
- `experiments.type`: `ENUM('regular' = 0, 'trial' = 1, 'mini-batch' = 2)` (migration 000021)

### **Before Writing Queries**

```java
// ✅ Good: Check migrations first
// 1. Review src/main/resources/liquibase/db-app-analytics/migrations/000001_init_script.sql
// 2. Check for schema changes in later migrations (e.g., 000002_change_decimal_precision.sql)
// 3. Verify column types match (Decimal64(9), DateTime64(9, 'UTC'), etc.)

// ❌ Bad: Assume schema without verification
// Don't write queries based on assumptions about table structure
```

## ReplacingMergeTree Deduplication

Most ClickHouse analytics tables use `ReplacingMergeTree` engine, which requires explicit deduplication to get the latest version of each row. 

**Important**: This does **not** apply to audit/log tables that use `ReplicatedMergeTree` (e.g., `automation_rule_evaluator_logs`, `alert_logs`), which are expected to keep full history and should never be deduplicated.

### **Three Deduplication Patterns**

```java
// Pattern 1: ORDER BY + LIMIT 1 BY (most common in Opik, better performance)
private static final String QUERY = """
    SELECT *
    FROM traces
    WHERE workspace_id = :workspace_id
    AND project_id = :project_id
    ORDER BY (workspace_id, project_id, id) DESC, last_updated_at DESC
    LIMIT 1 BY id
    ;
    """;

// Pattern 2: FINAL (simplest syntax, increasingly used, good for straightforward queries)
private static final String QUERY = """
    SELECT *
    FROM traces FINAL
    WHERE workspace_id = :workspace_id
    AND project_id = :project_id
    ;
    """;

// Pattern 3: CTE with deduplication (best for complex queries with multiple table references)
private static final String QUERY = """
    WITH feedback_scores_final AS (
        SELECT *
        FROM feedback_scores
        WHERE entity_type = 'trace'
        AND workspace_id = :workspace_id
        ORDER BY (workspace_id, project_id, entity_id, name) DESC, last_updated_at DESC
        LIMIT 1 BY entity_id, name
    )
    SELECT * FROM feedback_scores_final
    ;
    """;

// ❌ Bad: Missing deduplication - will return duplicate rows
private static final String QUERY = """
    SELECT *
    FROM traces
    WHERE workspace_id = :workspace_id
    ;
    """;
```

### **When to Use Each Pattern**

- **ORDER BY + LIMIT 1 BY**: Most common pattern in Opik, better performance for large tables, use when you need explicit ordering control
- **FINAL**: Simplest syntax, increasingly preferred for straightforward queries, good readability
- **CTE with deduplication**: Use when the deduplicated result is referenced multiple times in the same query (avoids redundant deduplication)

## String Template Engine Patterns

### **Conditional SQL with <if>**

```java
// ✅ Good: Use StringTemplate conditionals for optional filters
private static final String QUERY = """
    SELECT *
    FROM traces FINAL
    WHERE workspace_id = :workspace_id
    <if(project_id)> AND project_id = :project_id <endif>
    <if(uuid_from_time)> AND id >= :uuid_from_time <endif>
    <if(uuid_to_time)> AND id \\<= :uuid_to_time <endif>
    <if(filters)> AND <filters> <endif>
    ;
    """;

// Note: Escape < in comparisons: \\<
```

### **Template Rendering and Binding**

```java
// ✅ Good: Render template, then bind parameters
var template = getSTWithLogComment(QUERY, "queryName", workspaceId, projectId)
        .add("project_id", true)  // Add flags for conditionals
        .add("uuid_from_time", true);

var statement = connection.createStatement(template.render())
        .bind("workspace_id", workspaceId)
        .bind("project_id", projectId)
        .bind("uuid_from_time", uuidFromTime.toString());

// Bind optional parameters conditionally
if (request.uuidToTime() != null) {
    statement.bind("uuid_to_time", request.uuidToTime().toString());
}
```

## Parameter Binding Conventions

### **Parameter Naming**

```java
// ✅ Good: Use snake_case for ClickHouse parameters
.bind("workspace_id", workspaceId)
.bind("project_id", projectId)
.bind("uuid_from_time", uuidFromTime)
.bind("uuid_to_time", uuidToTime)

// ❌ Bad: Don't use camelCase for ClickHouse parameters
.bind("workspaceId", workspaceId)
.bind("projectId", projectId)
```

### **Parameter Types**

```java
// ✅ Good: Bind correct types
statement.bind("workspace_id", workspaceId);          // String
statement.bind("project_id", projectId.toString());   // String (FixedString(36))
statement.bind("limit", limit);                       // int
statement.bind("offset", offset);                     // int
statement.bind("tags", tags.toArray(new String[0]));  // Array

// ✅ Good: Convert UUIDs to strings
statement.bind("uuid_from_time", uuid.toString());

// ❌ Bad: Don't bind null directly, use conditional binding
if (value != null) {
    statement.bind("param", value);
}
```

## ClickHouse-Specific Functions

### **Quantiles for Percentiles**

```java
// ✅ Good: Use quantiles (plural) for multiple percentiles
"""
SELECT
    arrayMap(
        v -> toDecimal64(
            greatest(
                least(if(isFinite(v), v, 0), 999999999.999999999),
                -999999999.999999999
            ),
            9
        ),
        quantiles(0.5, 0.9, 0.99)(duration)
    ) AS duration
FROM traces_filtered
GROUP BY bucket
;
"""

// ❌ Bad: Don't use quantile (singular) for multiple values
quantile(0.5)(duration), quantile(0.9)(duration), quantile(0.99)(duration)
```

### **Handling NaN and Infinity**

```java
// ✅ Good: Always clamp numeric values to prevent NaN/Inf
toDecimal64(
    greatest(
        least(if(isFinite(v), v, 0), 999999999.999999999),
        -999999999.999999999
    ),
    9
)

// Pattern: isFinite check → clamp to max → clamp to min → convert to Decimal
```

### **Decimal Precision**

```java
// ✅ Good: Use correct Decimal types from schema
Decimal64(9)    // For feedback scores (from 000002_change_decimal_precision.sql)
Decimal(38, 12) // For total_estimated_cost (from 000017_change_tables_to_replicated.sql)

// ✅ Good: Convert aggregations to Decimal
toDecimal64(avg(value), 9)
toDecimal128(sum(total_estimated_cost), 12)  // Use Decimal128 for aggregations

// ❌ Bad: Don't use incorrect precision
toDecimal64(total_estimated_cost, 9)  // Should be Decimal(38, 12)
```

### **Array and Map Operations**

```java
// ✅ Good: ARRAY JOIN for expanding arrays
"""
SELECT span_time, name, value
FROM spans_filtered s
ARRAY JOIN mapKeys(usage) AS name, mapValues(usage) AS value
WHERE value > 0
;
"""

// ✅ Good: Map aggregation functions
sumMap(usage)           // Sum maps by key
avgMap(usage)           // Average maps by key
mapFromArrays(keys, values)  // Create map from arrays

// ✅ Good: Array functions
arrayConcat([prompt_id], mapKeys(prompt_versions))
groupArray(value)
arrayMap(x -> x * 2, values)
```

### **Date/Time Functions**

```java
// ✅ Good: Use ClickHouse date functions
UUIDv7ToDateTime(toUUID(id))  // Convert UUIDv7 to DateTime
toStartOfInterval(time, toIntervalDay(1))  // Bucket by day
dateDiff('microsecond', start_time, end_time)  // Time difference
parseDateTime64BestEffort(:timestamp, 9)  // Parse timestamp

// ✅ Good: Time interval constants
toIntervalWeek(1)
toIntervalDay(1)
toIntervalHour(1)
```

### **Duration Calculation Pattern**

```java
// ✅ Good: Standard duration calculation with null checks
"""
if(end_time IS NOT NULL AND start_time IS NOT NULL
    AND notEquals(start_time, toDateTime64('1970-01-01 00:00:00.000', 9)),
    (dateDiff('microsecond', start_time, end_time) / 1000.0),
    NULL) AS duration
"""

// Pattern: Check both times exist → Check start_time is not epoch → Calculate diff in microseconds → Convert to milliseconds
```

## Common Query Patterns

### **WITH Clause (CTEs)**

```java
// ✅ Good: Use CTEs for complex queries
private static final String QUERY = """
    WITH feedback_scores_combined_raw AS (
        SELECT workspace_id, project_id, entity_id, name, value
        FROM feedback_scores FINAL
        WHERE entity_type = 'trace'
        AND workspace_id = :workspace_id
        UNION ALL
        SELECT workspace_id, project_id, entity_id, name, value
        FROM authored_feedback_scores FINAL
        WHERE entity_type = 'trace'
        AND workspace_id = :workspace_id
    ), feedback_scores_with_ranking AS (
        SELECT *,
            ROW_NUMBER() OVER (
                PARTITION BY workspace_id, project_id, entity_id, name, author
                ORDER BY last_updated_at DESC
            ) as rn
        FROM feedback_scores_combined_raw
    ), feedback_scores_final AS (
        SELECT *
        FROM feedback_scores_with_ranking
        WHERE rn = 1
    )
    SELECT * FROM feedback_scores_final
    ;
    """;
```

### **Aggregation Patterns**

```java
// ✅ Good: Conditional aggregation
if(count() = 1, any(value), toDecimal64(avg(value), 9)) AS value

// ✅ Good: Filtering aggregations
countIf(error_info, error_info != '')
avgIf(total_estimated_cost, total_estimated_cost > 0)
sumIf(total_estimated_cost, total_estimated_cost > 0)

// ✅ Good: Group by with HAVING
GROUP BY entity_id
HAVING <feedback_scores_filters>
```

### **WITH FILL for Time Series**

```java
// ✅ Good: Fill gaps in time series data
"""
SELECT <bucket> AS bucket, count(*) as count
FROM traces_filtered
GROUP BY bucket
ORDER BY bucket
<if(with_fill)>WITH FILL
    FROM <fill_from>
    TO toDateTime(UUIDv7ToDateTime(toUUID(:uuid_to_time)))
    STEP <step><endif>
;
"""
```

## Query Output and Java Mapping

### **Column Aliases**

```java
// ✅ Good: Use clear aliases that match Java field names
"""
SELECT
    t.id as id,
    t.name as name,
    t.workspace_id as workspace_id,
    fs.feedback_scores as feedback_scores,
    ed.duration_values AS duration
FROM traces t
;
"""

// Java mapping:
row.get("id", String.class)
row.get("name", String.class)
row.get("workspace_id", String.class)
row.get("feedback_scores", Map.class)
row.get("duration", List.class)
```

### **Complex Type Mapping**

```java
// ✅ Good: Map complex ClickHouse types to Java
// Map(String, Decimal64) → Map<String, BigDecimal>
row.get("feedback_scores", Map.class)

// Array(Decimal64) → List<BigDecimal>
row.get("duration", List.class)

// Tuple → Custom mapping
Optional.ofNullable(row.get("duration", List.class))
    .map(durations -> Stream.of(
        Entry.builder().name("p50").value(getP(durations, 0)).build(),
        Entry.builder().name("p90").value(getP(durations, 1)).build(),
        Entry.builder().name("p99").value(getP(durations, 2)).build()
    ))
```

## Performance Considerations

### **Query Optimization**

```java
// ✅ Good: Filter early in CTEs
WITH traces_scope AS (
    SELECT id
    FROM traces
    WHERE workspace_id = :workspace_id
    AND project_id = :project_id
    <if(uuid_from_time)> AND id >= :uuid_from_time <endif>
    ORDER BY (workspace_id, project_id, id) DESC, last_updated_at DESC
    LIMIT 1 BY id
)
SELECT * FROM traces_scope;

// ✅ Good: Use IN subqueries for filtering (preferred over JOINs in ClickHouse)
WHERE entity_id IN (SELECT trace_id FROM experiment_items_final)

// ✅ Good: Subqueries generally perform better than JOINs for filtering
// Prefer subqueries as the first option; use JOINs sparingly and only when necessary
WHERE trace_id IN (
    SELECT id FROM traces WHERE project_id = :project_id
)

// ❌ Bad: Don't fetch all data then filter in Java
SELECT * FROM traces;  // Then filter in Java

// ❌ Bad: Avoid unnecessary JOINs when a subquery would suffice
FROM traces t
JOIN experiments e ON t.id = e.trace_id  // Use IN subquery instead
```

### **LIMIT 1 BY Performance**

```java
// ✅ Good: Use LIMIT 1 BY for better performance than FINAL
FROM traces
WHERE workspace_id = :workspace_id
ORDER BY (workspace_id, project_id, id) DESC, last_updated_at DESC
LIMIT 1 BY id

// Note: This is often faster than FINAL for large tables
```

## Error Prevention

### **Common Mistakes**

```java
// ❌ Bad: Incorrect comparison operator escaping
WHERE id <= :uuid_to_time  // Will break in StringTemplate

// ✅ Good: Escape comparison operators
WHERE id \\<= :uuid_to_time

// ❌ Bad: Using wrong Decimal precision
toDecimal64(total_estimated_cost, 9)

// ✅ Good: Match schema precision (Decimal(38, 12) from migration 000017)
toDecimal128(total_estimated_cost, 12)

// ❌ Bad: Not handling NaN/Inf
quantiles(0.5, 0.9, 0.99)(duration)

// ✅ Good: Clamp values
arrayMap(v -> toDecimal64(greatest(least(if(isFinite(v), v, 0), 999999999.999999999), -999999999.999999999), 9), quantiles(0.5, 0.9, 0.99)(duration))

// ❌ Bad: Missing FINAL or deduplication
SELECT * FROM feedback_scores WHERE workspace_id = :workspace_id

// ✅ Good: Use FINAL or LIMIT 1 BY
SELECT * FROM feedback_scores FINAL WHERE workspace_id = :workspace_id
```

## Query Logging

### **Log Comments**

```java
// ✅ Good: Always add log comments for query tracking
"""
SELECT * FROM traces
WHERE workspace_id = :workspace_id
SETTINGS log_comment = '<log_comment>'
;
"""

// Template setup:
var template = getSTWithLogComment(QUERY, "queryName", workspaceId, projectId);
```

## Best Practices Summary

### **Do's:**

- ✅ Always verify schema from migration files
- ✅ Search for similar queries before implementing
- ✅ Use FINAL or LIMIT 1 BY for ReplacingMergeTree tables
- ✅ Escape comparison operators in StringTemplate (\\<, \\>)
- ✅ Use snake_case for parameter names
- ✅ Clamp numeric values to prevent NaN/Inf
- ✅ Use correct Decimal precision (Decimal64(9), Decimal(38, 12))
- ✅ Add log comments to all queries
- ✅ Use CTEs for complex queries
- ✅ Match column aliases to Java field names

### **Don'ts:**

- ❌ Don't invent schema without checking migrations
- ❌ Don't forget FINAL or deduplication patterns
- ❌ Don't use camelCase for ClickHouse parameters
- ❌ Don't ignore NaN/Inf handling for numeric aggregations
- ❌ Don't use wrong Decimal precision
- ❌ Don't forget to escape < and > in StringTemplate
- ❌ Don't bind null directly (use conditional binding)
- ❌ Don't fetch all data and filter in Java

## Key References

- **Migration files**: `src/main/resources/liquibase/db-app-analytics/migrations/`
- **Migration script guidelines**: See `apps/opik-backend/.cursor/rules/db_migration_script.mdc` for creating new migrations
- **Example DAOs**: `ProjectMetricsDAO.java`, `TraceDAO.java`, `ExperimentDAO.java`
- **ClickHouse docs**: [ClickHouse Documentation](https://clickhouse.com/docs)
- **R2DBC ClickHouse**: [R2DBC ClickHouse Driver](https://github.com/ClickHouse/clickhouse-java)
