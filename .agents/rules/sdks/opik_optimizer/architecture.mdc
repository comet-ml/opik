---
description: Core architecture and design patterns for Opik Optimizer SDK
globs: sdks/opik_optimizer/src/opik_optimizer/**/*
alwaysApply: false
---
# Opik Optimizer Architecture

The optimizer SDK layers iterative search loops on top of Opik's evaluation stack. Keep orchestration, evaluation, and presentation concerns separate so optimizers stay composable and easy to extend.

## Key Components

- `BaseOptimizer` (`base_optimizer.py`): shared lifecycle hooks, evaluation orchestration, rate limiting, Opik optimisation tracking, history management (`OptimizationHistoryState`/`OptimizationRound`), and `OptimizationResult` construction.
- `OptimizableAgent` (`agents/optimizable_agent.py`): wraps LiteLLM agents, attaches the active optimizer, throttles `_llm_complete`, and pushes optimisation ids into Opik spans via `opik_context`. Treat this class as the boundary for provider-specific behaviour.
- `core/evaluation.evaluate` (`core/evaluation.py`): funnels evaluations through `opik.evaluation.evaluator`, handling sampling, threading, multi-metric objectives, and optimisation id tagging.
- `core/results` (`core/results.py`): result/history layer containing `OptimizationResult`, `OptimizationHistoryState`, and round/trial records.
- `core/state` (`core/state.py`): optimization runtime state (`OptimizationContext`, `AlgorithmResult`, finish reasons).
- `core/llm_calls` (`core/llm_calls.py`): LiteLLM call helpers, counters, metadata, and throttling integration.
- Support modules (`optimization_config/`, `utils/throttle.py`, `cache_config.py`, `utils/display/`, `utils/logging.py`, `multi_metric_objective.py`, `utils/dataset.py`) provide prompt structures, rate limiting, caching, console output, logging setup, dataset slicing/streaming, and multi-metric aggregation. Compatibility shims (`task_evaluator.py`, `optimization_result.py`, `_llm_calls.py`) re-export from `core/` for legacy imports.
- All optimizers must expose a consistent prompt-optimisation method and return the shared result object.

## Package Layout

Each algorithm sits in its own package under `algorithms/` (`evolutionary_optimizer/`, `few_shot_bayesian_optimizer/`, `gepa_optimizer/`, `hierarchical_reflective_optimizer/`, `meta_prompt_optimizer/`, `parameter_optimizer/`).

**All algorithms MUST follow the standardized structure** (see `code-structure.mdc` for details):
- `__init__.py` - Required: exports optimizer class
- `{algorithm_name}.py` - Required: main optimizer implementation
- `helpers.py` - Required: utility functions
- `types.py` - Required: type definitions
- `prompts.py` - Required: prompt templates
- `reporting.py` - Required: reporting utilities
- `adapter.py` - Optional: only if algorithm needs adapter
- `ops/` - Optional: only if algorithm has discrete operations

## Execution Flow

1. `optimize_prompt` validates inputs with `_validate_optimization_inputs`, seeds counters, and may start an Opik optimisation run (`self.opik_client.create_optimization`) storing the id on `self.current_optimization_id`.
2. Subclasses generate candidate prompts or parameter sets via helper modules (mutation ops, prompt planners, MCP workflows).
3. Candidate scoring flows `evaluate()` -> `evaluate_prompt` -> `core/evaluation.evaluate`. The evaluator enforces sampling, threading, LiteLLM throttling, and attaches optimisation ids using `opik_context.update_current_trace`.
4. Progress reporting runs through `utils/display` while `OptimizationHistoryState` captures round/trial snapshots.
5. The run returns an `OptimizationResult` built by the subclass or helper, including baseline metrics, final scores, metadata, and LiteLLM/tool counters. History is normalized via `OptimizationHistoryState` and exposed on the result.

Keep iteration logic free of side effects beyond the explicit history, reporting, and Opik telemetry work so optimizations remain resumable and testable.

## Boundaries and Dependencies

- Production code must not import from `tests/`.
- Optimizer packages may import from `base_optimizer`, shared utilities, and siblings within the same package. Avoid reaching into unrelated optimizer packages.
- Provider glue belongs in `integrations/` and must guard imports so the default install stays lightweight.
- `datasets/` exposes deterministic sample datasets for demos/tests; omit benchmark-only assets from runtime packages. Pay attention to the dataset name suffixes (`_train`, `_validation`, `_test`, and `_sample`) to ensure the dataset name is unique. Use `utils/dataset.py` helpers for slice/streaming to avoid full materialization.
- BaseOptimizer uses a consistent lifecycle hook set: `pre_optimize`, `post_optimize`, `pre_baseline`, `post_baseline`, `pre_round`, `post_round`, `pre_trial`, `on_trial`, `post_trial`. Optimizers should not call `evaluate_prompt` directly; use `evaluate()` to keep counters, early-stop checks, and history aligned.
- Keep algorithm-specific helpers, prompts, and operators inside the corresponding package. Cross-cutting utilities belong at the root (`optimization_config/`, `utils/`, `metrics/`).
- Tag Opik traces through `opik_context.update_current_trace` so optimisation ids surface in logs and UI.
