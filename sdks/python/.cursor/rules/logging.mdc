---
description: Structured logging patterns and best practices for the Python SDK
globs: sdks/python/src/opik/**/*
alwaysApply: false
---
# Python SDK Logging Guidelines

Comprehensive guidelines for implementing proper logging in the Opik Python SDK.

## Logger Configuration

### Logger Setup and Hierarchy

- **Use structured logging** with proper logger hierarchies
- **Configure logging** with customizable formatters
- **Create module-level loggers** consistently with the variable name `LOGGER`

```python
# ✅ Good: Standard logger declaration (from message_processors.py)
import logging

LOGGER = logging.getLogger(__name__)
```

### Logging Configuration

```python
# ✅ Good: Logger configuration (from _logging.py)
CONSOLE_MSG_FORMAT = "OPIK: %(message)s"
DEBUG_MSG_FORMAT = "%(asctime)s [%(process)d-%(processName)s:%(thread)d] %(relativeCreated)d OPIK %(levelname)s [%(filename)s:%(lineno)d]: %(message)s"
# 1MB, to prevent logger from frequent writing hundreds of megabytes in DEBUG mode
# when batches are big and payloads are heavy (e.g. base64 encoded data)
MAX_MESSAGE_LENGTH = 1024 * 1024

class TruncateFormatter(logging.Formatter):
    def __init__(
        self,
        fmt: str,
        datefmt: Optional[str] = None,
        max_length: int = MAX_MESSAGE_LENGTH,
    ) -> None:
        super().__init__(fmt, datefmt)
        self.max_length = max_length

def setup() -> None:
    opik_root_logger = logging.getLogger("opik")
    opik_root_logger.propagate = False

    config_ = config.OpikConfig()

    console_handler = logging.StreamHandler()
    console_level = config_.console_logging_level
    console_handler.setLevel(console_level)
    message_format = (
        DEBUG_MSG_FORMAT if console_level == "DEBUG" else CONSOLE_MSG_FORMAT
    )
    console_handler.setFormatter(TruncateFormatter(message_format))
    opik_root_logger.addHandler(console_handler)
```



## Log Content and Levels

### Appropriate Log Levels

- **ERROR**: System errors, exceptions, critical failures
- **WARNING**: Business rule violations, recoverable issues
- **INFO**: Important business events, successful operations
- **DEBUG**: Detailed debugging information

```python
# ✅ Good: Proper log levels (from message_processors.py)
def process(self, message: messages.BaseMessage) -> None:
    try:
        handler(message)
    except rest_api_core.ApiError as exception:
        # ERROR - System errors and exceptions
        LOGGER.error(
            logging_messages.FAILED_TO_PROCESS_MESSAGE_IN_BACKGROUND_STREAMER,
            message_type.__name__,
            str(exception),
            extra={"error_tracking_extra": error_tracking_extra},
        )
    except Exception as exception:
        # ERROR - Unexpected errors with full context
        LOGGER.error(
            "Failed to process message, unexpected error: %s",
            exception,
            exc_info=exception
        )

# ✅ Good: INFO level for rate limiting (from queue_consumer.py)
except exceptions.OpikCloudRequestsRateLimited as limit_exception:
    LOGGER.info(
        "Ingestion rate limited, retrying in %s seconds, remaining queue size: %d, details: %s",
        limit_exception.retry_after,
        len(self._message_queue) + 1,  # add 1 to account for the current message
        limit_exception.headers,
    )
```

### Error Logging Patterns

```python
# ✅ Good: Error logging with context (from opik_usage_factory.py)
def try_build_opik_usage_or_log_error(
    provider: Union[str, LLMProvider],
    usage: Dict[str, Any],
    logger: logging.Logger,
    error_message: str,
) -> Optional[opik_usage.OpikUsage]:
    try:
        return build_opik_usage(provider=provider, usage=usage)
    except Exception:
        logger.error(error_message, exc_info=True)
        return None

# ✅ Good: Structured error logging (from message_processors.py)
def process(self, message: messages.BaseMessage) -> None:
    try:
        handler(message)
    except pydantic.ValidationError as validation_error:
        error_tracking_extra = _generate_error_tracking_extra(validation_error, message)
        LOGGER.error(
            "Failed to process message: '%s' due to input data validation error:\n%s\n",
            message_type.__name__,
            validation_error,
            exc_info=True,
            extra={"error_tracking_extra": error_tracking_extra},
        )
```

### Performance and Timing Logging

```python
# ✅ Good: Performance logging with timing information
def log_response(
    method: str, url: str, status: int, duration: float, request_id: str
) -> None:
    """Log response with timing information."""
    LOGGER.info(
        "Request %s - %s %s %s (%.2fs)",
        request_id,
        method,
        url,
        status,
        duration
    )

# ✅ Good: Context-rich logging with timing
def process_batch(batch_size: int) -> None:
    start_time = time.time()
    LOGGER.info("Starting batch processing for %d items", batch_size)

    try:
        # Process batch
        process_items(batch_size)

        duration = time.time() - start_time
        LOGGER.info("Batch processing completed in %.2fs", duration)

    except Exception as exception:
        duration = time.time() - start_time
        LOGGER.error(
            "Batch processing failed after %.2fs: %s",
            duration,
            exception,
            exc_info=True
        )
        raise
```

## Sensitive Information Guidelines

### Never Log Sensitive Information

**❌ Never log:**
- API keys, tokens, credentials
- User data, email addresses, names
- Personal identifiers
- Database credentials
- Internal configuration secrets

**✅ Safe to log:**
- Request/response IDs
- HTTP status codes
- Error types and messages
- Timing and performance data
- Non-sensitive metadata

```python
# ✅ Good: Safe logging with IDs and status codes
LOGGER.info(
    "Ingestion rate limited, retrying in %s seconds, remaining queue size: %d, details: %s",
    limit_exception.retry_after,
    len(self._message_queue) + 1,
    limit_exception.headers,  # Safe: contains rate limit headers, not credentials
)

# ✅ Good: Error logging without sensitive data
LOGGER.error(
    "Failed to process message: '%s' due to input data validation error:\n%s\n",
    message_type.__name__,  # Safe: class name
    validation_error,       # Safe: validation error details
    exc_info=True,
)
```

## Logging Configuration

### Configuration Options

```python
# ✅ Good: Configurable logging levels (from config.py)
class OpikConfig(pydantic_settings.BaseSettings):
    console_logging_level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = "INFO"
    """Logging level for console logs."""

    file_logging_level: Optional[Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]] = None
    """Logging level for file logs. Is not configured - nothing is logged to the file."""

    logging_file: str = "opik.log"
    """File to write the logs to."""
```

### Performance Considerations

```python
# ✅ Good: Message truncation for performance (from _logging.py)
class TruncateFormatter(logging.Formatter):
    def __init__(
        self,
        fmt: str,
        datefmt: Optional[str] = None,
        max_length: int = MAX_MESSAGE_LENGTH,
    ) -> None:
        super().__init__(fmt, datefmt)
        self.max_length = max_length

    def format(self, record: logging.LogRecord) -> str:
        result = super().format(record)

        if len(result) > self.max_length:
            # Truncate to prevent huge log messages
            result = result[:self.max_length] + "... [TRUNCATED]"

        return result
```

## Best Practices

### Do's:
- ✅ Use `LOGGER = logging.getLogger(__name__)` pattern
- ✅ Include relevant context in log messages
- ✅ Use appropriate log levels for different types of messages
- ✅ Include timing information for performance monitoring
- ✅ Use structured logging with extra data when needed
- ✅ Log errors with `exc_info=True` for full traceback

### Don'ts:
- ❌ Log sensitive information (API keys, credentials, PII)
- ❌ Use print statements instead of proper logging
- ❌ Log at inappropriate levels (DEBUG for errors, ERROR for normal operations)
- ❌ Create huge log messages without truncation
- ❌ Log in tight loops without rate limiting

## Key References

- [API Design Guidelines](api-design.mdc)
- [Error Handling Guidelines](error-handling.mdc)
- [Code Structure Guidelines](code-structure.mdc)
- [Architecture Guidelines](architecture.mdc)
