---
description: Test fixtures, utilities, and implementation patterns for the Python SDK
globs: sdks/python/tests/**/*
alwaysApply: false
---
# Python SDK Test Implementation Guidelines

Comprehensive guidelines for implementing effective tests using fixtures, utilities, and patterns in the Opik Python SDK.

## Test Fixtures and Utilities

### Fake Backend Testing

- **Use `fake_backend` fixture** to emulate real backend when testing creation of spans, traces and feedback scores in unit tests and library integration tests
- **Use TraceModel, SpanModel, FeedbackScoreModel** and assertion helper functions
- **Extensive examples** in `opik/sdks/python/tests/unit/decorator/test_tracker_outputs.py` and `opik/sdks/python/tests/library_integration/openai/test_openai_responses.py`
- **Use and create verify_* functions** from `opik/sdks/python/tests/e2e/verifiers.py` for repeated assertion logic

### Assertion Helpers for Flexible Testing

The SDK provides specialized assertion helpers for flexible comparisons in tests:

```python
# ✅ Good: Using assertion helpers for flexible assertions
from tests.testlib import ANY, ANY_BUT_NONE, ANY_DICT, ANY_LIST, ANY_STRING

# ANY - Matches anything (including None)
expected_data = {
    "id": ANY,  # Will match any value
    "name": "test",
    "metadata": ANY,  # Will match any value including None
}

# ANY_BUT_NONE - Matches anything except None
span_model = SpanModel(
    id=ANY_BUT_NONE,  # Must exist but we don't care about exact value
    start_time=ANY_BUT_NONE,  # Must be present and not None
    end_time=ANY_BUT_NONE,  # Must be present and not None
    name="test_span"
)

# ANY_DICT - Matches any dictionary
trace_data = {
    "input": ANY_DICT,  # Will match any dict: {}, {"key": "value"}, etc.
    "output": ANY_DICT,
    "metadata": ANY_DICT,
}

# ANY_LIST - Matches any list
expected_result = {
    "tags": ANY_LIST,  # Will match any list: [], ["tag1"], ["tag1", "tag2"], etc.
    "spans": ANY_LIST,
    "feedback_scores": ANY_LIST,
}

# ANY_STRING - Matches any string
user_data = {
    "user_id": ANY_STRING,  # Will match any string value
    "session_id": ANY_STRING,
}

# Conditional matching with ANY_DICT
metadata_check = ANY_DICT.containing({"version": "1.0"})  # Must contain this key-value
input_check = ANY_STRING.starting_with("query:")  # Must start with this prefix

# ✅ Good: Using ANY_DICT.containing() for partial matching
expected_span = SpanModel(
    id=ANY_BUT_NONE,
    name="llm_call",
    # This will match any dict that contains at least these keys
    metadata=ANY_DICT.containing({
        "model": "gpt-4",
        "temperature": 0.7
    }),
    # Input must contain the required query field, but can have additional fields
    input=ANY_DICT.containing({
        "query": "What is AI?"
    }),
    # Output can be any dict structure
    output=ANY_DICT,
    usage=ANY_DICT.containing({
        "prompt_tokens": 10,
        "completion_tokens": 20
    })
)

# This will match spans like:
# metadata: {"model": "gpt-4", "temperature": 0.7, "extra_field": "value"}
# input: {"query": "What is AI?", "context": "...", "user_id": "123"}
# usage: {"prompt_tokens": 10, "completion_tokens": 20, "total_tokens": 30}

# ✅ Good: Real usage example in trace testing
EXPECTED_TRACE_TREE = TraceModel(
    id=ANY_BUT_NONE,  # ID must exist but we don't care about exact UUID
    name="llm_call",
    input=ANY_DICT,  # Input can be any dict structure
    output=ANY_DICT,  # Output can be any dict structure
    start_time=ANY_BUT_NONE,  # Must have a start time
    end_time=ANY_BUT_NONE,  # Must have an end time
    tags=ANY_LIST,  # Tags can be any list (empty or with values)
    spans=[
        SpanModel(
            id=ANY_BUT_NONE,
            name=ANY_STRING,  # Name can be any string
            usage=ANY_DICT,  # Usage data can be any dict structure
            metadata=ANY,  # Metadata can be anything (dict, None, etc.)
        )
    ]
)

# When you need specific values, use exact matches
EXPECTED_SPECIFIC_TRACE = TraceModel(
    id=ANY_BUT_NONE,
    name="specific_function",  # Exact name match required
    input={"query": "test"},  # Exact input structure required
    output={"result": "success"},  # Exact output structure required
    spans=[]  # Exactly empty list required
)
```

### Test Data Generation

```python
# ✅ Good: Using fake_backend for integration tests (from test_tracker_outputs.py)
from tests.testlib import TraceModel, SpanModel, ANY_BUT_NONE, assert_equal
from opik.decorator import tracker

def test_track__one_nested_function__happyflow(fake_backend):
    @tracker.track
    def f_inner(x):
        return "inner-output"

    @tracker.track
    def f_outer(x):
        f_inner("inner-input")
        return "outer-output"

    f_outer("outer-input")
    tracker.flush_tracker()

    EXPECTED_TRACE_TREE = TraceModel(
        id=ANY_BUT_NONE,
        name="f_outer",
        input={"x": "outer-input"},
        output={"output": "outer-output"},
        start_time=ANY_BUT_NONE,
        end_time=ANY_BUT_NONE,
        last_updated_at=ANY_BUT_NONE,
        spans=[
            SpanModel(
                id=ANY_BUT_NONE,
                name="f_outer",
                input={"x": "outer-input"},
                output={"output": "outer-output"},
                start_time=ANY_BUT_NONE,
                end_time=ANY_BUT_NONE,
                spans=[
                    SpanModel(
                        id=ANY_BUT_NONE,
                        name="f_inner",
                        input={"x": "inner-input"},
                        output={"output": "inner-output"},
                        start_time=ANY_BUT_NONE,
                        end_time=ANY_BUT_NONE,
                        spans=[],
                    )
                ],
            )
        ],
    )

    assert len(fake_backend.trace_trees) == 1
    assert_equal(EXPECTED_TRACE_TREE, fake_backend.trace_trees[0])

# ✅ Good: Understanding trace and span relationship for @track
# Note: In most cases, tracked root function calls produce traces and spans
# with the same name, inputs, outputs, etc. This is the standard pattern for
# @track-based integrations and should be reflected in tests.
def test_track__root_function__trace_and_span_have_same_data(fake_backend):
    @tracker.track
    def my_function(input_data):
        return f"processed: {input_data}"

    my_function("test_input")
    tracker.flush_tracker()

    # Both trace and its root span should have the same data
    EXPECTED_TRACE_TREE = TraceModel(
        id=ANY_BUT_NONE,
        name="my_function",  # Same name as the function
        input={"input_data": "test_input"},  # Same input
        output={"output": "processed: test_input"},  # Same output
        start_time=ANY_BUT_NONE,
        end_time=ANY_BUT_NONE,
        last_updated_at=ANY_BUT_NONE,
        spans=[
            SpanModel(
                id=ANY_BUT_NONE,
                name="my_function",  # Same name as trace
                input={"input_data": "test_input"},  # Same input as trace
                output={"output": "processed: test_input"},  # Same output as trace
                start_time=ANY_BUT_NONE,
                end_time=ANY_BUT_NONE,
                spans=[],  # No nested spans
            )
        ],
    )

    assert len(fake_backend.trace_trees) == 1
    assert_equal(EXPECTED_TRACE_TREE, fake_backend.trace_trees[0])

# ❌ Note: Exceptions to the trace/span similarity pattern
# - Callback-based integrations (e.g., LangChain with OpikTracer)
# - Manual context manipulation using opik_context
# - Some complex integrations where trace and span data diverge
# In these cases, test the actual expected behavior rather than assuming similarity
```

### E2E Testing Fixtures

```python
# ✅ Good: E2E test fixtures (from test_dataset.py)
def test_create_and_populate_dataset__happyflow(
    opik_client: opik.Opik, dataset_name: str
):
    # Create dataset
    dataset = opik_client.create_dataset(name=dataset_name)

    # Add items to dataset
    dataset.insert([
        {"input": {"query": "What is AI?"}, "expected_output": {"answer": "Artificial Intelligence"}},
        {"input": {"query": "What is ML?"}, "expected_output": {"answer": "Machine Learning"}},
    ])

    # Verify with domain-specific helper
    verifiers.verify_dataset(
        opik_client=opik_client,
        name=dataset_name,
        dataset_items=[
            dataset_item.DatasetItem(
                input={"query": "What is AI?"},
                expected_output={"answer": "Artificial Intelligence"}
            ),
            dataset_item.DatasetItem(
                input={"query": "What is ML?"},
                expected_output={"answer": "Machine Learning"}
            ),
        ],
    )
```

## Parameterized Testing

### Simple Parameterization

```python
# ✅ Good: Simple parameterized testing (from test_opik_query_language.py)
@pytest.mark.parametrize(
    "filter_string, expected",
    [
        ('name = "test"', [{"field": "name", "operator": "=", "value": "test"}]),
        (
            "usage.total_tokens > 100",
            [{"field": "usage.total_tokens", "operator": ">", "value": "100"}],
        ),
        (
            'tags contains "important"',
            [{"field": "tags", "operator": "contains", "value": "important"}],
        ),
        (
            'feedback_scores."Answer Relevance" < 0.8',
            [
                {
                    "field": "feedback_scores",
                    "key": "Answer Relevance",
                    "operator": "<",
                    "value": "0.8",
                }
            ],
        ),
    ],
)
def test_valid_oql_expressions(filter_string, expected):
    oql = OpikQueryLanguage(filter_string)
    parsed = json.loads(oql.parsed_filters)
    assert len(parsed) == len(expected)

    for i, line in enumerate(expected):
        for key, value in line.items():
            assert parsed[i][key] == value
```

### Complex Parameterization with Custom Parametrizers

```python
# ✅ Good: Complex parameterization (from test_langchain_bedrock.py)
parametrize_chat_model = pytest.mark.parametrize(
    "chat_model",
    [
        langchain_aws.ChatBedrock(
            model_id=BEDROCK_MODEL_FOR_TESTS,
            name=SOME_BEDROCK_CHAT_MODEL_NAME,
            max_tokens=10,
        ),
        langchain_aws.ChatBedrockConverse(
            model_id=BEDROCK_MODEL_FOR_TESTS,
            name=SOME_BEDROCK_CHAT_MODEL_NAME,
            max_tokens=10,
        ),
    ],
    ids=["ChatBedrock", "ChatBedrockConverse"],
)

@parametrize_chat_model
def test_langchain__bedrock_chat_is_used__token_usage_and_provider_is_logged__happyflow(
    fake_backend,
    chat_model,
):
    # Test implementation that works with both chat model types
    pass
```

### Validation Testing with Parameterization

```python
# ✅ Good: Validation testing (from test_feedback_score_validator.py)
@pytest.mark.parametrize(
    argnames="feedback_score_dict, is_valid",
    argvalues=[
        (
            {
                "id": "some-id",
                "name": "toxicity",
                "value": 0.5,
                "reason": "good reason",
                "category_name": "sentiment",
            },
            True,
        ),
        (
            {
                "id": 123213232,  # Invalid: should be string
                "name": "toxicity",
                "value": 0.5,
                "reason": "good reason",
                "category_name": "sentiment",
            },
            False,
        ),
        (
            {
                "id": "some-id",
                "name": "toxicity",
                "value": "0.5",  # Valid: string value accepted
            },
            True,
        ),
        ("not-even-a-dict", False),
    ],
)
def test_feedback_score_validator(feedback_score_dict, is_valid):
    tested = feedback_score.FeedbackScoreValidator(feedback_score_dict)
    assert tested.validate().ok() is is_valid, f"Failed with {feedback_score_dict}"
```

## Test Coverage and Scenarios

### Error Condition Testing

```python
# ✅ Good: Error testing with mocking (from test_sentiment.py)
def test_sentiment_import_error(monkeypatch):
    # Mock the import to simulate missing nltk
    def mock_import(*args, **kwargs):
        raise ImportError("No module named 'vaderSentiment'")

    monkeypatch.setattr("builtins.__import__", mock_import)

    with pytest.raises(MetricComputationError):
        metric = Sentiment()
        metric.score("Some text")
```

### Integration Testing with Different Models

```python
# ✅ Good: Model agnostic testing (from test_evaluation_metrics.py)
model_parametrizer = pytest.mark.parametrize(
    argnames="model",
    argvalues=[
        "gpt-4o",
        langchain_chat_model.LangchainChatModel(
            chat_model=langchain_openai.ChatOpenAI(
                model_name="gpt-4o",
            )
        ),
    ],
)

@model_parametrizer
def test__answer_relevance__context_provided_happyflow(model):
    answer_relevance_metric = metrics.AnswerRelevance(model=model, track=False)

    result = answer_relevance_metric.score(
        input="What's the capital of France?",
        output="The capital of France is Paris.",
        context=["France is a country in Europe."],
    )

    assert_helpers.assert_score_result(result)
```

### Async Testing Patterns

```python
# ✅ Good: Async testing with proper setup
@pytest.mark.asyncio
async def test_async_function__happyflow(fake_backend):
    @tracker.track
    async def async_function(input_data):
        await asyncio.sleep(0.1)  # Simulate async work
        return f"processed: {input_data}"

    result = await async_function("test_input")
    tracker.flush_tracker()

    expected_trace = TraceModel(
        id=ANY_BUT_NONE,
        name="async_function",
        input={"input_data": "test_input"},
        output={"output": "processed: test_input"},
        start_time=ANY_BUT_NONE,
        end_time=ANY_BUT_NONE,
        last_updated_at=ANY_BUT_NONE,
        spans=[ANY_BUT_NONE],  # Span details
    )

    assert len(fake_backend.trace_trees) == 1
    assert_equal(expected_trace, fake_backend.trace_trees[0])
```

## Best Practices

### Do's:
- ✅ Use parameterized tests for multiple scenarios efficiently
- ✅ Test both sync and async code paths where applicable
- ✅ Use appropriate test fixtures for isolation
- ✅ Test error conditions and edge cases thoroughly
- ✅ Use domain-specific assertion helpers
- ✅ Create reusable parametrizers for complex test scenarios
- ✅ Use meaningful test data in parameterized tests

### Don'ts:
- ❌ Write repetitive tests without parameterization
- ❌ Skip testing error scenarios
- ❌ Use magic values without explanation in test data
- ❌ Mix unit tests with integration tests inappropriately
- ❌ Forget to flush tracker in tests that create spans/traces

## Key References

- [Test Best Practices](test-best-practices.mdc)
- [Test Organization](test-organization.mdc)
- [Architecture Guidelines](architecture.mdc)
- [API Design Guidelines](api-design.mdc)
