---
description: Test structure, location, and organization guidelines for the Python SDK
globs: sdks/python/tests/**/*
alwaysApply: true
---
# Python SDK Test Organization Guidelines

Comprehensive guidelines for organizing and structuring tests in the Opik Python SDK.

## Test Structure and Location

### Core Organization Principles

- **Always add tests for new features**
- **Follow the same path principles as existing tests**
- **Use appropriate test type** based on what you're testing
- **Mirror source structure** in unit tests

### Test Directory Structure

The following shows the organizational structure with key examples (not a complete directory listing):

```
sdks/python/tests/
├── unit/                      # Unit tests - mirror src/opik structure
│   ├── api_objects/          # Tests for opik.api_objects
│   ├── decorator/            # Tests for opik.decorator
│   ├── evaluation/           # Tests for opik.evaluation
│   ├── message_processing/   # Tests for opik.message_processing
│   └── validation/           # Tests for opik.validation
├── library_integration/      # Integration tests for external libraries
│   ├── anthropic/           # Anthropic integration tests
│   ├── langchain/           # LangChain integration tests
│   ├── openai/              # OpenAI integration tests
│   └── guardrails/          # Guardrails integration tests
├── e2e/                      # End-to-end tests split by features
│   ├── evaluation/          # Evaluation workflow tests
│   ├── test_dataset.py      # Dataset CRUD operations
│   ├── test_tracing.py      # Tracing functionality
│   └── test_prompt.py       # Prompt management
├── e2e_library_integration/  # End-to-end tests for library integrations
│   ├── adk/                 # ADK integration e2e tests
│   └── litellm/            # LiteLLM integration e2e tests
└── testlib/                  # Test utilities and helpers
    ├── models.py            # TraceModel, SpanModel, etc.
    ├── assert_helpers.py    # assert_equal, ANY helpers
    └── backend_emulator_message_processor.py
```

## Test Types and Organization

### Unit Tests (`tests/unit/`)

- **Mirror package structure**: Replicate the `src/opik` package structure
- **Focus on single units**: Test individual classes and functions in isolation
- **Use mocks**: Mock external dependencies and complex interactions

```python
# ✅ Good: Unit test structure (from tests/unit/decorator/test_tracker_outputs.py)
from opik.decorator import tracker
from tests.testlib import TraceModel, SpanModel, assert_equal

def test_track__captures_function_metadata(fake_backend):
    @tracker.track(
        name="custom_name",
        metadata={"version": "1.0"},
        tags=["test", "unit"]
    )
    def test_function(x):
        return f"processed: {x}"
    
    test_function("input")
    tracker.flush_tracker()
    
    # Test focuses on decorator behavior, not external integrations
    expected_trace = TraceModel(
        name="custom_name",
        metadata={"version": "1.0"},
        tags=["test", "unit"]
    )
    assert_equal(expected_trace, fake_backend.trace_trees[0])
```

### Library Integration Tests (`tests/library_integration/`)

- **Split by integrations**: Each external library gets its own directory
- **Test integration behavior**: Verify that spans, traces, and feedback scores are created correctly
- **Use `fake_backend` fixture**: Emulate real backend without actual API calls
- **Integrations that log traces, spans or feedback scores via `opik.Opik` must be tested here**

```python
# ✅ Good: Library integration structure (from tests/library_integration/openai/)
tests/library_integration/openai/
├── __init__.py
├── requirements.txt          # Integration-specific requirements
├── constants.py             # Test constants and configurations
├── test_openai_chat_completions.py
├── test_openai_responses.py
└── agents_tests/            # Complex integration scenarios
    └── test_opik_tracing_processor.py

# Example test
def test_openai_chat_completions__basic_completion(fake_backend):
    client = openai.OpenAI(api_key="test-key")
    tracked_client = track_openai(client, project_name="test")
    
    response = tracked_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Hello"}]
    )
    
    # Verify integration creates proper traces and spans
    assert len(fake_backend.trace_trees) == 1
    assert_equal(expected_trace_tree, fake_backend.trace_trees[0])
```

### End-to-End Tests (`tests/e2e/`)

- **Split by features**: Group tests by functional areas (dataset, tracing, evaluation)
- **Test complete workflows**: Full API interactions with real backend
- **Use verifiers**: Domain-specific assertion helpers for complex validations

```python
# ✅ Good: E2E test structure
tests/e2e/
├── evaluation/              # Evaluation workflow tests
│   ├── test_experiment_evaluate.py
│   └── test_threads_evaluate.py
├── test_dataset.py         # Dataset CRUD operations
├── test_tracing.py        # Tracing functionality
├── test_prompt.py         # Prompt management
├── test_guardrails.py     # Guardrails functionality
└── verifiers.py           # Domain-specific assertion helpers

# Example test (from test_dataset.py)
def test_create_and_populate_dataset__happyflow(opik_client: opik.Opik, dataset_name: str):
    # Test complete dataset workflow
    dataset = opik_client.create_dataset(name=dataset_name)
    dataset.insert([
        {"input": {"query": "What is AI?"}, "expected_output": {"answer": "AI"}},
    ])
    
    # Use domain-specific verifiers
    verifiers.verify_dataset(
        opik_client=opik_client,
        name=dataset_name,
        dataset_items=[expected_items]
    )
```

### E2E Library Integration Tests (`tests/e2e_library_integration/`)

- **Complex integration scenarios**: End-to-end tests for library integrations
- **Real external services**: Test with actual external APIs when needed
- **Performance and reliability**: Test integration under realistic conditions

```python
# ✅ Good: E2E library integration structure
tests/e2e_library_integration/
├── adk/                    # ADK integration e2e tests
│   ├── requirements.txt
│   ├── sample_agent/       # Sample agent implementations
│   └── test_opik_tracer.py
└── litellm/               # LiteLLM integration e2e tests
    ├── requirements.txt
    └── test_litellm_chat_model.py
```

### Test Utilities (`tests/testlib/`)

- **Shared test infrastructure**: Common utilities used across all test types
- **Domain models**: `TraceModel`, `SpanModel`, `FeedbackScoreModel`
- **Assertion helpers**: `assert_equal`, `ANY`, `ANY_BUT_NONE`, etc.
- **Backend emulation**: `fake_backend` fixture implementation

```python
# ✅ Good: Test utilities organization
tests/testlib/
├── __init__.py                           # Export public utilities
├── models.py                            # TraceModel, SpanModel, etc.
├── assert_helpers.py                    # assert_equal, comparison utilities
├── any_compare_helpers.py               # ANY, ANY_BUT_NONE, etc.
├── backend_emulator_message_processor.py # fake_backend implementation
└── patch_helpers.py                     # Test patching utilities
```

## Test File Naming and Organization

### Naming Conventions

```python
# ✅ Good: Test file naming patterns
test_dataset.py              # Feature-based naming for e2e tests
test_openai_chat_completions.py  # Integration + specific functionality
test_tracker_outputs.py      # Component + specific aspect
test_opik_query_language.py  # Class or module being tested

# ✅ Good: Test function naming
def test_create_dataset__happyflow():                    # Feature + case
def test_openai_chat_completions__basic_completion():    # Integration + scenario
def test_track__captures_function_metadata():           # Method + specific behavior
```

### Configuration Files

```python
# ✅ Good: Test configuration organization
tests/
├── conftest.py              # Global test configuration and fixtures
├── pytest.ini              # Pytest configuration
├── test_requirements.txt   # Test-only dependencies
└── unit/conftest.py        # Unit test specific configuration
└── library_integration/
    ├── conftest.py         # Integration test configuration
    └── openai/requirements.txt  # Integration-specific dependencies
```

## Best Practices

### Do's:
- ✅ **Add tests for new features**
- ✅ **Follow established directory structure**
- ✅ **Use appropriate test type for what you're testing**
- ✅ **Use `fake_backend` for integration tests**
- ✅ **Use verifiers for e2e tests**
- ✅ **Mirror source structure in unit tests**
- ✅ **Group related tests in same directory**

### Don'ts:
- ❌ **Don't mix test types inappropriately**
- ❌ **Don't put unit tests in integration directories**
- ❌ **Don't skip tests for new features**
- ❌ **Don't create new test organization patterns**
- ❌ **Don't duplicate test utilities**

## Key References

- [Test Best Practices](test-best-practices.mdc)
- [Test Implementation](test-implementation.mdc)
- [Architecture Guidelines](architecture.mdc)
- [API Design Guidelines](api-design.mdc)
