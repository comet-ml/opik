---
description: Test structure, location, and organization guidelines for the Python SDK
globs: sdks/python/tests/**/*
alwaysApply: false
---
# Python SDK Test Organization Guidelines

Comprehensive guidelines for organizing and structuring tests in the Opik Python SDK.

## Test Structure and Location

### Core Organization Principles

- **Always add tests for new features**
- **Follow the same path principles as existing tests**
- **Use appropriate test type** based on what you're testing
- **Mirror source structure** in unit tests

### Test Directory Structure

The following shows the organizational structure with key examples (not a complete directory listing):

```
sdks/python/tests/
├── unit/                      # Unit tests - mirror src/opik structure
│   ├── api_objects/          # Tests for opik.api_objects
│   ├── decorator/            # Tests for opik.decorator
│   ├── evaluation/           # Tests for opik.evaluation
│   ├── message_processing/   # Tests for opik.message_processing
│   └── validation/           # Tests for opik.validation
├── library_integration/      # Integration tests for external libraries
│   ├── anthropic/           # Anthropic integration tests
│   ├── langchain/           # LangChain integration tests
│   ├── openai/              # OpenAI integration tests
│   └── guardrails/          # Guardrails integration tests
├── e2e/                      # End-to-end tests split by features
│   ├── evaluation/          # Evaluation workflow tests
│   ├── test_dataset.py      # Dataset CRUD operations
│   ├── test_tracing.py      # Tracing functionality
│   └── test_prompt.py       # Prompt management
├── e2e_library_integration/  # End-to-end tests for library integrations
│   ├── adk/                 # ADK integration e2e tests
│   └── litellm/            # LiteLLM integration e2e tests
└── testlib/                  # Test utilities and helpers
    ├── models.py            # TraceModel, SpanModel, etc.
    ├── assert_helpers.py    # assert_equal, ANY helpers
    └── backend_emulator_message_processor.py
```

## Test Types and Organization

### Unit Tests (`tests/unit/`)

- **Mirror package structure**: Replicate the `src/opik` package structure
- **Focus on single units**: Test individual classes and functions in isolation
- **Use mocks**: Mock external dependencies and complex interactions

```python
# ✅ Good: Unit test structure (from tests/unit/decorator/test_tracker_outputs.py)
from opik.decorator import tracker
from tests.testlib import TraceModel, SpanModel, assert_equal

def test_track__captures_function_metadata(fake_backend):
    @tracker.track(
        name="custom_name",
        metadata={"version": "1.0"},
        tags=["test", "unit"]
    )
    def test_function(x):
        return f"processed: {x}"

    test_function("input")
    tracker.flush_tracker()

    # Test focuses on decorator behavior, not external integrations
    expected_trace = TraceModel(
        name="custom_name",
        metadata={"version": "1.0"},
        tags=["test", "unit"]
    )
    assert_equal(expected_trace, fake_backend.trace_trees[0])
```

### Library Integration Tests (`tests/library_integration/`)

- **Split by integrations**: Each external library gets its own directory
- **Test integration behavior**: Verify that spans, traces, and feedback scores are created correctly
- **Use `fake_backend` fixture**: Emulate real backend without actual API calls
- **Integrations that log traces, spans or feedback scores via `opik.Opik` must be tested here**

```python
# ✅ Good: Library integration structure (from tests/library_integration/openai/)
tests/library_integration/openai/
├── __init__.py
├── requirements.txt          # Integration-specific requirements
├── constants.py             # Test constants and configurations
├── test_openai_chat_completions.py
├── test_openai_responses.py
└── agents_tests/            # Complex integration scenarios
    └── test_opik_tracing_processor.py

# Example test
def test_openai_chat_completions__basic_completion(fake_backend):
    client = openai.OpenAI(api_key="test-key")
    tracked_client = track_openai(client, project_name="test")

    response = tracked_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": "Hello"}]
    )

    # Verify integration creates proper traces and spans
    assert len(fake_backend.trace_trees) == 1
    assert_equal(expected_trace_tree, fake_backend.trace_trees[0])
```

### End-to-End Tests (`tests/e2e/`)

- **Split by features**: Group tests by functional areas (dataset, tracing, evaluation)
- **Test complete workflows**: Full API interactions with real backend
- **Use verifiers**: Domain-specific assertion helpers for complex validations

```python
# ✅ Good: E2E test structure
tests/e2e/
├── evaluation/              # Evaluation workflow tests
│   ├── test_experiment_evaluate.py
│   └── test_threads_evaluate.py
├── test_dataset.py         # Dataset CRUD operations
├── test_tracing.py         # Tracing functionality
├── test_prompt.py         # Prompt management
├── test_guardrails.py     # Guardrails functionality
└── verifiers.py           # Domain-specific assertion helpers

# Example test (from test_dataset.py)
def test_create_and_populate_dataset__happyflow(opik_client: opik.Opik, dataset_name: str):
    # Create dataset
    dataset = opik_client.create_dataset(name=dataset_name)
    dataset.insert([
        {"input": {"query": "What is AI?"}, "expected_output": {"answer": "Artificial Intelligence"}},
        {"input": {"query": "What is ML?"}, "expected_output": {"answer": "Machine Learning"}},
    ])

    # Verify with domain-specific helper
    verifiers.verify_dataset(
        opik_client=opik_client,
        name=dataset_name,
        dataset_items=[
            dataset_item.DatasetItem(
                input={"query": "What is AI?"},
                expected_output={"answer": "Artificial Intelligence"}
            ),
            dataset_item.DatasetItem(
                input={"query": "What is ML?"},
                expected_output={"answer": "Machine Learning"}
            ),
        ],
    )
```

### E2E Library Integration Tests (`tests/e2e_library_integration/`)

- **Complex integration scenarios**: End-to-end tests for library integrations
- **Real external services**: Test with actual external APIs when needed
- **Performance and reliability**: Test integration under realistic conditions

```python
# ✅ Good: E2E library integration structure
tests/e2e_library_integration/
├── adk/                    # ADK integration e2e tests
│   ├── requirements.txt
│   ├── sample_agent/       # Sample agent implementations
│   └── test_opik_tracer.py
└── litellm/               # LiteLLM integration e2e tests
    ├── requirements.txt
    └── test_litellm_chat_model.py
```

### Test Utilities (`tests/testlib/`)

- **Shared test infrastructure**: Common utilities used across all test types
- **Domain models**: `TraceModel`, `SpanModel`, `FeedbackScoreModel`
- **Assertion helpers**: `assert_equal`, `ANY`, `ANY_BUT_NONE`, etc.
- **Backend emulation**: `fake_backend` fixture implementation

```python
# ✅ Good: Test utilities organization
tests/testlib/
├── __init__.py                           # Export public utilities
├── models.py                            # TraceModel, SpanModel, etc.
├── assert_helpers.py                    # assert_equal, comparison utilities
├── any_compare_helpers.py               # ANY, ANY_BUT_NONE, etc.
├── backend_emulator_message_processor.py # fake_backend implementation
└── patch_helpers.py                     # Test patching utilities
```

## Test File Naming and Organization

### Naming Conventions

```python
# ✅ Good: Test file naming patterns
test_dataset.py              # Feature-based naming for e2e tests
test_openai_chat_completions.py  # Integration + specific functionality
test_tracker_outputs.py      # Component + specific aspect
test_opik_query_language.py  # Class or module being tested

# ✅ Good: Test function naming
def test_create_dataset__happyflow():                    # Feature + case
def test_openai_chat_completions__basic_completion():    # Integration + scenario
def test_track__captures_function_metadata():           # Method + specific behavior
```

### Configuration Files

```python
# ✅ Good: Test configuration organization
tests/
├── conftest.py              # Global test configuration and fixtures
├── pytest.ini              # Pytest configuration
├── test_requirements.txt   # Test-only dependencies
└── unit/conftest.py        # Unit test specific configuration
└── library_integration/
    ├── conftest.py         # Integration test configuration
    └── openai/requirements.txt  # Integration-specific dependencies
```

## Writing Tests

### Test Naming Convention

Follow the pattern: `test_WHAT__CASE_DESCRIPTION__EXPECTED_RESULT`

```python
# ✅ Good
def test_optimization_lifecycle__happyflow(opik_client: opik.Opik, dataset_name: str):
def test_tracked_function__error_inside_inner_function__caught_in_top_level_span__inner_span_has_error_info():
def test_insert_and_update_item__dataset_size_should_be_the_same__an_item_with_the_same_id_should_have_new_content():

# ❌ Bad
def test_tracking():
def test_error():
def test_evaluate():
```

### Using Fake Backend

```python
def test_my_feature(fake_backend):
    # 1. Execute code that creates traces/spans
    @opik.track
    def my_function(x):
        return x * 2

    result = my_function(5)
    opik.flush_tracker()  # Always flush!

    # 2. Build expected structure
    EXPECTED_TRACE_TREE = TraceModel(
        id=ANY_BUT_NONE,
        name="my_function",
        input={"x": 5},
        output={"output": 10},
        start_time=ANY_BUT_NONE,
        end_time=ANY_BUT_NONE,
        spans=[
            SpanModel(
                id=ANY_BUT_NONE,
                name="my_function",
                input={"x": 5},
                output={"output": 10},
                start_time=ANY_BUT_NONE,
                end_time=ANY_BUT_NONE,
                spans=[]
            )
        ]
    )

    # 3. Assert
    assert len(fake_backend.trace_trees) == 1
    assert_equal(EXPECTED_TRACE_TREE, fake_backend.trace_trees[0])
```

### Testing with Real Backend

```python
def test_my_e2e_feature(opik_client, temporary_project_name):
    # 1. Create resources
    trace_id = opik_client.trace(
        name="test_trace",
        project_name=temporary_project_name
    )
    opik_client.flush()

    # 2. Verify using verifiers
    verify_trace(
        opik_client,
        trace_id=trace_id,
        name="test_trace",
        project_name=temporary_project_name
    )
```

### Parametrized Tests

```python
@pytest.mark.parametrize(
    "input_value, expected_output",
    [
        (5, 10),
        (10, 20),
        (0, 0),
    ],
)
def test_double_function__various_inputs__correct_outputs(
    fake_backend, input_value, expected_output
):
    @opik.track
    def double(x):
        return x * 2

    result = double(input_value)
    opik.flush_tracker()

    assert len(fake_backend.trace_trees) == 1
    assert fake_backend.trace_trees[0].spans[0].output == {"output": expected_output}
```

### Integration Test Requirements

Each integration should have:
1. `requirements.txt` with integration dependencies
2. `conftest.py` with integration-specific fixtures
3. `constants.py` for test constants (models, etc.)
4. Tests for main integration features

```python
# library_integration/myintegration/requirements.txt
myintegration>=1.0.0

# library_integration/myintegration/conftest.py
import pytest
import os

@pytest.fixture
def ensure_myintegration_configured():
    if not os.getenv("MYINTEGRATION_API_KEY"):
        pytest.skip("MYINTEGRATION_API_KEY not configured")

# library_integration/myintegration/test_myintegration.py
def test_myintegration_basic(fake_backend, ensure_myintegration_configured):
    # Test implementation
```

## Best Practices

### Do's:
- ✅ **Add tests for new features**
- ✅ **Follow established directory structure**
- ✅ **Use appropriate test type for what you're testing**
- ✅ **Use `fake_backend` for integration tests**
- ✅ **Use verifiers for e2e tests**
- ✅ **Mirror source structure in unit tests**
- ✅ **Group related tests in same directory**

### Don'ts:
- ❌ **Don't mix test types inappropriately**
- ❌ **Don't put unit tests in integration directories**
- ❌ **Don't skip tests for new features**
- ❌ **Don't create new test organization patterns**
- ❌ **Don't duplicate test utilities**

## Key References

- [Testing Design Document](TESTING.md)
- [Test Best Practices](test-best-practices.mdc)
- [Test Implementation](test-implementation.mdc)
- [Architecture Guidelines](architecture.mdc)
- [API Design Guidelines](api-design.mdc)
