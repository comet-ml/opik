---
description: Test naming conventions and performance guidelines for the Python SDK
globs: sdks/python/tests/**/*
alwaysApply: false
---
# Python SDK Test Best Practices Guidelines

Comprehensive guidelines for writing and organizing tests in the Opik Python SDK.

## Test Naming and Structure

### Test Naming Conventions

- **Use convention**: `test_WHAT__CASE_DESCRIPTION__EXPECTED_RESULT` or `test_WHAT__happyflow`
- **Make tests easy to read and understand**
- **Before adding a new test**, study existing similar tests nearby and make the new one consistent
- **Always test public API only**, never violate privacy

```python
# ✅ Good: Clear naming patterns (from actual test files)
def test_optimization_lifecycle__happyflow(opik_client: opik.Opik, dataset_name: str):
    """Happy path testing for optimization lifecycle."""
    pass

def test_tracked_function__error_inside_inner_function__caught_in_top_level_span__inner_span_has_error_info():
    """Descriptive test name explaining the scenario and expected outcome."""
    pass

def test_insert_and_update_item__dataset_size_should_be_the_same__an_item_with_the_same_id_should_have_new_content():
    """Complex scenario with clear expected behavior."""
    pass

# ✅ Good: Simple parameterized test naming (from test_sentiment.py)
@pytest.mark.parametrize(
    "text,expected_sentiment",
    [
        ("I love this product! It's amazing.", "positive"),
        ("This is terrible, I hate it.", "negative"),
        ("The sky is blue.", "neutral"),
    ],
)
def test_sentiment_classification(text, expected_sentiment):
    """Simple, focused test with parametrization."""
    pass
```

### Test Structure and Organization

```python
# ✅ Good: Clear test structure (from test_tracing.py)
def test_tracked_function__happyflow(opik_client, project_name):
    # Setup
    ID_STORAGE = {}

    @opik.track(
        tags=["outer-tag1", "outer-tag2"],
        metadata={"outer-metadata-key": "outer-metadata-value"},
        project_name=project_name,
    )
    def f_outer(x):
        ID_STORAGE["f_outer-trace-id"] = opik_context.get_current_trace_data().id
        return "outer-output"

    # Call
    f_outer("outer-input")
    opik.flush_tracker()

    # Verify trace
    verifiers.verify_trace(
        opik_client=opik_client,
        trace_id=ID_STORAGE["f_outer-trace-id"],
        name="f_outer",
        input={"x": "outer-input"},
        output={"output": "outer-output"},
        metadata={"outer-metadata-key": "outer-metadata-value"},
        tags=["outer-tag1", "outer-tag2"],
        project_name=project_name or OPIK_E2E_TESTS_PROJECT_NAME,
    )
```

## Test Performance and Tools

### Performance Guidelines

- **Make tests run as fast as possible**
- **`unittest.mock` or stub/fake objects can be used if needed**
- **`pytest` is used for testing**
- **End-to-end (e2e) tests should not test client-side logic branching**, unless it leads to different endpoints used
- **Always test public API only** - if a test is hard to make without reading protected attributes, that's usually a sign of poor test design
- **Creating read-only properties to access private state should be a last resort** - by default we should never do it

### Test Fixtures and Utilities

```python
# ✅ Good: Using fake_backend fixture (from test_langchain.py)
def test_langchain__happyflow(
    fake_backend,
    project_name,
    expected_project_name,
):
    """Use fake_backend for testing integrations that create traces/spans."""
    pass

# ✅ Good: Using verifiers for repeated assertions (from test_optimization.py)
def test_optimization_lifecycle__happyflow(opik_client: opik.Opik, dataset_name: str):
    optimization = opik_client.create_optimization(
        objective_name="some-objective-name",
        dataset_name=dataset.name,
        name="some-optimization-name",
    )

    verifiers.verify_optimization(
        opik_client=opik_client,
        optimization_id=optimization.id,
        name="some-optimization-name",
        dataset_name=dataset.name,
        status="running",
        objective_name="some-objective-name",
    )
```

### Parameterized Testing

```python
# ✅ Good: Comprehensive parameterized testing (from test_opik_query_language.py)
@pytest.mark.parametrize(
    "filter_string, expected",
    [
        ('name = "test"', [{"field": "name", "operator": "=", "value": "test"}]),
        (
            "usage.total_tokens > 100",
            [{"field": "usage.total_tokens", "operator": ">", "value": "100"}],
        ),
        (
            'tags contains "important"',
            [{"field": "tags", "operator": "contains", "value": "important"}],
        ),
        (
            'feedback_scores."Answer Relevance" < 0.8',
            [
                {
                    "field": "feedback_scores",
                    "key": "Answer Relevance",
                    "operator": "<",
                    "value": "0.8",
                }
            ],
        ),
    ],
)
def test_valid_oql_expressions(filter_string, expected):
    oql = OpikQueryLanguage(filter_string)
    parsed = json.loads(oql.parsed_filters)
    assert len(parsed) == len(expected)

    for i, line in enumerate(expected):
        for key, value in line.items():
            assert parsed[i][key] == value

# ✅ Good: Error case parameterization (from test_opik_query_language.py)
@pytest.mark.parametrize(
    "filter_string, error_pattern",
    [
        ('invalid_field.key = "value"', r"Field invalid_field\.key is not supported.*"),
        ("name = test", r"Invalid value.*"),
        (
            "usage.invalid_metric = 100",
            r"When querying usage, invalid_metric is not supported.*",
        ),
    ],
)
def test_invalid_oql_expressions__should_raise_error(filter_string, error_pattern):
    with pytest.raises(ValueError, match=error_pattern):
        OpikQueryLanguage(filter_string)
```

## Test Coverage and Scenarios

### Testing Both Success and Error Cases

```python
# ✅ Good: Success case testing (from test_sentiment.py)
def test_sentiment_classification(text, expected_sentiment):
    metric = Sentiment()
    result = metric.score(text)

    # Check that the reason contains the expected sentiment category
    assert expected_sentiment in result.reason

    # Verify the compound score is in the correct range
    assert -1.0 <= result.value <= 1.0

# ✅ Good: Error case testing (from test_sentiment.py)
def test_sentiment_import_error(monkeypatch):
    # Mock the import to simulate missing nltk
    def mock_import(*args, **kwargs):
        raise ImportError("No module named 'vaderSentiment'")

    monkeypatch.setattr("builtins.__import__", mock_import)

    with pytest.raises(MetricComputationError):
        metric = Sentiment()
        metric.score("Some text")
```

### Using Domain-Specific Assertion Helpers

#### Fake Backend Testing with Models and assert_equal

```python
# ✅ Good: Using fake_backend with testlib models (from test_tracker_outputs.py)
from tests.testlib import TraceModel, SpanModel, ANY_BUT_NONE, assert_equal
from opik.decorator import tracker

def test_track__one_nested_function__happyflow(fake_backend):
    @tracker.track
    def f_inner(x):
        return "inner-output"

    @tracker.track
    def f_outer(x):
        f_inner("inner-input")
        return "outer-output"

    f_outer("outer-input")
    tracker.flush_tracker()

    EXPECTED_TRACE_TREE = TraceModel(
        id=ANY_BUT_NONE,
        name="f_outer",
        input={"x": "outer-input"},
        output={"output": "outer-output"},
        start_time=ANY_BUT_NONE,
        end_time=ANY_BUT_NONE,
        last_updated_at=ANY_BUT_NONE,
        spans=[
            SpanModel(
                id=ANY_BUT_NONE,
                name="f_outer",
                input={"x": "outer-input"},
                output={"output": "outer-output"},
                start_time=ANY_BUT_NONE,
                end_time=ANY_BUT_NONE,
                spans=[
                    SpanModel(
                        id=ANY_BUT_NONE,
                        name="f_inner",
                        input={"x": "inner-input"},
                        output={"output": "inner-output"},
                        start_time=ANY_BUT_NONE,
                        end_time=ANY_BUT_NONE,
                        spans=[],
                    )
                ],
            )
        ],
    )

    assert len(fake_backend.trace_trees) == 1
    assert_equal(EXPECTED_TRACE_TREE, fake_backend.trace_trees[0])

# ✅ Good: Using testlib utilities for specific scenarios (from test_tracker_outputs.py)
def test_track__inputs_and_outputs_not_captured(fake_backend):
    @tracker.track(capture_output=False, capture_input=False)
    def f(x):
        return "the-output"

    f("the-input")
    tracker.flush_tracker()

    EXPECTED_TRACE_TREE = TraceModel(
        id=ANY_BUT_NONE,
        name="f",
        input=None,  # Input not captured due to capture_input=False
        output=None, # Output not captured due to capture_output=False
        start_time=ANY_BUT_NONE,
        end_time=ANY_BUT_NONE,
        last_updated_at=ANY_BUT_NONE,
        spans=[
            SpanModel(
                id=ANY_BUT_NONE,
                name="f",
                input=None,
                output=None,
                start_time=ANY_BUT_NONE,
                end_time=ANY_BUT_NONE,
                spans=[],
            )
        ],
    )

    assert len(fake_backend.trace_trees) == 1
    assert_equal(EXPECTED_TRACE_TREE, fake_backend.trace_trees[0])
```

#### E2E Testing with Verifiers Module

```python
# ✅ Good: Using verify_* functions for e2e tests that make actual API calls
from tests.e2e import verifiers

def test_trace_creation__e2e__happyflow(opik_client: opik.Opik):
    # Create actual trace via API
    trace = opik_client.trace(
        name="test-trace",
        input={"query": "test"},
        output={"result": "success"}
    )

    # Use domain-specific verifiers for e2e assertions
    # Verifiers use synchronization.until() under the hood for waiting
    verifiers.verify_trace(
        opik_client=opik_client,
        trace_id=trace.id,
        name="test-trace",
        input={"query": "test"},
        output={"result": "success"},
        project_name="Default Project"
    )

# ✅ Good: Using synchronization.until directly when not using verifiers
import tests.e2e.synchronization as synchronization

def test_experiment_creation__manual_waiting(opik_client: opik.Opik):
    experiment = opik_client.create_experiment(name="test-experiment")

    # If not using verifiers, manually use synchronization.until for waiting
    if not synchronization.until(
        lambda: opik_client.get_experiment_by_name("test-experiment") is not None,
        allow_errors=True,
    ):
        raise AssertionError("Experiment was not created within timeout")

    # Then perform manual assertions
    retrieved_experiment = opik_client.get_experiment_by_name("test-experiment")
    assert retrieved_experiment.name == "test-experiment"
```

## Test Types and Organization

### Integration Testing with fake_backend

```python
# ✅ Good: Library integration testing (from guardrails tests)
def test_guardrails__trace_and_span_per_one_validation_check(
    fake_backend, ensure_openai_configured, project_name, expected_project_name
):
    """Test guardrails integration with fake backend for trace/span validation."""
    # Test integrations that log traces, spans or feedback scores via opik.Opik
    # Must be tested via library_integration tests using fake_backend fixture
    pass

# ✅ Good: DSPy integration testing (from test_dspy.py)
def test_dspy__happyflow(
    fake_backend,
    project_name,
    expected_project_name,
):
    """Test DSPy integration patterns."""
    pass
```

### Efficient Test Organization

```python
# ✅ Good: Focused unit tests (from test_feedback_score_validator.py)
def test_feedback_score_validator(feedback_score_dict, is_valid):
    tested = FeedbackScoreValidator(feedback_score_dict)
    assert tested.validate().ok() is is_valid, f"Failed with {feedback_score_dict}"

# ✅ Good: Using testlib utilities
from ..testlib import ANY_STRING

def test_trace_creation():
    verifiers.verify_trace(
        trace_id=ANY_STRING,  # Use testlib utilities for flexible assertions
        name="expected-name",
        # ... other assertions
    )
```

## Best Practices

### Do's:
- ✅ Use descriptive test names following the `test_WHAT__CASE__EXPECTED` pattern
- ✅ Study existing similar tests before adding new ones
- ✅ Use `fake_backend` fixture for integration tests that create traces/spans
- ✅ Use verify_* functions for repeated assertion logic in e2e tests
- ✅ Test both success and error scenarios thoroughly
- ✅ Use parameterized tests for multiple scenarios efficiently
- ✅ Keep tests focused and independent

### Don'ts:
- ❌ Test private/protected methods directly
- ❌ Create tests that depend on external state
- ❌ Write tests without clear naming
- ❌ Skip testing error conditions
- ❌ Use manual assertions when domain-specific helpers exist
- ❌ Test implementation details instead of behavior

## Key References

- [Test Organization Guidelines](test-organization.mdc)
- [Test Implementation Guidelines](test-implementation.mdc)
- [Architecture Guidelines](architecture.mdc)
- [API Design Guidelines](api-design.mdc)
