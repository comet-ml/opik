# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.ollama_connection_test_response import OllamaConnectionTestResponse
from ..types.ollama_model import OllamaModel
from .raw_client import AsyncRawOllamaClient, RawOllamaClient

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class OllamaClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawOllamaClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawOllamaClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawOllamaClient
        """
        return self._raw_client

    def list_models(
        self,
        *,
        base_url: str,
        api_key: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[OllamaModel]:
        """
        Fetches the list of models available from the Ollama instance. URL may be provided with or without /v1 suffix (e.g., http://localhost:11434 or http://localhost:11434/v1). The /v1 suffix will be automatically removed for model discovery. For actual LLM inference, use the URL with /v1 suffix for OpenAI-compatible endpoints.

        Parameters
        ----------
        base_url : str
            Base URL of the Ollama instance. May include /v1 suffix which will be automatically removed for connection testing. For inference, use the URL with /v1 suffix for OpenAI-compatible endpoints.

        api_key : typing.Optional[str]
            Optional API key for authenticated Ollama instances. If provided, will be sent as Bearer token in Authorization header.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[OllamaModel]
            Models retrieved successfully

        Examples
        --------
        from Opik import OpikApi
        client = OpikApi(api_key="YOUR_API_KEY", workspace_name="YOUR_WORKSPACE_NAME", )
        client.ollama.list_models(base_url='http://localhost:11434/v1', )
        """
        _response = self._raw_client.list_models(base_url=base_url, api_key=api_key, request_options=request_options)
        return _response.data

    def test_connection(
        self,
        *,
        base_url: str,
        api_key: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> OllamaConnectionTestResponse:
        """
        Validates that the provided Ollama URL is reachable. URL may be provided with or without /v1 suffix (e.g., http://localhost:11434 or http://localhost:11434/v1). The /v1 suffix will be automatically removed for connection testing. For inference, use the URL with /v1 suffix.

        Parameters
        ----------
        base_url : str
            Base URL of the Ollama instance. May include /v1 suffix which will be automatically removed for connection testing. For inference, use the URL with /v1 suffix for OpenAI-compatible endpoints.

        api_key : typing.Optional[str]
            Optional API key for authenticated Ollama instances. If provided, will be sent as Bearer token in Authorization header.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        OllamaConnectionTestResponse
            Connection test successful

        Examples
        --------
        from Opik import OpikApi
        client = OpikApi(api_key="YOUR_API_KEY", workspace_name="YOUR_WORKSPACE_NAME", )
        client.ollama.test_connection(base_url='http://localhost:11434/v1', )
        """
        _response = self._raw_client.test_connection(
            base_url=base_url, api_key=api_key, request_options=request_options
        )
        return _response.data


class AsyncOllamaClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawOllamaClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawOllamaClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawOllamaClient
        """
        return self._raw_client

    async def list_models(
        self,
        *,
        base_url: str,
        api_key: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[OllamaModel]:
        """
        Fetches the list of models available from the Ollama instance. URL may be provided with or without /v1 suffix (e.g., http://localhost:11434 or http://localhost:11434/v1). The /v1 suffix will be automatically removed for model discovery. For actual LLM inference, use the URL with /v1 suffix for OpenAI-compatible endpoints.

        Parameters
        ----------
        base_url : str
            Base URL of the Ollama instance. May include /v1 suffix which will be automatically removed for connection testing. For inference, use the URL with /v1 suffix for OpenAI-compatible endpoints.

        api_key : typing.Optional[str]
            Optional API key for authenticated Ollama instances. If provided, will be sent as Bearer token in Authorization header.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[OllamaModel]
            Models retrieved successfully

        Examples
        --------
        from Opik import AsyncOpikApi
        import asyncio
        client = AsyncOpikApi(api_key="YOUR_API_KEY", workspace_name="YOUR_WORKSPACE_NAME", )
        async def main() -> None:
            await client.ollama.list_models(base_url='http://localhost:11434/v1', )
        asyncio.run(main())
        """
        _response = await self._raw_client.list_models(
            base_url=base_url, api_key=api_key, request_options=request_options
        )
        return _response.data

    async def test_connection(
        self,
        *,
        base_url: str,
        api_key: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> OllamaConnectionTestResponse:
        """
        Validates that the provided Ollama URL is reachable. URL may be provided with or without /v1 suffix (e.g., http://localhost:11434 or http://localhost:11434/v1). The /v1 suffix will be automatically removed for connection testing. For inference, use the URL with /v1 suffix.

        Parameters
        ----------
        base_url : str
            Base URL of the Ollama instance. May include /v1 suffix which will be automatically removed for connection testing. For inference, use the URL with /v1 suffix for OpenAI-compatible endpoints.

        api_key : typing.Optional[str]
            Optional API key for authenticated Ollama instances. If provided, will be sent as Bearer token in Authorization header.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        OllamaConnectionTestResponse
            Connection test successful

        Examples
        --------
        from Opik import AsyncOpikApi
        import asyncio
        client = AsyncOpikApi(api_key="YOUR_API_KEY", workspace_name="YOUR_WORKSPACE_NAME", )
        async def main() -> None:
            await client.ollama.test_connection(base_url='http://localhost:11434/v1', )
        asyncio.run(main())
        """
        _response = await self._raw_client.test_connection(
            base_url=base_url, api_key=api_key, request_options=request_options
        )
        return _response.data
