---
description: Core architecture and design patterns for Opik Optimizer SDK
globs: sdks/opik_optimizer/src/opik_optimizer/**/*
alwaysApply: true
---
# Opik Optimizer Architecture

The optimizer SDK layers iterative search loops on top of Opik's evaluation stack. Keep orchestration, evaluation, and presentation concerns separate so optimizers stay composable and easy to extend.

## Key Components

- `BaseOptimizer` (`base_optimizer.py`): shared lifecycle hooks, LiteLLM cache bootstrap, rate limiting, Opik optimisation tracking, history management (`OptimizationRound`), and `OptimizationResult` construction.
- `OptimizableAgent` (`optimizable_agent.py`): wraps LiteLLM agents, attaches the active optimizer, throttles `_llm_complete`, and pushes optimisation ids into Opik spans via `opik_context`. Treat this class as the boundary for provider-specific behaviour.
- `task_evaluator.evaluate` (`task_evaluator.py`): funnels evaluations through `opik.evaluation.evaluator`, handling sampling, threading, multi-metric objectives, and optimisation id tagging.
- `OptimizationResult` (`optimization_result.py`): normalised result object containing final prompt, score, metadata, baseline metrics, LiteLLM/tool counters, and helpers for chaining optimizers.
- Support modules (`optimization_config/`, `_throttle.py`, `cache_config.py`, `reporting_utils.py`, `logging_config.py`, `multi_metric_objective.py`) provide prompt structures, rate limiting, caching, console output, logging setup, and multi-metric aggregation.

## Package Layout

Each algorithm sits in its own package (`evolutionary_optimizer/`, `few_shot_bayesian_optimizer/`, `gepa_optimizer/`, `hierarchical_reflective_optimizer/`, `meta_prompt_optimizer/`, `mipro_optimizer/`, `parameter_optimizer/`). Keep algorithm-specific helpers, prompts, and operators inside the corresponding package. Cross-cutting utilities belong at the root (`optimization_config/`, `utils/`, `mcp_utils/`, `metrics/`). Production code must never import from `tests/`.

## Execution Flow

1. `optimize_prompt` (or `optimize_mcp`) validates inputs with `_validate_optimization_inputs`, seeds counters, and may start an Opik optimisation run (`self.opik_client.create_optimization`) storing the id on `self.current_optimization_id`.
2. Subclasses generate candidate prompts or parameter sets via helper modules (mutation ops, prompt planners, MCP workflows).
3. Candidate scoring flows `_evaluate_prompt` -> `evaluate_prompt` -> `task_evaluator.evaluate`. The evaluator enforces sampling, threading, LiteLLM throttling, and attaches optimisation ids using `opik_context.update_current_trace`.
4. Progress reporting runs through `reporting_utils` context managers while `_add_to_history` captures `OptimizationRound` snapshots.
5. The run returns an `OptimizationResult` built by the subclass or helper, including baseline metrics, final scores, metadata, and LiteLLM/tool counters.

Keep iteration logic free of side effects beyond the explicit history, reporting, and Opik telemetry work so optimizations remain resumable and testable.

## API Contract

All optimizers must expose a consistent prompt-optimisation method and return the shared result object:

```python
from collections.abc import Callable
from opik import Dataset
from opik_optimizer.optimization_config import chat_prompt
from opik_optimizer import optimization_result
from opik_optimizer.optimizable_agent import OptimizableAgent

class BaseOptimizer:
    def optimize_prompt(
        self,
        prompt: chat_prompt.ChatPrompt,
        dataset: Dataset,
        metric: Callable[[dict[str, object], str], object],
        experiment_config: dict[str, object] | None = None,
        n_samples: int | None = None,
        auto_continue: bool = False,
        agent_class: type[OptimizableAgent] | None = None,
        project_name: str = "Optimization",
        **kwargs: object,
    ) -> optimization_result.OptimizationResult: ...
```

`OptimizationResult` provides fields such as `optimizer`, `prompt`, `score`, `metric_name`, `optimization_id`, `initial_prompt`, `initial_score`, `details`, `history`, `llm_calls`, and `tool_calls`. Interface guarantees are exercised in `tests/unit/test_optimization_result.py` and the e2e suite under `tests/e2e/optimizers/`, which instantiate multiple optimizers to confirm they produce compatible signatures and result structures.

## Boundaries and Dependencies

- Production code must not import from `tests/`.
- Optimizer packages may import from `base_optimizer`, shared utilities, and siblings within the same package. Avoid reaching into unrelated optimizer packages.
- Provider glue belongs in `integrations/` and must guard imports so the default install stays lightweight.
- `datasets/` exposes deterministic sample datasets for demos/tests; omit benchmark-only assets from runtime packages.

## Caching, Throttling, and Telemetry

- Use `_throttle.get_rate_limiter_for_current_opik_installation` and decorators such as `@rate_limited` for outbound LiteLLM calls instead of ad-hoc sleep loops.
- `cache_config.initialize_cache()` configures a shared LiteLLM disk cache under `~/.litellm_cache`. If an optimizer needs different behaviour, add configuration knobs rather than mutating global state.
- Tag Opik traces through `opik_context.update_current_trace` so optimisation ids surface in logs and UI.

## Adding a New Optimizer

1. Create a new package under `src/opik_optimizer/` with an `__init__.py` that re-exports the public class.
2. Subclass `BaseOptimizer` and implement `optimize_prompt` (and optionally `optimize_mcp`). Extract helper modules for mutation, selection, or reporting to keep the main loop readable.
3. Reuse `reporting_utils` context managers for console output and populate `OptimizationResult` with baseline, final metrics, metadata, and counters.
4. Document new configuration knobs in `optimization_config/` and update README/docs/notebooks.
5. Add unit coverage for helper modules plus a lightweight e2e test under `tests/e2e/optimizers/` running on a tiny dataset to prove the interface contract.
