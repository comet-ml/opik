{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTaMBjChz7m5"
   },
   "source": [
    "# Optimzing using Synthetic Q&A Data from Opik Traces\n",
    "\n",
    "You will need:\n",
    "\n",
    "1. A Comet account, for seeing Opik visualizations (free!) - [comet.com](https://comet.com)\n",
    "2. An OpenAI account, for using an LLM\n",
    "[platform.openai.com/settings/organization/api-keys](https://platform.openai.com/settings/organization/api-keys)\n",
    "\n",
    "This example will use:\n",
    "\n",
    "- [tinyqabenchmarkpp](https://pypi.org/project/tinyqabenchmarkpp/) to generate synthetic test dataset\n",
    "- [opik-optimizer](https://pypi.org/project/opik-optimizer/) to optimize prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM1lU0dBBnJs"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This pip-install takes about a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2Tx6HwuU1rB4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opik-optimizer in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (0.8.1)\n",
      "Collecting tinyqabenchmarkpp\n",
      "  Downloading tinyqabenchmarkpp-1.2.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: opik>=1.7.17 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (1.7.31)\n",
      "Requirement already satisfied: dspy<3,>=2.6.18 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (2.6.21)\n",
      "Requirement already satisfied: litellm in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (1.67.2)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (4.67.1)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (2.21.0)\n",
      "Requirement already satisfied: optuna in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (4.1.0)\n",
      "Requirement already satisfied: pydantic in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (2.11.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (2.2.3)\n",
      "Requirement already satisfied: hf_xet in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (1.1.0)\n",
      "Requirement already satisfied: pyrate-limiter in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (3.7.0)\n",
      "Requirement already satisfied: deap>=1.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik-optimizer) (1.4.3)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from deap>=1.4.3->opik-optimizer) (1.26.4)\n",
      "Requirement already satisfied: backoff>=2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (2.2.1)\n",
      "Requirement already satisfied: joblib~=1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (1.4.2)\n",
      "Requirement already satisfied: openai>=0.28.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (1.76.2)\n",
      "Requirement already satisfied: regex>=2023.10.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (2024.11.6)\n",
      "Requirement already satisfied: ujson>=5.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (5.10.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (2.32.3)\n",
      "Requirement already satisfied: magicattr>=0.1.6 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (0.1.6)\n",
      "Requirement already satisfied: diskcache>=5.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (5.6.3)\n",
      "Requirement already satisfied: json-repair>=0.30.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (0.35.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (9.1.2)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (4.9.0)\n",
      "Requirement already satisfied: asyncer==0.0.8 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (0.0.8)\n",
      "Requirement already satisfied: cachetools>=5.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (5.5.0)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (3.1.0)\n",
      "Requirement already satisfied: rich>=13.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from dspy<3,>=2.6.18->opik-optimizer) (13.9.4)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->opik-optimizer) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (3.11.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (0.31.4)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from datasets->opik-optimizer) (6.0.2)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (8.1.8)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (8.4.0)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (4.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (0.8.0)\n",
      "Requirement already satisfied: tokenizers in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from litellm->opik-optimizer) (0.19.1)\n",
      "Requirement already satisfied: boto3-stubs>=1.34.110 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from boto3-stubs[bedrock-runtime]>=1.34.110->opik>=1.7.17->opik-optimizer) (1.36.20)\n",
      "Requirement already satisfied: levenshtein<1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik>=1.7.17->opik-optimizer) (0.26.1)\n",
      "Requirement already satisfied: pydantic-settings!=2.9.0,<3.0.0,>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik>=1.7.17->opik-optimizer) (2.7.0)\n",
      "Requirement already satisfied: pytest in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik>=1.7.17->opik-optimizer) (8.3.5)\n",
      "Requirement already satisfied: sentry_sdk>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik>=1.7.17->opik-optimizer) (2.14.0)\n",
      "Requirement already satisfied: uuid6 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from opik>=1.7.17->opik-optimizer) (2024.7.10)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from optuna->opik-optimizer) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from optuna->opik-optimizer) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from optuna->opik-optimizer) (2.0.32)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pandas->opik-optimizer) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pandas->opik-optimizer) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pandas->opik-optimizer) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pydantic->opik-optimizer) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pydantic->opik-optimizer) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pydantic->opik-optimizer) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pydantic->opik-optimizer) (0.4.0)\n",
      "Requirement already satisfied: Mako in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->opik-optimizer) (1.3.6)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from anyio->dspy<3,>=2.6.18->opik-optimizer) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from anyio->dspy<3,>=2.6.18->opik-optimizer) (1.3.1)\n",
      "Requirement already satisfied: botocore-stubs in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik>=1.7.17->opik-optimizer) (1.36.20)\n",
      "Requirement already satisfied: types-s3transfer in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik>=1.7.17->opik-optimizer) (0.11.2)\n",
      "Requirement already satisfied: mypy-boto3-bedrock-runtime<1.37.0,>=1.36.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from boto3-stubs[bedrock-runtime]>=1.34.110->opik>=1.7.17->opik-optimizer) (1.36.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from aiohttp->datasets->opik-optimizer) (1.18.3)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from httpx>=0.23.0->litellm->opik-optimizer) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from httpx>=0.23.0->litellm->opik-optimizer) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm->opik-optimizer) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm->opik-optimizer) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm->opik-optimizer) (3.0.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik-optimizer) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik-optimizer) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm->opik-optimizer) (0.22.3)\n",
      "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from levenshtein<1.0.0->opik>=1.7.17->opik-optimizer) (3.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai>=0.28.1->dspy<3,>=2.6.18->opik-optimizer) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai>=0.28.1->dspy<3,>=2.6.18->opik-optimizer) (0.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->opik-optimizer) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from requests>=2.31.0->dspy<3,>=2.6.18->opik-optimizer) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from requests>=2.31.0->dspy<3,>=2.6.18->opik-optimizer) (2.0.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from rich>=13.7.1->dspy<3,>=2.6.18->opik-optimizer) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from rich>=13.7.1->dspy<3,>=2.6.18->opik-optimizer) (2.18.0)\n",
      "Requirement already satisfied: iniconfig in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pytest->opik>=1.7.17->opik-optimizer) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pytest->opik>=1.7.17->opik-optimizer) (1.5.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy<3,>=2.6.18->opik-optimizer) (0.1.2)\n",
      "Requirement already satisfied: types-awscrt in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from botocore-stubs->boto3-stubs>=1.34.110->boto3-stubs[bedrock-runtime]>=1.34.110->opik>=1.7.17->opik-optimizer) (0.23.10)\n",
      "Downloading tinyqabenchmarkpp-1.2.3-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: tinyqabenchmarkpp\n",
      "Successfully installed tinyqabenchmarkpp-1.2.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install opik-optimizer tinyqabenchmarkpp --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step configures the Opik library for your session. It will prompt for your Comet API key if not already set in your environment or through Opik's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H0DNm-un_0Np"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /Users/jacquesverre/.opik.config\n"
     ]
    }
   ],
   "source": [
    "import opik\n",
    "\n",
    "opik.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we'll use OpenAI models, so we need to set our OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vN72mHQy_7Ou"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching Traces from Opik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up we will \"fetch\" existing traces of our AI application within Opik (we will use the demo project that ships with every Opik installation). \n",
    "\n",
    "This fetches traces from the demo project and formats them as a context string. Returns a formatted string with explanatory text and cleaned traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Opik is already configured. You can check the settings by viewing the config file at /Users/jacquesverre/.opik.config\n"
     ]
    }
   ],
   "source": [
    "OPIK_PROJECT_NAME = \"Demo chatbot ðŸ¤–\"\n",
    "\n",
    "# Will prompt for API key if not set\n",
    "opik.configure()\n",
    "\n",
    "# Fetch traces from the demo project\n",
    "#\n",
    "# Commented out the project name to\n",
    "# fetch all traces across all projects\n",
    "client = opik.Opik()\n",
    "traces = client.search_traces(\n",
    "    # project_name=OPIK_PROJECT_NAME,\n",
    "    max_results=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40 traces\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(traces)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define some helper functions to clean and traverse the traces as we don't wan to send noise to the LLM and break the input to the synthetic data generation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_text_from_dict(d: dict) -> list[str]:\n",
    "    \"\"\"\n",
    "    Recursively extracts text from a dictionary.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, str):\n",
    "            texts.append(value)\n",
    "        elif isinstance(value, dict):\n",
    "            texts.extend(extract_text_from_dict(value))\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, str):\n",
    "                    texts.append(item)\n",
    "                elif isinstance(item, dict):\n",
    "                    texts.extend(extract_text_from_dict(item))\n",
    "    return texts\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans text by removing special characters and normalizing whitespace.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    # Replace special characters with spaces, but keep basic punctuation\n",
    "    text = re.sub(r'[^\\w\\s.,!?;:\\'\"-]', \" \", text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    # Remove any leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to extract and clean the text from the traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and clean text from traces\n",
    "cleaned_texts = []\n",
    "for i, trace in enumerate(traces):\n",
    "    # Extract from input\n",
    "    if trace.input:\n",
    "        if isinstance(trace.input, dict):\n",
    "            texts = extract_text_from_dict(trace.input)\n",
    "            for text in texts:\n",
    "                cleaned = clean_text(text)\n",
    "                if cleaned:\n",
    "                    cleaned_texts.append(cleaned)\n",
    "        elif isinstance(trace.input, str):\n",
    "            cleaned = clean_text(trace.input)\n",
    "            if cleaned:\n",
    "                cleaned_texts.append(cleaned)\n",
    "\n",
    "    # Extract from output\n",
    "    if trace.output:\n",
    "        if isinstance(trace.output, dict):\n",
    "            texts = extract_text_from_dict(trace.output)\n",
    "            for text in texts:\n",
    "                cleaned = clean_text(text)\n",
    "                if cleaned:\n",
    "                    cleaned_texts.append(cleaned)\n",
    "        elif isinstance(trace.output, str):\n",
    "            cleaned = clean_text(trace.output)\n",
    "            if cleaned:\n",
    "                cleaned_texts.append(cleaned)\n",
    "\n",
    "    # Extract from metadata if it exists\n",
    "    if trace.metadata:\n",
    "        if isinstance(trace.metadata, dict):\n",
    "            texts = extract_text_from_dict(trace.metadata)\n",
    "            for text in texts:\n",
    "                cleaned = clean_text(text)\n",
    "                if cleaned:\n",
    "                    cleaned_texts.append(cleaned)\n",
    "\n",
    "if not cleaned_texts:\n",
    "    print(\"Debug: No text content found in traces. Here's what we got:\")\n",
    "    for i, trace in enumerate(traces[:5]):  # Show first 5 traces for debugging\n",
    "        print(f\"\\nTrace {i}:\")\n",
    "        print(f\"Input: {trace.input}\")\n",
    "        print(f\"Output: {trace.output}\")\n",
    "        print(f\"Metadata: {trace.metadata}\")\n",
    "    raise ValueError(\"No valid text content found in traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly inspect the traces we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TracePublic(id='06841c27-6d6d-796a-8000-8da96e8a23e9', project_id='0194b879-5a22-7182-9327-1c34f833133d', name='chat.completion', start_time=datetime.datetime(2025, 6, 5, 17, 14, 46, 838183, tzinfo=TzInfo(UTC)), end_time=datetime.datetime(2025, 6, 5, 17, 14, 46, 838712, tzinfo=TzInfo(UTC)), input=[{'role': 'system', 'content': '\\nYou are a prompt editor that modifies a message list to support few-shot learning. Your job is to insert a placeholder where few-shot examples can be inserted and generate a reusable string template for formatting those examples.\\n\\nYou will receive a JSON object with the following fields:\\n\\n- \"message_list\": a list of messages, each with a role (system, user, or assistant) and a content field.\\n- \"examples\": a list of example pairs, each with input and output fields.\\n\\nYour task:\\n\\n- Insert the string \"FEW_SHOT_EXAMPLE_PLACEHOLDER\" into one of the messages in the list. Make sure to:\\n    - Insert it at the most logical point for including few-shot examples â€” typically as part of the system message\\n    - Add a section title in XML or markdown format. The examples will be provided as `example_1\\nexample_2\\n...` with each example following the example template.\\n- Analyze the examples to infer a consistent structure, and create a single string few_shot_example_template using the Python .format() style. Make sure to follow the following instructions:\\n    - Unless absolutely relevant, do not return an object but instead a string that can be inserted as part of FEW_SHOT_EXAMPLE_PLACEHOLDER\\n    - Make sure to include the variables as part of this string so we can before string formatting with actual examples. Only variables available in the examples can be used. Do not use anything else, do not apply any transformations to the variables either.\\n    - The few shot examples should include the expected response as the goal is to provide examples of the expected output format.\\n    - Ensure the format of the few shot examples are consistent with how the model will be called\\n\\nReturn your output as a JSON object with:\\n\\n- message_list_with_placeholder: the updated list with \"FEW_SHOT_EXAMPLE_PLACEHOLDER\" inserted.\\n- example_template: a string template using the fields provided in the examples (you don\\'t need to use all of them)\\n\\nRespond only with the JSON object. Do not include any explanation or extra text.\\n'}, {'role': 'user', 'content': '{\"message_list\": [{\"role\": \"system\", \"content\": \"Provide an answer to the question\"}, {\"role\": \"user\", \"content\": \"{answer}\"}], \"examples\": [{\"question\": \"Are Smyrnium and Nymania both types of plant?\", \"answer\": \"yes\"}, {\"question\": \"That Darn Cat! and Never a Dull Moment were both produced by what studio?\", \"answer\": \"Walt Disney Productions\"}, {\"question\": \"Was Yakov Protazanov or Marcel Duchamp born in 1881\", \"answer\": \"Yakov Alexandrovich Protazanov (Russian: \\\\u042f\\\\u0301\\\\u043a\\\\u043e\\\\u0432 \\\\u0410\\\\u043b\\\\u0435\\\\u043a\\\\u0441\\\\u0430\\\\u0301\\\\u043d\\\\u0434\\\\u0440\\\\u043e\\\\u0432\\\\u0438\\\\u0447 \\\\u041f\\\\u0440\\\\u043e\\\\u0442\\\\u0430\\\\u0437\\\\u0430\\\\u0301\\\\u043d\\\\u043e\\\\u0432 ; January 23 (O.S. February 4), 1881\"}, {\"question\": \"What was the name of the man who Carol Kane played the wife of who was a lovable-but-goofy mechanic in a television series?\", \"answer\": \"Latka Gravas\"}, {\"question\": \"How many copies were sold of the last UK number-one single that was written by Noddy Holder and Jim Lea?\", \"answer\": \"in excess of one million copies\"}, {\"question\": \"Long Lake is located in which town?\", \"answer\": \"Harrison\"}, {\"question\": \"The actor who voiced Shaggy in the \\\\\"Scooby-Doo\\\\\" franchise also appeared as a radio personality on what radio network?\", \"answer\": \"Clear Channel Radio\"}, {\"question\": \"Which three Disney locations host a ride in which an animatronic character described as \\\\\"an elderly [male] ghost ... clutching a hatbox\\\\\" originally appeared in the ride\\'s attic scene?\", \"answer\": \"Disneyland, Magic Kingdom, and Tokyo Disneyland\"}, {\"question\": \"Are Truckin\\' Magazine and Girlfriends both publications that offer relationship advice?\", \"answer\": \"no\"}, {\"question\": \"Who did Holly Dunn record \\\\\"Daddy\\'s Hands\\\\\" for?\", \"answer\": \"MTM Records\"}]}'}], output={'id': 'chatcmpl-Bf6k3ijhM7rMAVPyMfgLdqrOXNgkW', 'created': 1749137063, 'model': 'gpt-4o-mini-2024-07-18', 'object': 'chat.completion', 'system_fingerprint': 'fp_34a54ae93c', 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': '{\\n  \"message_list_with_placeholder\": [\\n    {\\n      \"role\": \"system\",\\n      \"content\": \"Provide an answer to the question\\\\n\\\\n### Few Shot Examples\\\\nFEW_SHOT_EXAMPLE_PLACEHOLDER\"\\n    },\\n    {\\n      \"role\": \"user\",\\n      \"content\": \"{answer}\"\\n    }\\n  ],\\n  \"example_template\": \"Question: {question}\\\\nAnswer: {answer}\"\\n}', 'role': 'assistant', 'tool_calls': None, 'function_call': None, 'annotations': []}}], 'usage': {'completion_tokens': 86, 'prompt_tokens': 912, 'total_tokens': 998, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'service_tier': 'default'}, metadata={'created_from': 'litellm', 'user_api_key_hash': None, 'user_api_key_alias': None, 'user_api_key_team_id': None, 'user_api_key_org_id': None, 'user_api_key_user_id': None, 'user_api_key_team_alias': None, 'user_api_key_user_email': None, 'spend_logs_metadata': None, 'requester_ip_address': None, 'requester_metadata': None, 'user_api_key_end_user_id': None, 'prompt_management_metadata': None, 'applied_guardrails': [], 'mcp_tool_call_metadata': None, 'usage_object': {'completion_tokens': 86, 'prompt_tokens': 912, 'total_tokens': 998, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'type': 'completion', 'status': 'success', 'cost': {'total_tokens': 0.0, 'currency': 'USD'}, 'model_map_information': {'model_map_key': 'gpt-4o-mini-2024-07-18', 'model_map_value': {'key': 'gpt-4o-mini-2024-07-18', 'max_tokens': 16384, 'max_input_tokens': 128000, 'max_output_tokens': 16384, 'input_cost_per_token': 1.5e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': 7.5e-08, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': 7.5e-08, 'output_cost_per_token_batches': 3e-07, 'output_cost_per_token': 6e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'openai', 'mode': 'chat', 'supports_system_messages': True, 'supports_response_schema': True, 'supports_vision': True, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': True, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': True, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': {'search_context_size_low': 30.0, 'search_context_size_medium': 35.0, 'search_context_size_high': 50.0}, 'tpm': None, 'rpm': None, 'supported_openai_params': ['frequency_penalty', 'logit_bias', 'logprobs', 'top_logprobs', 'max_tokens', 'max_completion_tokens', 'modalities', 'prediction', 'n', 'presence_penalty', 'seed', 'stop', 'stream', 'stream_options', 'temperature', 'top_p', 'tools', 'tool_choice', 'function_call', 'functions', 'max_retries', 'extra_headers', 'parallel_tool_calls', 'audio', 'response_format', 'user']}}, 'model': 'gpt-4o-mini', 'model_id': '', 'model_group': '', 'api_base': '', 'cache_hit': True, 'saved_cache_cost': 0.0001884, 'error_str': None, 'model_parameters': {}, 'hidden_params': {'model_id': None, 'cache_key': None, 'api_base': None, 'response_cost': None, 'additional_headers': {}, 'litellm_overhead_time_ms': None, 'batch_models': None, 'litellm_model_name': None, 'usage_object': None}}, tags=None, error_info=None, usage={'completion_tokens': 86, 'prompt_tokens': 912, 'total_tokens': 998}, created_at=datetime.datetime(2025, 6, 5, 16, 14, 46, 958671, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 6, 5, 16, 14, 46, 960273, tzinfo=TzInfo(UTC)), created_by='jverre', last_updated_by='jverre', feedback_scores=[FeedbackScorePublic(name='Correctness', category_name=None, value=0.0, reason='The evaluation cannot be completed as the input and output are not provided for assessment.', source='online_scoring', created_at=datetime.datetime(2025, 6, 5, 16, 14, 49, 802098, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 6, 5, 16, 14, 49, 802098, tzinfo=TzInfo(UTC)), created_by='jverre', last_updated_by='jverre'), FeedbackScorePublic(name='Hallucination', category_name=None, value=0.0, reason='The OUTPUT is entirely faithful to the CONTEXT, adhering strictly to the information provided without introducing any new or contradictory details. The analysis confirms that all statements are accurate and correctly attributed, with no evidence of hallucination or misattribution.', source='online_scoring', created_at=datetime.datetime(2025, 6, 5, 16, 14, 48, 759848, tzinfo=TzInfo(UTC)), last_updated_at=datetime.datetime(2025, 6, 5, 16, 14, 48, 759848, tzinfo=TzInfo(UTC)), created_by='jverre', last_updated_by='jverre')], comments=None, guardrails_validations=None, total_estimated_cost=None, span_count=1, duration=0.529, thread_id=None, visibility_mode='default', llm_span_count=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(traces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove any duplicates while preserving the order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates while preserving order\n",
    "seen = set()\n",
    "unique_texts = []\n",
    "for text in cleaned_texts:\n",
    "    if text not in seen:\n",
    "        seen.add(text)\n",
    "        unique_texts.append(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create the `context` to pass to the synthetic data generation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = f\"\"\"\n",
    "This is a collection of AI/LLM conversation traces from\n",
    "a given Comet Opik observability project. The following\n",
    "text contains various interactions and responses that\n",
    "can be used to generate relevant questions and answers.\n",
    "<input>\n",
    "{chr(10).join(unique_texts)}\n",
    "</input>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and cleaned 72 unique text segments from traces\n",
      "Total context length: 4235 characters\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found and cleaned {len(unique_texts)} unique text segments from traces\")\n",
    "print(f\"Total context length: {len(context)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Synthethic Data\n",
    "\n",
    "We are now ready to generate the synthethic data using `tinyqabenchmarkpp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model for tinyqabenchmarkpp\n",
    "TQB_GENERATOR_MODEL = \"openai/gpt-4o-mini\"\n",
    "\n",
    "# Number of questions to generate\n",
    "TQB_NUM_QUESTIONS = 20\n",
    "\n",
    "# Languages to generate questions in\n",
    "TQB_LANGUAGES = \"en\"\n",
    "\n",
    "# Categories to generate questions in\n",
    "TQB_CATEGORIES = (\n",
    "    \"use context provided and elaborate on it to generate a more detailed answers\"\n",
    ")\n",
    "\n",
    "# Difficulty of the questions to generate\n",
    "TQB_DIFFICULTY = \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to generate the synthetic data\n",
    "command = [\n",
    "    \"python\",\n",
    "    \"-m\",\n",
    "    \"tinyqabenchmarkpp.generate\",\n",
    "    \"--num\",\n",
    "    str(TQB_NUM_QUESTIONS),\n",
    "    \"--languages\",\n",
    "    TQB_LANGUAGES,\n",
    "    \"--categories\",\n",
    "    TQB_CATEGORIES,\n",
    "    \"--difficulty\",\n",
    "    TQB_DIFFICULTY,\n",
    "    \"--model\",\n",
    "    TQB_GENERATOR_MODEL,\n",
    "    \"--str-output\",\n",
    "    \"--context\",\n",
    "    context,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the synthetic data generation step, please be patient as the language model is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data generated successfully\n",
      "{\"text\": \"What are the significant themes explored in George Orwell's '1984'?\", \"label\": \"totalitarianism, surveillance, individualism\", \"context\": \"In George Orwell's '1984', significant themes include totalitarianism, surveillance, and individualism.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"a359f43f6634190402c17bca0520e2764968e61f0256184bc6c3c5a7699f0ff3\", \"id\": \"d9296f97\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does the concept of doublethink function in '1984'?\", \"label\": \"the acceptance of contradictory beliefs\", \"context\": \"In '1984', doublethink is the acceptance of contradictory beliefs, which is a crucial mechanism of control.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"20daf41703959f6ec3cbe989a2cf340da6cd1a732ac6ef135c16bd4c8f0def81\", \"id\": \"928c5397\", \"lang\": \"en\"}\n",
      "{\"text\": \"What role does the character Winston Smith play in '1984'?\", \"label\": \"a symbol of rebellion against oppression\", \"context\": \"Winston Smith in '1984' serves as a symbol of rebellion against oppression and the struggle for freedom.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"99841e4f5250a100e02a59d2f1cf95e9010b68cf1f1c406e60e0ae5b43eead82\", \"id\": \"4414e062\", \"lang\": \"en\"}\n",
      "{\"text\": \"How is the setting of '1984' significant to the story?\", \"label\": \"a dystopian future under constant surveillance\", \"context\": \"The setting of '1984' is significant as it portrays a dystopian future under constant surveillance by a totalitarian regime.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"2e2a6219bbc5032ad8698537365e2fc35bc572869bf6873c56a20dfbaeebe45d\", \"id\": \"0911f783\", \"lang\": \"en\"}\n",
      "{\"text\": \"What does the slogan 'Big Brother is watching you' signify in '1984'?\", \"label\": \"the pervasive surveillance and control of the Party\", \"context\": \"The slogan 'Big Brother is watching you' signifies the pervasive surveillance and control exerted by the Party in '1984'.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"3fed8d33f72d9f467efd7a0ffabbdbae7b1ffc23e5f206c4cdaa7345cd909eb1\", \"id\": \"fe70ec8b\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does Orwell depict the concept of Newspeak in '1984'?\", \"label\": \"a language designed to limit freedom of thought\", \"context\": \"Orwell depicts Newspeak in '1984' as a language designed to limit freedom of thought and concepts of rebellion.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"1c392719a3eea42cf401888f9265e8e2b747b5cd69960657d7eb816af93e531c\", \"id\": \"8880cd33\", \"lang\": \"en\"}\n",
      "{\"text\": \"What impact does the character O'Brien have on Winston in '1984'?\", \"label\": \"a manipulative figure representing the Party's ideology\", \"context\": \"O'Brien impacts Winston as a manipulative figure representing the Party's ideology and the betrayal of trust.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"84c061834dbbeb5553dd74df58eb9f60d04212be4bc5ee21aa523de37c0c6fb3\", \"id\": \"9e13ea8b\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does the concept of memory play a role in '1984'?\", \"label\": \"the control of the past to manipulate the present\", \"context\": \"In '1984', memory plays a role in the control of the past to manipulate the present and maintain power.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"350b247c3207d617fab7f70598049109e4d6c9deefbcde56e3582dc48fdfbf63\", \"id\": \"85b9a423\", \"lang\": \"en\"}\n",
      "{\"text\": \"What is the significance of the character Julia in '1984'?\", \"label\": \"a representation of personal rebellion and desire\", \"context\": \"Julia in '1984' represents personal rebellion and desire against the oppressive regime of the Party.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"d6d870c6c4a1a53d0414fc6d8bdf372e2ac4932f05c5970a0d0969720b91c38a\", \"id\": \"42f67f0b\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does '1984' address the idea of love and relationships?\", \"label\": \"as tools of rebellion and control\", \"context\": \"'1984' addresses love and relationships as tools of rebellion against the Party and as instruments of control.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"45ecd501f1b5f52dd318d06a24a0124b20f718db01486d6d1589e05e9429d66a\", \"id\": \"651ff036\", \"lang\": \"en\"}\n",
      "{\"text\": \"What role does the Ministry of Truth play in '1984'?\", \"label\": \"the manipulation of information and history\", \"context\": \"The Ministry of Truth in '1984' plays a role in the manipulation of information and history to support the Party's narrative.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"fb8782e62a7ba9d7faf197fb1a38b72b4e2ffb5eb8e359a701967d45dd44c7ad\", \"id\": \"4f9a1c41\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does the ending of '1984' reflect Orwell's views on totalitarianism?\", \"label\": \"a bleak outcome of total control and loss of individuality\", \"context\": \"The ending of '1984' reflects Orwell's views on totalitarianism by presenting a bleak outcome of total control and loss of individuality.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"b61d9f5633216474d588caba7268cb37f38e2bfe09b2111be7ef0649b728d20b\", \"id\": \"be07961c\", \"lang\": \"en\"}\n",
      "{\"text\": \"What is the significance of the character Mr. Charrington in '1984'?\", \"label\": \"a deceptive figure representing the Party's infiltration\", \"context\": \"Mr. Charrington in '1984' is significant as a deceptive figure representing the Party's infiltration into private lives.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"1e44f26dfb0e0547db0a83c5d1cf9139d0e442b90d58bcb53366180fa7b02b56\", \"id\": \"eb580d5b\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does George Orwell portray the concept of freedom in '1984'?\", \"label\": \"as an unattainable ideal under oppressive regimes\", \"context\": \"Orwell portrays the concept of freedom in '1984' as an unattainable ideal under the oppressive regimes of totalitarianism.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"4e6f99cfc10684d89ff704038671a56091dcf5b307490fdff8cb37cbeb89fb87\", \"id\": \"3f0fb1d9\", \"lang\": \"en\"}\n",
      "{\"text\": \"What does the character of Emmanuel Goldstein represent in '1984'?\", \"label\": \"the scapegoat for the Party's failures\", \"context\": \"In '1984', Emmanuel Goldstein represents the scapegoat for the Party's failures and the embodiment of rebellion.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"efc99a6e35f1e015364cdbe3cb43f20c4db7be29306a91653244c21ae374bbf4\", \"id\": \"d33505a2\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does Orwell use symbolism in '1984'?\", \"label\": \"to convey complex ideas about society and control\", \"context\": \"Orwell uses symbolism in '1984' to convey complex ideas about society and the mechanisms of control by the Party.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"db80acdc11a035d8c519e183ff5ca10227fc6e8473b22c56d8f4c7d7551b0103\", \"id\": \"7c1ef99d\", \"lang\": \"en\"}\n",
      "{\"text\": \"What does the concept of the 'proles' signify in '1984'?\", \"label\": \"the possibility of rebellion and hope\", \"context\": \"The concept of the 'proles' in '1984' signifies the possibility of rebellion and hope for a future without oppression.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"0d9aec4e4a8f2ef9e764aebd44d1e24f2a87b94b8483056d2151bfc488658c8a\", \"id\": \"eae345fb\", \"lang\": \"en\"}\n",
      "{\"text\": \"How does the use of propaganda manifest in '1984'?\", \"label\": \"as a means to control public perception and opinion\", \"context\": \"In '1984', propaganda manifests as a means to control public perception and opinion, shaping the citizens' beliefs.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"41e1ec3f7265378e230d1647201c53f81b0017632e1cdc3bb85ab632ce41cae1\", \"id\": \"6015f67c\", \"lang\": \"en\"}\n",
      "{\"text\": \"What is the role of technology in the society depicted in '1984'?\", \"label\": \"as a tool for surveillance and oppression\", \"context\": \"Technology in '1984' serves as a tool for surveillance and oppression, enabling the Party to maintain control over citizens.\", \"tags\": {\"category\": \"literature\", \"difficulty\": \"medium\"}, \"sha256\": \"ab3aeb6c288144ed8ff228038e1284abc923236d7a1b93cfdd0c3f90deb177b5\", \"id\": \"0c318ee9\", \"lang\": \"en\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use a subprocess to run the command\n",
    "import subprocess\n",
    "\n",
    "process = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "\n",
    "if process.stderr:\n",
    "    # Print the errors\n",
    "    print(\"tinyqabenchmarkpp errors:\")\n",
    "    print(process.stderr)\n",
    "else:\n",
    "    # Print the output\n",
    "    print(\"Synthetic data generated successfully\")\n",
    "    print(process.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store New Dataset in Opik\n",
    "We can use the Opik SDK to push this dataset to Opik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = process.stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to process the JSONL response and push to Opik\n",
    "Once we have defined we will be able to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def load_synthetic_data_to_opik(data_str):\n",
    "    \"\"\"Load JSONL synthetic data into Opik as a dataset.\"\"\"\n",
    "    items = []\n",
    "    for line in data_str.strip().split(\"\\n\"):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            if not isinstance(data, dict):\n",
    "                continue\n",
    "            item = {\n",
    "                \"question\": data.get(\"text\"),\n",
    "                \"answer\": data.get(\"label\"),\n",
    "                \"generated_context\": data.get(\"context\"),\n",
    "                \"category\": data.get(\"tags\", {}).get(\"category\"),\n",
    "                \"difficulty\": data.get(\"tags\", {}).get(\"difficulty\"),\n",
    "            }\n",
    "            if item[\"question\"] and item[\"answer\"]:\n",
    "                items.append(item)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if not items:\n",
    "        print(\"No valid items found.\")\n",
    "        return None\n",
    "\n",
    "    dataset_name = (\n",
    "        f\"demo-tinyqab-dataset-{TQB_CATEGORIES.replace(',', '_')}-{TQB_NUM_QUESTIONS}\"\n",
    "    )\n",
    "    dataset_name = \"\".join(\n",
    "        c if c.isalnum() or c in [\"-\", \"_\"] else \"_\" for c in dataset_name\n",
    "    )\n",
    "\n",
    "    opik_client = opik.Opik()\n",
    "    dataset = opik_client.get_or_create_dataset(\n",
    "        name=dataset_name,\n",
    "        description=f\"Synthetic QA from tinyqabenchmarkpp for {TQB_CATEGORIES}\",\n",
    "    )\n",
    "    dataset.insert(items)\n",
    "    print(f\"Opik Dataset '{dataset.name}' created with ID: {dataset.id}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push the data to Opik using helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opik Dataset 'demo-tinyqab-dataset-use_context_provided_and_elaborate_on_it_to_generate_a_more_detailed_answers-20' created with ID: 0196ea91-e92e-7fac-9ae7-a2745c8d715c\n"
     ]
    }
   ],
   "source": [
    "opik_synthetic_dataset = load_synthetic_data_to_opik(generated_data)\n",
    "if not opik_synthetic_dataset:\n",
    "    print(\"Failed to load synthetic data into Opik. Exiting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Optimization Using Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets import the required packages for the Opik Agent Optimizer SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik_optimizer import MetaPromptOptimizer\n",
    "from opik.evaluation.metrics import LevenshteinRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to setup some intputs to our optimizer such as our starting prompt and some other configuration items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik_optimizer import ChatPrompt\n",
    "\n",
    "# Initial prompt for the optimizer\n",
    "OPTIMIZER_INITIAL_PROMPT = ChatPrompt(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"{question}\"},\n",
    "    ],\n",
    "    project_name=OPIK_PROJECT_NAME,\n",
    ")\n",
    "\n",
    "# Model for Opik Agent Optimizer\n",
    "OPTIMIZER_MODEL = \"openai/gpt-4o-mini\"\n",
    "\n",
    "# Population size for the optimizer\n",
    "# Reduced for quicker demo\n",
    "OPTIMIZER_POPULATION_SIZE = 5\n",
    "\n",
    "# Number of generations for the optimizer\n",
    "# Reduced for quicker demo\n",
    "OPTIMIZER_NUM_GENERATIONS = 2\n",
    "\n",
    "# Number of samples from dataset for optimization eval\n",
    "OPTIMIZER_N_SAMPLES_OPTIMIZATION = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can setup the metric configuration used for the evaluation, as well as he task_config for passing in the dataset headings and initial prompt.\n",
    "\n",
    "We finally pass this to our optimizer to set this up. We are opting to use the `MetaPromptOptimizer` optimizer in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Configuration\n",
    "def levenshtein_ratio(dataset_item, llm_output):\n",
    "    return LevenshteinRatio().score(reference=dataset_item[\"answer\"], output=llm_output)\n",
    "\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = MetaPromptOptimizer(\n",
    "    model=OPTIMIZER_MODEL,\n",
    "    population_size=OPTIMIZER_POPULATION_SIZE,\n",
    "    num_generations=OPTIMIZER_NUM_GENERATIONS,\n",
    "    infer_output_style=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the optimizer on the dataset and initial starting prompt to find the best prompt based on our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out if you wan to pull the dataset from Opik without having\n",
    "# to generate the synthetic data again\n",
    "\n",
    "# import opik\n",
    "# opik_client = opik.Opik()\n",
    "# opik_synthetic_dataset = opik_client.get_or_create_dataset(\"demo-tinyqab-dataset-use_context_provided_and_elaborate_on_it_to_generate_a_more_detailed_answers-20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®\n",
      "â”‚ \u001b[32mâ— \u001b[0mRunning Opik Evaluation - \u001b[34mMetaPromptOptimizer\u001b[0m                    â”‚\n",
      "â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n",
      "\n",
      "\n",
      "> Let's optimize the prompt:\n",
      "\n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m  You are a helpful assistant.                                      \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m  {question}                                                        \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "\u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\n",
      "Using MetaPromptOptimizer with the parameters: \n",
      "\u001b[2m  - n_samples: \u001b[0m\u001b[36m10\u001b[0m\n",
      "\u001b[2m  - auto_continue: \u001b[0m\u001b[36mFalse\u001b[0m\n",
      "\n",
      "\n",
      "> First we will establish the baseline performance:\n",
      "\u001b[32m  Baseline score was: 0.0405.\u001b[0m\n",
      "\n",
      "> Starting the optimization run\n",
      "â”‚\n",
      "â”‚ - Starting optimization round 1 of 3\n",
      "â”‚    Generating candidate prompts:\n",
      "\u001b[2mâ”‚      Successfully generated 4 candidate prompts\u001b[0m\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 1:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a knowledgeable assistant specializing in literature      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  analysis.                                                         \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  Given the question '{question}', please provide a concise answer  \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  that reflects the key themes and context from the text.           \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[32mâ”‚          Evaluation score: 0.0872 (115.19%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 2:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a helpful assistant.                                      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  For the question '{question}', generate a detailed answer that    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  includes the main idea and supporting details, ensuring it        \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  aligns closely with the provided answer.                          \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[31mâ”‚          Evaluation score: 0.0382 (-5.73%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 3:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are an expert in answering literature-related questions.      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  Analyze the question '{question}' and respond with an answer      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  that captures the essence of the text, ensuring to include        \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  relevant context.                                                 \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[31mâ”‚          Evaluation score: 0.0389 (-4.03%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 4:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a literature assistant.                                   \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  For the question '{question}', please provide a precise answer    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  that directly addresses the question and mirrors the provided     \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  answer structure.                                                 \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[32mâ”‚          Evaluation score: 0.1236 (205.20%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Completed optimization round 1 of 3\n",
      "\u001b[32mâ”‚    Found a new best performing prompt: 0.1236 (205.20%)\u001b[0m\n",
      "â”‚\n",
      "â”‚ - Starting optimization round 2 of 3\n",
      "â”‚    Generating candidate prompts:\n",
      "\u001b[2mâ”‚      Successfully generated 4 candidate prompts\u001b[0m\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 1:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a literature analysis assistant with expertise in         \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  thematic interpretation.                                          \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  For the question '{question}', provide a concise answer that      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  directly addresses the question, includes key themes from the     \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  text, and follows the structure of the expected answer:           \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  '{answer}'.                                                       \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[32mâ”‚          Evaluation score: 0.1252 (1.31%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 2:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a specialized assistant in literary analysis.             \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  Answer the question '{question}' by providing a direct and        \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  concise response that captures the essence of the text, ensuring  \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  to reflect the answer format: '{answer}'.                         \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[32mâ”‚          Evaluation score: 0.4121 (233.35%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 3:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are an expert in literature with a focus on thematic and      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  contextual analysis.                                              \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  For the question '{question}', please provide a precise answer    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  that directly addresses the question, incorporates relevant       \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  context from the text, and matches the expected response          \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  structure: '{answer}'.                                            \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[31mâ”‚          Evaluation score: 0.0894 (-27.69%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 4:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a knowledgeable literature assistant.                     \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  Given the question '{question}', provide a clear and concise      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  answer that directly addresses the question, mirrors the          \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  expected answer structure: '{answer}', and highlights key themes  \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  from the text.                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[32mâ”‚          Evaluation score: 0.1415 (14.45%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Completed optimization round 2 of 3\n",
      "\u001b[32mâ”‚    Found a new best performing prompt: 0.4121 (233.35%)\u001b[0m\n",
      "â”‚\n",
      "â”‚ - Starting optimization round 3 of 3\n",
      "â”‚    Generating candidate prompts:\n",
      "\u001b[2mâ”‚      Successfully generated 4 candidate prompts\u001b[0m\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 1:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a specialized assistant in literary analysis.             \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  For the question '{question}', provide a direct and concise       \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  answer that captures the essence of the text, focusing on key     \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  themes and context. Ensure your response follows the exact        \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  format of the expected answer: '{answer}'.                        \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[32mâ”‚          Evaluation score: 0.6476 (57.14%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 2:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are an expert in literary analysis.                           \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  Analyze the question '{question}' and provide a concise answer    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  that directly addresses it. Your response should reflect the      \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  expected answer format: '{answer}', and include relevant themes   \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  from the text.                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[31mâ”‚          Evaluation score: 0.1246 (-69.77%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 3:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a knowledgeable literature assistant.                     \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  Given the question '{question}', deliver a precise answer that    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  encapsulates the main ideas of the text. Ensure your answer       \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  matches the expected format: '{answer}' and highlights            \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  significant themes.                                               \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[31mâ”‚          Evaluation score: 0.1521 (-63.10%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Evaluating candidate prompt 4:\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  You are a literature analysis expert.                             \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "â”‚         \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  For the question '{question}', provide a clear and concise        \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  answer that directly addresses the question while adhering to     \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  the expected answer format: '{answer}'. Include key themes and    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m  context from the text to enhance your response.                   \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ”‚\u001b[0m                                                                    \u001b[2mâ”‚\u001b[0m\n",
      "â”‚         \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n",
      "\u001b[31mâ”‚          Evaluation score: 0.1025 (-75.13%)\u001b[0m\n",
      "â”‚\n",
      "â”‚\n",
      "â”‚    Completed optimization round 3 of 3\n",
      "\u001b[32mâ”‚    Found a new best performing prompt: 0.6476 (57.14%)\u001b[0m\n",
      "â”‚\n",
      "\n",
      "> Optimization complete\n",
      "\n",
      "\u001b[32mâ•­â”€\u001b[0m\u001b[32m Optimization results \u001b[0m\u001b[32mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[32mâ”€â•®\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m                                                                    \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[1;32mPrompt was optimized and improved from 0.0405 to 0.6476 \u001b[0m          \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[1;32m(1498.70%)\u001b[0m                                                        \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m                                                                    \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  Optimized prompt:                                                 \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ•­â”€\u001b[0m\u001b[2m system \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m                                                              \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m  You are a specialized assistant in literary analysis.       \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m                                                              \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ•­â”€\u001b[0m\u001b[2m user \u001b[0m\u001b[2mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[2mâ”€â•®\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m                                                              \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m  For the question '{question}', provide a direct and         \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m  concise answer that captures the essence of the text,       \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m  focusing on key themes and context. Ensure your response    \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m  follows the exact format of the expected answer:            \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m  '{answer}'.                                                 \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ”‚\u001b[0m                                                              \u001b[2mâ”‚\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m  \u001b[2mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m  \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ”‚\u001b[0m                                                                    \u001b[32mâ”‚\u001b[0m\n",
      "\u001b[32mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the optimizer\n",
    "result = optimizer.optimize_prompt(\n",
    "    prompt=OPTIMIZER_INITIAL_PROMPT,\n",
    "    dataset=opik_synthetic_dataset,\n",
    "    metric=levenshtein_ratio,\n",
    "    n_samples=OPTIMIZER_N_SAMPLES_OPTIMIZATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization process finished\n",
    "We can output our results to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mâ•”â•\u001b[0m\u001b[33mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\u001b[33m \u001b[0m\u001b[1;33mOptimization Complete\u001b[0m\u001b[33m \u001b[0m\u001b[33mâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\u001b[33mâ•â•—\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m                                                                                                                 \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mOptimizer:        \u001b[0m\u001b[2m \u001b[0m\u001b[1mMetaPromptOptimizer\u001b[0m                                                                          \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mModel Used:       \u001b[0m\u001b[2m \u001b[0mopenai/gpt-4o-mini (\u001b[2mTemp:\u001b[0m \u001b[2mN/A\u001b[0m)                                                               \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mMetric Evaluated: \u001b[0m\u001b[2m \u001b[0m\u001b[1mlevenshtein_ratio\u001b[0m                                                                            \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mInitial Score:    \u001b[0m\u001b[2m \u001b[0m0.0405                                                                                       \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mFinal Best Score: \u001b[0m\u001b[2m \u001b[0m\u001b[1;36m0.6476\u001b[0m                                                                                       \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mTotal Improvement:\u001b[0m\u001b[2m \u001b[0m\u001b[1;32m1498.70%\u001b[0m                                                                                     \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mRounds Completed: \u001b[0m\u001b[2m \u001b[0m3                                                                                            \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[2mStopped Early:    \u001b[0m\u001b[2m \u001b[0mN/A                                                                                          \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m                                                                                                                 \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m                                                                                                                 \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ•­â”€\u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34m \u001b[0m\u001b[1;34mFinal Optimized Prompt\u001b[0m\u001b[34m \u001b[0m\u001b[34mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\u001b[34mâ”€â•®\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m                                                                                                             \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m  \u001b[1;35mSystem:\u001b[0m You are a specialized assistant in literary analysis.                                              \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m  ---                                                                                                        \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m  \u001b[1;32mUser:\u001b[0m For the question '{question}', provide a direct and concise answer that captures the essence of the  \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m  text, focusing on key themes and context. Ensure your response follows the exact format of the expected    \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m  answer: '{answer}'.                                                                                        \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m  ---                                                                                                        \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ”‚\u001b[0m                                                                                                             \u001b[34mâ”‚\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m \u001b[34mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\u001b[0m \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•‘\u001b[0m                                                                                                                 \u001b[33mâ•‘\u001b[0m\n",
      "\u001b[33mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSDJ1bFx51kd"
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "You can try out other optimizers. More details can be found in the [Opik Agent Optimizer documentation](https://www.comet.com/docs/opik/agent_optimization/overview)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
