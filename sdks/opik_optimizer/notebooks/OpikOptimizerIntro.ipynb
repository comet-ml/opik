{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Opik Optimizer (beta)\n",
        "\n",
        "Welcome to the Opik Opitimizer beta program! In this notebook we'll walk through a basic examples of a number of an optimizer algorithm.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, let's install the needed Python packages.\n",
        "\n",
        "We'll of course need `opik`:"
      ],
      "metadata": {
        "id": "LbN7qkaDs-gS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHLhruvis8ti",
        "outputId": "bbd41f8c-7ef2-4480-8b39-1d3fad9575d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.3/149.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m458.7/458.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m114.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install opik --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll install the beta version of the `opik-optimizer`:\n",
        "\n",
        "(There may be some package conflicts with torch and nvidia libraries, but that is fine)."
      ],
      "metadata": {
        "id": "L-n8gjG45B1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install git+https://github.com/comet-ml/opik#subdirectory=sdks/opik_optimizer --upgrade --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkjxdMfdtB1w",
        "outputId": "8be2e049-21ce-4643-96ba-70d97a9181e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.0/259.0 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m124.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m460.6/460.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for opik_optimizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Ignore any errors above)\n",
        "\n",
        "\n",
        "[Comet](https://www.comet.com/site?from=llm&utm_source=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=langchain&utm_campaign=opik) and grab your API Key.\n",
        "\n",
        "> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik) for more information.\n",
        "\n",
        "You can use your own workspace."
      ],
      "metadata": {
        "id": "hNtXvK2Kt3GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opik\n",
        "\n",
        "# Configure Opik\n",
        "opik.configure()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-mkTrf9vHXk",
        "outputId": "ac27f33b-641a-42ee-9441-b85248f2ec2d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Your Opik API key is available in your account settings, can be found at https://www.comet.com/api/my/settings/ for Opik cloud\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your Opik API key:··········\n",
            "Do you want to use \"dsblank\" workspace? (Y/n)y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Configuration saved to file: /root/.opik.config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we'll use OpenAI models, so we need to set our OpenAI API key:"
      ],
      "metadata": {
        "id": "kLSeOY24vWfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf2IJP2Z28P5",
        "outputId": "496921cb-8ddf-418a-ae45-159bd0dc9091"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To optimize any prompt, we'll need:\n",
        "\n",
        "1. A starting prompt\n",
        "2. A metric\n",
        "3. A dataset\n",
        "\n",
        "For this initial test, we'll start with a portion of the HotpotQA dataset.\n",
        "\n",
        "HotpotQA is a question/answering dataset featuring natural questions, with correct answers. It was collected by a team of NLP researchers at Carnegie Mellon University, Stanford University, and Université de Montréal.\n",
        "\n",
        "Let's take a look at the demo dataset \"hotpot-300\".\n",
        "\n",
        "First, we get or create it. This will add it to our default Opik project:"
      ],
      "metadata": {
        "id": "NSj91yEMvqHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer.demo import get_or_create_dataset\n",
        "\n",
        "opik_dataset = get_or_create_dataset(\"hotpot-300\")"
      ],
      "metadata": {
        "id": "Rvg_dl5MwmwO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at a specific row from the dataset:"
      ],
      "metadata": {
        "id": "n8mwHiP_xfNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows = opik_dataset.get_items()\n",
        "rows[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5la3y6Kw00C",
        "outputId": "42329df8-96ac-4a22-eb84-8e80624b39b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0195d400-517b-7f17-b746-ff3d084463ac',\n",
              " 'question': 'Were both drinks, the Smoking Bishop and the Mickey Slim, popular in different countries?',\n",
              " 'answer': 'yes'}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that each item has a \"question\" and \"answer\". Some of the answers are short and direct, and some of them are more complicated:"
      ],
      "metadata": {
        "id": "dc7GqXTlxmnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rows[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOU4GVxkxF9D",
        "outputId": "c2557e72-f371-423b-c366-5c3059f4405e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0195d400-5179-7259-8104-1f0a55a13ac2',\n",
              " 'question': 'Woody Wuthrie wrote the song \"Do Re Mi\" which was about migrants and their experiences when they arrive in California and are greeted with severe storms that came in waves in what years?',\n",
              " 'answer': '1934, 1936, and 1939–1940'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the answers can be a little messy. We'll need a good metric to able to determine whether a prompt is good or not. But let's start simple.\n",
        "\n",
        "We'll use the `LevenshteinRatio` metric from Opik.\n",
        "\n",
        "It works like this:"
      ],
      "metadata": {
        "id": "cA_73ohDx8zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik.evaluation.metrics import LevenshteinRatio\n",
        "\n",
        "metric = LevenshteinRatio()\n",
        "\n",
        "metric.score(\"Hello\", \"Hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyN9fjDqysDl",
        "outputId": "36dbaf0a-2ed2-460e-ef0e-621570652875"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OPIK: Started logging traces to the \"Default Project\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=0196213d-a988-737a-88e8-f069b6408310&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ScoreResult(name='levenshtein_ratio_metric', value=1.0, reason=None, metadata=None, scoring_failed=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that value is 1.0. That meens that there is no difference between \"Hello\" and \"Hello\". That is as close as you can get!\n",
        "\n",
        "As the difference between the two strings get further away, the value gets smaller, approaching 0.0:"
      ],
      "metadata": {
        "id": "F8Dhc8NXzark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric.score(\"Hello\", \"Hello!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml39af7HzUsF",
        "outputId": "bd06f0b3-114a-416c-f851-1babf3dcbbcb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ScoreResult(name='levenshtein_ratio_metric', value=0.9090909090909091, reason=None, metadata=None, scoring_failed=False)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric.score(\"one\", \"six\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1fbHU0qzy0H",
        "outputId": "b9dd5f8f-2fbb-4611-be90-feb7327b7d0c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ScoreResult(name='levenshtein_ratio_metric', value=0.0, reason=None, metadata=None, scoring_failed=False)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, slightly different have values close to 1, and no letters in common give a zero.\n",
        "\n",
        "To find out more about the metric, see [Levenshtein Distance](https://en.wikipedia.org/wiki/Levenshtein_distance)."
      ],
      "metadata": {
        "id": "85oysQAMPeCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, we have a dataset and a metric, now we are ready to construct an optimizer.\n",
        "\n",
        "We can use an OpenAI model name, or more generally, a LiteLLM model name. Just make sure you have your model API key set as we did above.\n",
        "\n",
        "We'll start with the `FewShotOptimizer`:"
      ],
      "metadata": {
        "id": "-I0Coxgt0QNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from opik_optimizer import FewShotOptimizer\n",
        "\n",
        "optimizer = FewShotOptimizer(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    temperature=0.1,\n",
        "    max_tokens=5000,\n",
        ")"
      ],
      "metadata": {
        "id": "ixQUjZS5tPOU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we need a prompt to optimize. Given the examples above, let's try something like:"
      ],
      "metadata": {
        "id": "fBdeiCOIQIf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_prompt = \"Provide an answer to the question\""
      ],
      "metadata": {
        "id": "uD7I2j-cQS0E"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = optimizer.evaluate_prompt(\n",
        "    dataset=opik_dataset,\n",
        "    metric=metric,\n",
        "    prompt=initial_prompt,\n",
        "    # Algorithm-specific kwargs:\n",
        "    input_key=\"question\",\n",
        "    output_key=\"answer\",\n",
        ")\n",
        "score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8nqp8deQciO",
        "outputId": "3555e0ec-fe58-43fd-aa29-24b789e52e89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 1514.35it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11992387410132233"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score 0.11 is pretty low. Let's see if we can optimize it!"
      ],
      "metadata": {
        "id": "wO6Dax5vQ1iY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The FewShotOptimizer can, fairly quickly, create better prompts.\n",
        "\n",
        "Let's try it out. It takes exactly the same parameters as evaluate_prompt(), but will run for a minute or so."
      ],
      "metadata": {
        "id": "Tr9o0G-_2rg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = optimizer.optimize_prompt(\n",
        "    dataset=opik_dataset,\n",
        "    metric=metric,\n",
        "    prompt=\"Answer the question with a short, 1 to 5 word phrase\",\n",
        "    # Algorithm-specific kwargs:\n",
        "    input_key=\"question\",\n",
        "    output_key=\"answer\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fRxnVXX0kqv",
        "outputId": "85388496-cb02-4cc7-86ef-dc30f1fa0ea2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Bootstrap few-shot examples...\n",
            "Bootstrapping set 1/5\n",
            "Bootstrapping set 2/5\n",
            "Bootstrapping set 3/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 4/60 [00:03<00:54,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
            "Bootstrapping set 4/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 2/60 [00:07<03:32,  3.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  7%|▋         | 4/60 [00:03<00:46,  1.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrapped 4 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
            "Step 2: Propose instruction candidates...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.49it/s]\n",
            "100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n",
            "100%|██████████| 10/10 [00:06<00:00,  1.45it/s]\n",
            "100%|██████████| 10/10 [00:06<00:00,  1.47it/s]\n",
            "100%|██████████| 10/10 [00:06<00:00,  1.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYG0Ld45vAm8",
        "outputId": "a96c5686-80e0-4a33-83be-c23b9dad348d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimization Results:\n",
            "    Best prompt: Provide a brief answer (1-5 words) to the given trivia question.\n",
            "    Best score: 0.5740\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although there is some randomness, you probably got a better prompt. My result is about 5 times better.\n",
        "\n",
        "This is just the initial framework for optimizers for Opik!\n",
        "\n",
        "Please see the [Opik Optimizer repo](https://github.com/comet-ml/opik/tree/main/sdks/opik_optimizer) for additional examples using different algorithms."
      ],
      "metadata": {
        "id": "xSZoEVBDRoyZ"
      }
    }
  ]
}