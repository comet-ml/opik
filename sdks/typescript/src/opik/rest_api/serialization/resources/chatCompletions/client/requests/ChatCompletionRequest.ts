/**
 * This file was auto-generated by Fern from our API Definition.
 */

import * as serializers from "../../../../index";
import * as OpikApi from "../../../../../api/index";
import * as core from "../../../../../core";
import { Message } from "../../../../types/Message";
import { StreamOptions } from "../../../../types/StreamOptions";
import { ResponseFormat } from "../../../../types/ResponseFormat";
import { Tool } from "../../../../types/Tool";
import { Function } from "../../../../types/Function";
import { FunctionCall } from "../../../../types/FunctionCall";

export const ChatCompletionRequest: core.serialization.Schema<
    serializers.ChatCompletionRequest.Raw,
    OpikApi.ChatCompletionRequest
> = core.serialization.object({
    model: core.serialization.string().optional(),
    messages: core.serialization.list(Message).optional(),
    temperature: core.serialization.number().optional(),
    topP: core.serialization.property("top_p", core.serialization.number().optional()),
    n: core.serialization.number().optional(),
    stream: core.serialization.boolean().optional(),
    streamOptions: core.serialization.property("stream_options", StreamOptions.optional()),
    stop: core.serialization.list(core.serialization.string()).optional(),
    maxTokens: core.serialization.property("max_tokens", core.serialization.number().optional()),
    maxCompletionTokens: core.serialization.property("max_completion_tokens", core.serialization.number().optional()),
    presencePenalty: core.serialization.property("presence_penalty", core.serialization.number().optional()),
    frequencyPenalty: core.serialization.property("frequency_penalty", core.serialization.number().optional()),
    logitBias: core.serialization.property(
        "logit_bias",
        core.serialization.record(core.serialization.string(), core.serialization.number()).optional(),
    ),
    user: core.serialization.string().optional(),
    responseFormat: core.serialization.property("response_format", ResponseFormat.optional()),
    seed: core.serialization.number().optional(),
    tools: core.serialization.list(Tool).optional(),
    toolChoice: core.serialization.property(
        "tool_choice",
        core.serialization.record(core.serialization.string(), core.serialization.unknown()).optional(),
    ),
    parallelToolCalls: core.serialization.property("parallel_tool_calls", core.serialization.boolean().optional()),
    store: core.serialization.boolean().optional(),
    metadata: core.serialization.record(core.serialization.string(), core.serialization.string()).optional(),
    reasoningEffort: core.serialization.property("reasoning_effort", core.serialization.string().optional()),
    serviceTier: core.serialization.property("service_tier", core.serialization.string().optional()),
    functions: core.serialization.list(Function).optional(),
    functionCall: core.serialization.property("function_call", FunctionCall.optional()),
});

export declare namespace ChatCompletionRequest {
    export interface Raw {
        model?: string | null;
        messages?: Message.Raw[] | null;
        temperature?: number | null;
        top_p?: number | null;
        n?: number | null;
        stream?: boolean | null;
        stream_options?: StreamOptions.Raw | null;
        stop?: string[] | null;
        max_tokens?: number | null;
        max_completion_tokens?: number | null;
        presence_penalty?: number | null;
        frequency_penalty?: number | null;
        logit_bias?: Record<string, number> | null;
        user?: string | null;
        response_format?: ResponseFormat.Raw | null;
        seed?: number | null;
        tools?: Tool.Raw[] | null;
        tool_choice?: Record<string, unknown> | null;
        parallel_tool_calls?: boolean | null;
        store?: boolean | null;
        metadata?: Record<string, string> | null;
        reasoning_effort?: string | null;
        service_tier?: string | null;
        functions?: Function.Raw[] | null;
        function_call?: FunctionCall.Raw | null;
    }
}
