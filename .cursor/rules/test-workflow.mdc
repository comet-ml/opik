---
description: Opik Test Workflow
globs: 
alwaysApply: true
---
# Opik Test Workflow

## **Initial Testing Framework Setup**

Before implementing the Test workflow, ensure your project has a proper testing framework configured. This section covers setup for different technology stacks.

### **Detecting Project Type & Framework Needs**

**AI Agent Assessment Checklist:**

1. **Language Detection**: Check for `pom.xml` (Java), `package.json` (TypeScript), `requirements.txt` (Python) etc.
2. **Existing Tests**: Look for test directories or test files (`Test`, `test_`, `.test.`, `.spec.`, `_test.` etc.)
3. **Framework Detection**: Check for existing test runners in dependencies
4. **Project Structure**: Analyze directory structure for testing patterns

### **Generic Testing Framework Setup (Any Language)**

#### **Universal Testing Principles**

**Test Organization:**

- **Unit Tests**: Fast, isolated, no external dependencies
- **Integration Tests**: Test component interactions
- **E2E Tests**: Test complete user workflows
- **Performance Tests**: Load and stress testing (if applicable)

**Naming Conventions:**

- **Test Files**: `Test`, `test_`, `.test.`, `.spec.` etc. or language-specific patterns
- **Test Functions**: Descriptive names (e.g., `should_return_error_for_invalid_input`)
- **Test Directories**: Organized by test type and mirroring source structure

## **Test Maintenance & Evolution**

### **Adding Tests for New Features**

1. **Create test file** following the location conventions
2. **Follow established patterns**
3. **Use existing fixtures**
4. **Apply proper mocking** follow the patterns for dependencies
5. **Meet coverage thresholds** for the component being tested

### **Test Performance Optimization**

1. **Parallel Execution**: Ensure unit tests can run in parallel
2. **Test Isolation**: Each test should be independent
3. **Mock Dependencies**: Mock external services and databases
4. **Efficient Setup**: Use fixtures and setup methods appropriately
5. **Database efficiency**: Avoid unnecessary cleanups
6. **Avoid Sleep**: Use proper waiting mechanisms instead of `Thread.sleep()` etc.

### **When to Update Tests**

1. **New Features**: Always add tests for new functionality
2. **Bug Fixes**: Add tests to prevent regression
3. **Refactoring**: Update tests to match new implementation
4. **API Changes**: Update tests when interfaces change

### **Test Review Checklist**

- [ ] Tests cover all code paths
- [ ] Edge cases are tested
- [ ] Error conditions are handled
- [ ] Tests are readable and maintainable
- [ ] Test data is randomly generated, but realistic
- [ ] Mocks are used appropriately
- [ ] Tests run in reasonable time

## **Test Execution Guidelines**

### **Running Tests**

**Java:**

```bash
# Run all tests
mvn test

# Run specific test class
mvn test -Dtest="UserServiceTest"

# Run specific test method
mvn test -Dtest="UserServiceTest#shouldCreateUser_whenValidRequest"

# Skip tests (for compilation check)
mvn compile -DskipTests
```

**TypeScript:**

```bash
# Run all tests
npm test

# Run tests in watch mode
npm run test:watch

# Run specific test file
npm test -- UserProfile.test.tsx

# Run e2e tests
npm run test:e2e
```

**Python:**

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/unit/test_user.py

# Run specific test function
pytest tests/unit/test_user.py::test_create_user_success

# Run with coverage
pytest --cov=opik
```

### **Test Output Management**

To efficiently analyze test results without re-running tests, always capture test output to log files.

**Best Practices:**

```bash
# ✅ DO: Capture test output to a log file
mvn test | tee test-output.log
npm test 2>&1 | tee test-output.log
pytest tests/ 2>&1 | tee test-output.log

# ✅ DO: Search log files instead of re-running tests
grep "FAILED" test-output.log
grep -A 5 "AssertionError" test-output.log
grep -i "error" test-output.log

# ✅ DO: Use descriptive log file names for multiple test runs
mvn test -Dtest="UserServiceTest" | tee user-service-test.log
pytest tests/unit/ | tee unit-tests.log
pytest tests/integration/ | tee integration-tests.log

# ❌ DON'T: Re-run tests just to search output
mvn test | grep "FAILED"  # Wastes time re-running all tests
pytest tests/ | grep "error"  # Unnecessary re-execution
```

**Rules:**

1. **Always log test output**: Use `tee` to both display and save output
2. **Search logs, not tests**: Use `grep` on log files to find specific information
3. **Only re-run after code changes**: Never re-run tests unless you've modified code or test files
4. **Use descriptive names**: Name log files based on test scope (unit, integration, specific class)
5. **Redirect stderr**: Use `2>&1` to capture both stdout and stderr in logs

---

**Key References:**

- [Project Structure](mdc:.cursor/rules/project-structure.mdc)
- [Technology stack](mdc:.cursor/rules/tech-stack.mdc)
- [Git Workflow](mdc:.cursor/rules/git-workflow.mdc)
