---
description: Opik Test Workflow
globs: 
alwaysApply: true
---
# Opik Test Workflow

## **Initial Testing Framework Setup**

Before implementing the Test workflow, ensure your project has a proper testing framework configured. This section covers setup for different technology stacks.

### **Detecting Project Type & Framework Needs**

**AI Agent Assessment Checklist:**

1. **Language Detection**: Check for `pom.xml` (Java), `package.json` (TypeScript), `requirements.txt` (Python) etc.
2. **Existing Tests**: Look for test directories or test files (`Test`, `test_`, `.test.`, `.spec.`, `_test.` etc.)
3. **Framework Detection**: Check for existing test runners in dependencies
4. **Project Structure**: Analyze directory structure for testing patterns

### **Generic Testing Framework Setup (Any Language)**

#### **Universal Testing Principles**

**Test Organization:**

- **Unit Tests**: Fast, isolated, no external dependencies
- **Integration Tests**: Test component interactions
- **E2E Tests**: Test complete user workflows
- **Performance Tests**: Load and stress testing (if applicable)

**Naming Conventions:**

- **Test Files**: `Test`, `test_`, `.test.`, `.spec.` etc. or language-specific patterns
- **Test Functions**: Descriptive names (e.g., `should_return_error_for_invalid_input`)
- **Test Directories**: Organized by test type and mirroring source structure

## **Test Maintenance & Evolution**

### **Adding Tests for New Features**

1. **Create test file** following the location conventions
2. **Follow established patterns**
3. **Use existing fixtures**
4. **Apply proper mocking** follow the patterns for dependencies
5. **Meet coverage thresholds** for the component being tested

### **Test Performance Optimization**

1. **Parallel Execution**: Ensure unit tests can run in parallel
2. **Test Isolation**: Each test should be independent
3. **Mock Dependencies**: Mock external services and databases
4. **Efficient Setup**: Use fixtures and setup methods appropriately
5. **Database efficiency**: Avoid unnecessary cleanups
6. **Avoid Sleep**: Use proper waiting mechanisms instead of `Thread.sleep()` etc.

### **When to Update Tests**

1. **New Features**: Always add tests for new functionality
2. **Bug Fixes**: Add tests to prevent regression
3. **Refactoring**: Update tests to match new implementation
4. **API Changes**: Update tests when interfaces change

### **Test Review Checklist**

- [ ] Tests cover all code paths
- [ ] Edge cases are tested
- [ ] Error conditions are handled
- [ ] Tests are readable and maintainable
- [ ] Test data is randomly generated, but realistic
- [ ] Mocks are used appropriately
- [ ] Tests run in reasonable time

## **Test Execution Guidelines**

### **Running Tests**

**Java:**

```bash
# Run all tests
mvn test

# Run specific test class
mvn test -Dtest="UserServiceTest"

# Run specific test method
mvn test -Dtest="UserServiceTest#shouldCreateUser_whenValidRequest"

# Skip tests (for compilation check)
mvn compile -DskipTests
```

**TypeScript:**

```bash
# Run all tests
npm test

# Run tests in watch mode
npm run test:watch

# Run specific test file
npm test -- UserProfile.test.tsx

# Run e2e tests
npm run test:e2e
```

**Python:**

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/unit/test_user.py

# Run specific test function
pytest tests/unit/test_user.py::test_create_user_success

# Run with coverage
pytest --cov=opik
```

### **Test Output Management**

To efficiently analyze test results without re-running tests, always capture test output to log files in a temporary directory.

**Setup:**

```bash
# Create temp directory for test logs (do this once at the start of testing)
# Note: /tmp/ is standard on macOS and Linux. On Windows, use %TEMP% or $env:TEMP
mkdir -p /tmp/opik-test-logs
```

**Best Practices:**

```bash
# ✅ DO: Run ONLY relevant tests for your changes (preferred approach)
mvn test -Dtest="UserServiceTest" 2>&1 | tee /tmp/opik-test-logs/user-service-test.log
pytest tests/unit/test_user.py 2>&1 | tee /tmp/opik-test-logs/unit-test-user.log
npm test -- UserProfile.test.tsx 2>&1 | tee /tmp/opik-test-logs/user-profile-test.log

# ✅ DO: Search log files instead of re-running tests
grep "FAILED" /tmp/opik-test-logs/user-service-test.log
grep -A 5 "AssertionError" /tmp/opik-test-logs/user-service-test.log
grep -i "error" /tmp/opik-test-logs/user-service-test.log

# ✅ DO: Use descriptive log file names for test suites
pytest tests/unit/ 2>&1 | tee /tmp/opik-test-logs/unit-tests.log
pytest tests/integration/ 2>&1 | tee /tmp/opik-test-logs/integration-tests.log

# ✅ DO: Use timestamps for multiple runs of the same test
# Note: date command syntax works on GNU/Linux and BSD/macOS
mvn test -Dtest="UserServiceTest" 2>&1 | tee /tmp/opik-test-logs/user-service-test-$(date +%Y%m%d-%H%M%S).log

# ❌ DON'T: Run all tests unless absolutely necessary
mvn test 2>&1 | tee /tmp/opik-test-logs/all-tests.log  # Avoid - wastes time
pytest tests/ 2>&1 | tee /tmp/opik-test-logs/all-tests.log  # Avoid - run specific tests instead

# ❌ DON'T: Re-run tests just to search output
mvn test | grep "FAILED"  # Wastes time re-running tests
pytest tests/ | grep "error"  # Unnecessary re-execution

# ❌ DON'T: Store log files in the project directory
mvn test | tee test-output.log  # Clutters workspace
```

**Rules:**

1. **Run relevant tests only**: Always run specific test classes/files/methods related to your changes, not the entire test suite
2. **Use temp directory**: Always store test logs in `/tmp/opik-test-logs/` to keep workspace clean (use `%TEMP%` on Windows)
3. **Create directory first**: Ensure the temp directory exists before running tests
4. **Always log test output**: Use `tee` to both display and save output
5. **Search logs, not tests**: Use `grep` on log files to find specific information
6. **Only re-run after code changes**: Never re-run tests unless you've modified code or test files
7. **Use descriptive names**: Name log files based on test scope (specific class name, unit, integration)
8. **Redirect stderr**: Always use `2>&1` to capture both stdout and stderr in logs
9. **Add timestamps**: For repeated test runs, append timestamps to avoid overwriting logs
10. **Avoid running all tests**: Only run the full test suite when explicitly required or for final validation

---

**Key References:**

- [Project Structure](mdc:.cursor/rules/project-structure.mdc)
- [Technology stack](mdc:.cursor/rules/tech-stack.mdc)
- [Git Workflow](mdc:.cursor/rules/git-workflow.mdc)
