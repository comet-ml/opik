import React from "react";

import SliderInputControl from "@/components/shared/SliderInputControl/SliderInputControl";
import PromptModelSettingsTooltipContent from "@/components/pages/PlaygroundPage/PlaygroundPrompt/PromptModelSettings/providerConfigs/PromptModelConfigsTooltipContent";
import { PlaygroundOpenAIConfigsType } from "@/types/providers";
import { DEFAULT_OPEN_AI_CONFIGS } from "@/constants/playground";

interface OpenAIModelSettingsProps {
  configs: PlaygroundOpenAIConfigsType;
  onChange: (configs: Partial<PlaygroundOpenAIConfigsType>) => void;
}

const OpenAIModelConfigs = ({
  configs,
  onChange,
}: OpenAIModelSettingsProps) => {
  return (
    <div className="flex w-72 flex-col gap-6">
      <SliderInputControl
        value={configs.temperature}
        onChange={(v) => onChange({ temperature: v })}
        id="temperature"
        min={0}
        max={1}
        step={0.01}
        defaultValue={DEFAULT_OPEN_AI_CONFIGS.TEMPERATURE}
        label="Temperature"
        tooltip={
          <PromptModelSettingsTooltipContent text="Controls randomness: Lowering results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive." />
        }
      />

      <SliderInputControl
        value={configs.maxCompletionTokens}
        onChange={(v) => onChange({ maxCompletionTokens: v })}
        id="maxCompletionTokens"
        min={0}
        max={10000}
        step={1}
        defaultValue={DEFAULT_OPEN_AI_CONFIGS.MAX_COMPLETION_TOKENS}
        label="Max output tokens"
        tooltip={
          <PromptModelSettingsTooltipContent text="The maximum number of tokens to generate shared between the prompt and completion. The exact limit varies by model. (One token is roughly 4 characters for standard English text)." />
        }
      />

      <SliderInputControl
        value={configs.topP}
        onChange={(v) => onChange({ topP: v })}
        id="topP"
        min={0}
        max={1}
        step={0.01}
        defaultValue={DEFAULT_OPEN_AI_CONFIGS.TOP_P}
        label="Top P"
        tooltip={
          <PromptModelSettingsTooltipContent text="Controls diversity via nucleus sampling: 0.5 means half of all likelihood-weighted options are considered" />
        }
      />

      <SliderInputControl
        value={configs.frequencyPenalty}
        onChange={(v) => onChange({ frequencyPenalty: v })}
        id="frequencyPenalty"
        min={0}
        max={1}
        step={0.01}
        defaultValue={DEFAULT_OPEN_AI_CONFIGS.FREQUENCY_PENALTY}
        label="Frequency penalty"
        tooltip={
          <PromptModelSettingsTooltipContent text="How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim" />
        }
      />

      <SliderInputControl
        value={configs.presencePenalty}
        onChange={(v) => onChange({ presencePenalty: v })}
        id="presencePenalty"
        min={0}
        max={1}
        step={0.01}
        defaultValue={DEFAULT_OPEN_AI_CONFIGS.PRESENCE_PENALTY}
        label="Presence penalty"
        tooltip={
          <PromptModelSettingsTooltipContent text="How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics" />
        }
      />
    </div>
  );
};

export default OpenAIModelConfigs;
