<Tip>
  If you are just getting started with Opik, we recommend first checking out the [Quickstart](/quickstart) guide that
  will walk you through the process of logging your first LLM call.
</Tip>

LLM applications are complex systems that do more than just call an LLM API, they will often involve retrieval, pre-processing and post-processing steps.
Tracing is a tool that helps you understand the flow of your application and identify specific points in your application that may be causing issues.

Opik's tracing functionality allows you to track not just all the LLM calls made by your application but also any of the other steps involved.

<Frame>
  <img src="/img/tracing/introduction.png" />
</Frame>

Opik supports agent observability using our [Typescript SDK](/reference/typescript-sdk/overview),
[Python SDK](/reference/python-sdk/overview), [first class OpenTelemetry support](/integrations/opentelemetry)
and our [REST API](/reference/rest-api/overview).

<Tip>
  We recommend starting with one of our integrations to get started quickly, you can find a full list of our
  integrations in the [integrations overview](/integrations/overview) page.
</Tip>

We won't be covering how to track chat conversations in this guide, you can learn more about this in the
[Logging conversations](/tracing/log_chat_conversations) guide.

## Enable agent observability

### 1. Installing the SDK

Before adding observability to your application, you will first need to install and configure the
Opik SDK.

<Tabs>
    <Tab value="Typescript SDK" title="Typescript SDK" language="typescript">

    ```bash
    npm install opik
    ```

    You can then set the Opik environment variables in your `.env` file:

    ```bash
    # Set OPIK_API_KEY and OPIK_WORKSPACE_NAME in your .env file
    OPIK_API_KEY=your_api_key_here
    OPIK_WORKSPACE=your_workspace_name

    # Optional if you are using Opik Cloud:
    OPIK_URL_OVERRIDE=https://www.comet.com/opik/api
    ```

    </Tab>
    <Tab value="Python SDK" title="Python SDK" language="python">

    ```bash
    # Install the SDK
    pip install opik
    ```

    You can then configure the SDK using the `opik configure` CLI command or by calling
    [`opik.configure`](https://www.comet.com/docs/opik/python-sdk-reference/configure.html) from
    your Jupyter Notebook.

    </Tab>
    <Tab value="OpenTelemetry" title="OpenTelemetry">

    You will need to set the following environment variables for your OpenTelemetry setup:

    ```bash
    export OTEL_EXPORTER_OTLP_ENDPOINT=https://www.comet.com/opik/api/v1/private/otel
    export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'

    # If you are using self-hosted instance:
    # export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5173/api/v1/private/otel
    ```

    </Tab>

</Tabs>

<Tip>
  Opik is open-source and can be hosted locally using Docker, please refer to the [self-hosting
  guide](/self-host/overview) to get started. Alternatively, you can use our hosted platform by creating an account on
  [Comet](https://www.comet.com/signup?from=llm).
</Tip>

### 2. Using an integration

Once you have installed and configured the Opik SDK, you can start using it to track your agent calls:

<Tabs>
  <Tab title="OpenAI (TS)" value="openai-ts-sdk" language="typescript">
    If you are using the OpenAI TypeScript SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik TypeScript SDK:

        ```bash
        npm install opik-openai
        ```
      </Step>
      <Step>
        Configure the Opik TypeScript SDK using environment variables:

        ```bash
        export OPIK_API_KEY="<your-api-key>" # Only required if you are using the Opik Cloud version
        export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>
      <Step>
        Wrap your OpenAI client with the `trackOpenAI` function:

        ```typescript
        import OpenAI from "openai";
        import { trackOpenAI } from "opik-openai";

        // Initialize the original OpenAI client
        const openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
        });

        // Wrap the client with Opik tracking
        const trackedOpenAI = trackOpenAI(openai);

        // Use the tracked client just like the original
        const completion = await trackedOpenAI.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: "Hello, how can you help me today?" }],
        });
        console.log(completion.choices[0].message.content);

        // Ensure all traces are sent before your app terminates
        await trackedOpenAI.flush();
        ```

        All OpenAI calls made using the `trackedOpenAI` will now be logged to Opik.

      </Step>
    </Steps>

  </Tab>
  <Tab title="OpenAI (Python)" value="openai-python-sdk" language="python">
    If you are using the OpenAI Python SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik Python SDK, this will prompt you for your API key if you are using Opik
        Cloud or your Opik server address if you are self-hosting:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your OpenAI client with the `track_openai` function:

        ```python
        from opik.integrations.openai import track_openai
        from openai import OpenAI

        # Wrap your OpenAI client
        openai_client = OpenAI()
        openai_client = track_openai(openai_client)
        ```

        All OpenAI calls made using the `openai_client` will now be logged to Opik.

      </Step>
    </Steps>

  </Tab>
  <Tab title="AI Vercel SDK" value="ai-vercel-sdk" language="typescript">
    If you are using the AI Vercel SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik AI Vercel SDK:

        ```bash
        npm install opik
        ```
      </Step>
      <Step>
        Configure the Opik AI Vercel SDK using environment variables and set your Opik API key:

        ```bash
        export OPIK_API_KEY="<your-api-key>"
        export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>
      <Step>
        Initialize the OpikExporter with your AI SDK:

        ```ts
        import { openai } from "@ai-sdk/openai";
        import { generateText } from "ai";
        import { NodeSDK } from "@opentelemetry/sdk-node";
        import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
        import { OpikExporter } from "opik/vercel";

        // Set up OpenTelemetry with Opik
        const sdk = new NodeSDK({
          traceExporter: new OpikExporter(),
          instrumentations: [getNodeAutoInstrumentations()],
        });
        sdk.start();

        // Your AI SDK calls with telemetry enabled
        const result = await generateText({
          model: openai("gpt-4o"),
          prompt: "What is love?",
          experimental_telemetry: { isEnabled: true },
        });

        console.log(result.text);
        ```

        All AI SDK calls with `experimental_telemetry: { isEnabled: true }` will now be logged to Opik.
      </Step>
    </Steps>

  </Tab>
  <Tab title="ADK" value="adk-python" language="python">
    If you are using the ADK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your ADK agent with the `OpikTracer` decorator:

        ```python
        from opik.integrations.adk import OpikTracer, track_adk_agent_recursive

        opik_tracer = OpikTracer()

        # Define your ADK agent

        # Wrap your ADK agent with the OpikTracer
        track_adk_agent_recursive(agent, opik_tracer)
        ```

        All ADK agent calls will now be logged to Opik.
      </Step>
    </Steps>

  </Tab>
  <Tab title="LangGraph" value="langgraph" language="python">
    If you are using LangGraph, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your LangGraph graph with the `OpikTracer` decorator:

        ```python
        from opik.integrations.langchain import OpikTracer

        # Create your LangGraph graph
        graph = ...
        app = graph.compile(...)

        # Wrap your LangGraph graph with the OpikTracer
        opik_tracer = OpikTracer(graph=app.get_graph(xray=True))

        # Pass the OpikTracer callback to the invoke functions
        result = app.invoke({"messages": [HumanMessage(content = "How to use LangGraph ?")]},
                      config={"callbacks": [opik_tracer]})
        ```

        All LangGraph calls will now be logged to Opik.
      </Step>
    </Steps>

  </Tab>
  <Tab title="Function Decorators" value="python-function-decorator" language="python">
    If you are using the Python function decorator, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik Python SDK:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your function with the `@track` decorator:

        ```python
        from opik import track

        @track
        def my_function(input: str) -> str:
            return input
        ```

        All calls to the `my_function` will now be logged to Opik. This works well for any function
        even nested ones and is also supported by most integrations (just wrap any parent function
        with the `@track` decorator).
      </Step>
    </Steps>

  </Tab>
  <Tab title="AI Wizard" value="ai-installation" language="other">
    The AI integration wizard will get you setup with Opik in just a few minutes with just one
    prompt !

    <Steps>
      <Step title="1. Install the Opik extension for Cursor">
      We will first install the Opik extension for Cursor that will both setup the Opik MCP server
      for you and also log your Cursor conversations to Opik. You can learn more about the extension
      [here](/tracing/integrations/cursor):

      <Button intent="primary" href="cursor:extension/opik.opik">Install Cursor extension</Button>

      </Step>
      <Step title="2. Run the following prompt in the Cursor chat">
        Once it is installed, you can run the following prompt in the Cursor chat (make sure to run
        in agent mode so tools can be called):

        ```text
        Integrate Opik with my project.
        ```

        Once the integration is complete, you will need to setup your Opik Cloud API key or point
        to your self-hosted Opik server. You can learn more about the configuration
        [here](/tracing/sdk_configuration).

      </Step>
      <Step title="3. (Optional) Manually install the Opik MCP server">
            If you are using another AI assistant, first install the Opik MCP server following the
      instructions in the [MCP Server Integration](/prompt_engineering/mcp_server) documentation and
      then run the prompt above.
      </Step>
    </Steps>

  </Tab>
  <Tab title="Other" value="other" language="other">
    Opik has more than 30 integrations with the most popular frameworks and libraries, you can find
    a full list of integrations [here](/integrations/overview). For example:

    - [Dify](/integrations/dify)
    - [Agno](/integrations/agno)
    - [Ollama](/integrations/ollama)

    If you are using a framework or library that is not listed, you can still log your traces
    using either the function decorator or the Opik client, check out the
    [Log Traces](/tracing/log_traces) guide for more information.

  </Tab>
</Tabs>

<Tip>
  Opik has more than 40 integrations with the majority of the popular frameworks and libraries. You can find a full list
  of integrations in the integrations [overview page](/integrations/overview).
</Tip>

If you would like more control over the logging process, you can use the low-level SDKs to log
your traces and spans.

### 3. Analyzing your agents

Now that you have observability enabled for your agents, you can start to review and analyze the
agent calls in Opik. In the Opik UI, you can review each agent call, see the
[agent graph](/tracing/log_agent_graph) and review all the tool calls made by the agent.

<Frame>
  <img src="/img/tracing/tracing_agent_overview.png" />
</Frame>

As a next step, you can create an [offline evaluation](/evaluation/evaluate_prompt) to evaluate your
agent's performance on a fixed set of samples.

## Advanced usage

### Using function decorators

Function decorators are a great way to add Opik logging to your existing application. When you add
the `@track` decorator to a function, Opik will create a span for that function call and log the
input parameters and function output for that function. If we detect that a decorated function
is being called within another decorated function, we will create a nested span for the inner
function.

While decorators are most popular in Python, we also support them in our Typescript SDK:

<Tabs>
    <Tab title="Typescript" value="typescript" language="typescript">
        TypeScript started supporting decorators from version 5 but it's use is still not widespread.
        The Opik typescript SDK also supports decorators but it's currently considered experimental.

        ```typescript maxLines=100
        import { track } from "opik";

        class TranslationService {
            @track({ type: "llm" })
            async generateText() {
                // Your LLM call here
                return "Generated text";
            }

            @track({ name: "translate" })
            async translate(text: string) {
                // Your translation logic here
                return `Translated: ${text}`;
            }

            @track({ name: "process", projectName: "translation-service" })
            async process() {
                const text = await this.generateText();
                return this.translate(text);
            }
        }
    ```

    <Info>
        You can also specify custom `tags`, `metadata`, and/or a `thread_id` for each trace and/or
        span logged for the decorated function. For more information, see
        [Logging additional data using the opik_args parameter](#logging-additional-data)
    </Info>

    </Tab>
    <Tab title="Python" value="python" language="python">
        You can add the `@track` decorator to any function in your application and track not just
        LLM calls but also any other steps in your application:

        ```python maxLines=100
        import opik
        import openai

        client = openai.OpenAI()

        @opik.track
        def retrieve_context(input_text):
            # Your retrieval logic here, here we are just returning a
            # hardcoded list of strings
            context =[
                "What specific information are you looking for?",
                "How can I assist you with your interests today?",
                "Are there any topics you'd like to explore?",
            ]
            return context

        @opik.track
        def generate_response(input_text, context):
            full_prompt = (
                f" If the user asks a non-specific question, use the context to provide a relevant response.\n"
                f"Context: {', '.join(context)}\n"
                f"User: {input_text}\n"
                f"AI:"
            )

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": full_prompt}]
            )
            return response.choices[0].message.content

        @opik.track(name="my_llm_application")
        def llm_chain(input_text):
            context = retrieve_context(input_text)
            response = generate_response(input_text, context)

            return response

        # Use the LLM chain
        result = llm_chain("Hello, how are you?")
        print(result)
        ```

        When using the track decorator, you can customize the data associated with both the trace
        and the span using either the `opik_args` parameter or the
        [`opik_context`](https://www.comet.com/docs/opik/python-sdk-reference/opik_context/index.html)
        module. This is particularly useful if you want to specify the conversation thread id, tags
        and metadata for example.

        <CodeBlocks>
            ```python title="opik_context module"
            import opik

            @opik.track
            def llm_chain(text: str) -> str:
                opik_context.update_current_trace(
                    tags=["llm_chatbot"],
                    metadata={"version": "1.0", "method": "simple"},
                    thread_id="conversation-123",
                    feedback_scores=[
                        {
                            "name": "user_feedback",
                            "value": 1
                        }
                    ],
                )
                opik_context.update_current_span(
                    metadata={"model": "gpt-4o"},
                )
                return f"Processed: {text}"
            ```

            ```python title="opik_args parameter"
            import opik

            @opik.track
            def llm_chain(text: str) -> str:
                # LLM chain code
                # ...
                return f"Processed: {text}"

            # Call with opik_args - it won't be passed to the function
            result = llm_chain(
                "hello world",
                opik_args={
                    "span": {
                        "tags": ["llm", "agent"],
                        "metadata": {"version": "1.0", "method": "simple"}
                    },
                    "trace": {
                        "thread_id": "conversation-123",
                        "tags": ["user-session"],
                        "metadata": {"user_id": "user-456"}
                    }
                }
            )

            print(result)
            ```
        </CodeBlocks>

        <Tip>
            If you specify the opik_args parameter as part of your function call, you can propagate
            the configuration to the nested functions.
        </Tip>
    </Tab>

</Tabs>

### Using the low-level SDKs

If you need full control over the logging process, you can use the low-level SDKs to log your traces and spans:

<Tabs>
    <Tab title="Typescript" value="typescript" language="typescript">
        You can use the [`Opik`](/reference/typescript-sdk/overview) client to log your traces and spans:

        ```typescript
        import { Opik } from "opik";

        const client = new Opik({
            apiUrl: "https://www.comet.com/opik/api",
            apiKey: "your-api-key", // Only required if you are using Opik Cloud
            projectName: "your-project-name",
            workspaceName: "your-workspace-name", // Only required if you are using Opik Cloud
        });

        // Log a trace with an LLM span
        const trace = client.trace({
            name: `Trace`,
            input: {
                prompt: `Hello!`,
            },
            output: {
                response: `Hello, world!`,
            },
        });

        const span = trace.span({
            name: `Span`,
            type: "llm",
            input: {
                prompt: `Hello, world!`,
            },
            output: {
                response: `Hello, world!`,
            },
        });

        // Flush the client to send all traces and spans
        await client.flush();
        ```

        <Tip>
            Make sure you define the environment variables for the Opik client in your `.env` file,
            you can find more information about the configuration [here](/tracing/sdk_configuration).
        </Tip>
    </Tab>
    <Tab title="Python" value="python" language="python">
        If you want full control over the data logged to Opik, you can use the
        [`Opik`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html) client.


        Logging traces and spans can be achieved by first creating a trace using
        [`Opik.trace`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.trace)
        and then adding spans to the trace using the
        [`Trace.span`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/Trace.html#opik.api_objects.trace.Trace.span)
        method:

        ```python
        from opik import Opik

        client = Opik(project_name="Opik client demo")

        # Create a trace
        trace = client.trace(
            name="my_trace",
            input={"user_question": "Hello, how are you?"},
            output={"response": "Comment ça va?"}
        )

        # Add a span
        trace.span(
            name="Add prompt template",
            input={"text": "Hello, how are you?", "prompt_template": "Translate the following text to French: {text}"},
            output={"text": "Translate the following text to French: hello, how are you?"}
        )

        # Add an LLM call
        trace.span(
            name="llm_call",
            type="llm",
            input={"prompt": "Translate the following text to French: hello, how are you?"},
            output={"response": "Comment ça va?"}
        )

        # End the trace
        trace.end()
        ```

        <Note>
        It is recommended to call `trace.end()` and `span.end()` when you are finished with the trace and span to ensure that
        the end time is logged correctly.
        </Note>

        Opik's logging functionality is designed with production environments in mind. To optimize
        performance, all logging operations are executed in a background thread.

        If you want to ensure all traces are logged to Opik before exiting your program, you can use the `opik.Opik.flush` method:

        ```python
        from opik import Opik

        client = Opik()

        # Log some traces
        client.flush()
        ```

    </Tab>

</Tabs>

### Logging to a specific project

By default, traces are logged to the `Default Project` project. You can change the project you want
the trace to be logged to in a couple of ways:

<Tabs>
    <Tab title="Typescript" value="typescript" language="typescript">
    You can use the `OPIK_PROJECT_NAME` environment variable to set the project you want the trace
    to be logged or pass a parameter to the `Opik` client.

    ```typescript
    import { Opik } from "opik";

    const client = new Opik({
        projectName: "my_project",
        // apiKey: "my_api_key",
        // apiUrl: "https://www.comet.com/opik/api",
        // workspaceName: "my_workspace",
    });
    ```
    </Tab>
    <Tab title="Python" value="python" language="python">
        You can use the `OPIK_PROJECT_NAME` environment variable to set the project you want traces
        to be logged to.

        If you are using function decorators, you can set the project as part of the decorator parameters:

        ```python
        @track(project_name="my_project")
        def my_function():
            pass
        ```

        If you are using the low level SDK, you can set the project as part of the `Opik` client constructor:

        ```python
        from opik import Opik

        client = Opik(project_name="my_project")
        ```
    </Tab>

</Tabs>

### Flushing traces and spans

This process is optional and is only needed if you are running a short-lived script or if you are
debugging why traces and spans are not being logged to Opik.

<Tabs>
    <Tab title="Typescript" value="typescript" language="typescript">
        As the Typescript SDK has been designed to be used in production environments, we batch traces
        and spans and send them to Opik in the background.

        If you are running a short-lived script, you can flush the traces to Opik by using the
        `flush` method of the `Opik` client.

        ```typescript
        import { Opik } from "opik";

        const client = new Opik();
        client.flush();
        ```
    </Tab>
    <Tab title="Python" value="python" language="python">
        As the Python SDK has been designed to be used in production environments, we batch traces
        and spans and send them to Opik in the background.

        If you are running a short-lived script, you can flush the traces to Opik by using the
        `flush` method of the `Opik` client.

        ```python maxLines=100
        from opik import Opik

        client = Opik()
        client.flush()
        ```

        You can also set the `flush` parameter to `True` when you are using the `@track` decorator to make sure
        the traces are flushed to Opik before the program exits.

        ```python
        from opik import track

        @track(flush=True)
        def llm_chain(input_text):
            # LLM chain code
            # ...
            return f"Processed: {input_text}"
        ```
    </Tab>

</Tabs>

### Disabling the logging process

<Tabs>
    <Tab title="Typescript" value="typescript" language="typescript">
        This is currently not supported in the Typescript SDK. To disable the logging process, 
    </Tab>
    <Tab title="Python" value="python" language="python">
        You can disable the logging process globally using the `OPIK_TRACK_DISABLE` environment variable.

        If you are looking for more control, you can also use the `set_tracing_active` function to
        dynamically disable the logging process.

        ```python
        import opik

        # Check the current state of the tracing flag
        print(opik.is_tracing_active())

        # Disable the logging process
        opik.set_tracing_active(False)

        # re-enable the logging process
        print(opik.set_tracing_active(True))
        ```
    </Tab>

</Tabs>

## Next steps

Once you have the observability set up for your agent, you can go one step further and:

- [Logging chat conversations](/tracing/log_chat_conversations)
- [Logging user feedback](/tracing/log_feedback)
- [Setup online evaluation metrics](/production/rules)
