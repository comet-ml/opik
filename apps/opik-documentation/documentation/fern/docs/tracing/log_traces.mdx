<Tip>
  If you are just getting started with Opik, we recommend first checking out the [Quickstart](/quickstart) guide that
  will walk you through the process of logging your first LLM call.
</Tip>

LLM applications are complex systems that do more than just call an LLM API, they will often involve retrieval, pre-processing and post-processing steps.
Tracing is a tool that helps you understand the flow of your application and identify specific points in your application that may be causing issues.

Opik's tracing functionality allows you to track not just all the LLM calls made by your application but also any of the other steps involved.

<Frame>
  <img src="/img/tracing/introduction.png" />
</Frame>

Opik supports agent observability using our [Typescript SDK](/reference/typescript-sdk/overview),
[Python SDK](/reference/python-sdk/overview), [first class OpenTelemetry support](/integrations/opentelemetry)
and our [REST API](/reference/rest-api/overview).

<Tip>
  We recommend starting with one of our integrations to get started quickly, you can find a full list of our
  integrations in the [integrations overview](/integrations/overview) page.
</Tip>

We won't be covering how to track chat conversations in this guide, you can learn more about this in the
[Logging conversations](/tracing/log_chat_conversations) guide.

## Enabling observability

### 1. Installing the SDK

Before adding observability to your application, you will first need to install and configure the
Opik SDK.

<Tabs>
    <Tab value="Typescript SDK" title="Typescript SDK">

    ```bash
    npm install opik
    ```

    You can then set the Opik environment variables in your `.env` file:

    ```bash
    # Set OPIK_API_KEY and OPIK_WORKSPACE_NAME in your .env file
    OPIK_API_KEY=your_api_key_here
    OPIK_WORKSPACE=your_workspace_name

    # Optional if you are using Opik Cloud:
    OPIK_URL_OVERRIDE=https://www.comet.com/opik/api
    ```

    </Tab>
    <Tab value="Python SDK" title="Python SDK">

    ```bash
    # Install the SDK
    pip install opik
    ```

    You can then configure the SDK using the `opik configure` CLI command or by calling
    [`opik.configure`](https://www.comet.com/docs/opik/python-sdk-reference/configure.html) from
    your Jupyter Notebook.

    </Tab>
    <Tab value="OpenTelemetry" title="OpenTelemetry">

    You will need to set the following environment variables for your OpenTelemetry setup:

    ```bash
    export OTEL_EXPORTER_OTLP_ENDPOINT=https://www.comet.com/opik/api/v1/private/otel
    export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'

    # If you are using self-hosted instance:
    # export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5173/api/v1/private/otel
    ```

    </Tab>

</Tabs>

<Tip>
  Opik is open-source and can be hosted locally using Docker, please refer to the [self-hosting
  guide](/self-host/overview) to get started. Alternatively, you can use our hosted platform by creating an account on
  [Comet](https://www.comet.com/signup?from=llm).
</Tip>

### 2. Using an integration

Once you have installed and configured the Opik SDK, you can start using it to track your agent calls:

<Tabs>
  <Tab title="OpenAI (TS)" value="openai-ts-sdk">
    If you are using the OpenAI TypeScript SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik TypeScript SDK:

        ```bash
        npm install opik-openai
        ```
      </Step>
      <Step>
        Configure the Opik TypeScript SDK using environment variables:

        ```bash
        export OPIK_API_KEY="<your-api-key>" # Only required if you are using the Opik Cloud version
        export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>
      <Step>
        Wrap your OpenAI client with the `trackOpenAI` function:

        ```typescript
        import OpenAI from "openai";
        import { trackOpenAI } from "opik-openai";

        // Initialize the original OpenAI client
        const openai = new OpenAI({
          apiKey: process.env.OPENAI_API_KEY,
        });

        // Wrap the client with Opik tracking
        const trackedOpenAI = trackOpenAI(openai);

        // Use the tracked client just like the original
        const completion = await trackedOpenAI.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: "Hello, how can you help me today?" }],
        });
        console.log(completion.choices[0].message.content);

        // Ensure all traces are sent before your app terminates
        await trackedOpenAI.flush();
        ```

        All OpenAI calls made using the `trackedOpenAI` will now be logged to Opik.

      </Step>
    </Steps>

  </Tab>
  <Tab title="OpenAI (Python)" value="openai-python-sdk">
    If you are using the OpenAI Python SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik Python SDK, this will prompt you for your API key if you are using Opik
        Cloud or your Opik server address if you are self-hosting:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your OpenAI client with the `track_openai` function:

        ```python
        from opik.integrations.openai import track_openai
        from openai import OpenAI

        # Wrap your OpenAI client
        openai_client = OpenAI()
        openai_client = track_openai(openai_client)
        ```

        All OpenAI calls made using the `openai_client` will now be logged to Opik.

      </Step>
    </Steps>

  </Tab>
  <Tab title="AI Vercel SDK" value="ai-vercel-sdk">
    If you are using the AI Vercel SDK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik AI Vercel SDK:

        ```bash
        npm install opik
        ```
      </Step>
      <Step>
        Configure the Opik AI Vercel SDK using environment variables and set your Opik API key:

        ```bash
        export OPIK_API_KEY="<your-api-key>"
        export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
        # export OPIK_URL_OVERRIDE="http://localhost:5173/api" # Self-hosting
        ```
      </Step>
      <Step>
        Initialize the OpikExporter with your AI SDK:

        ```ts
        import { openai } from "@ai-sdk/openai";
        import { generateText } from "ai";
        import { NodeSDK } from "@opentelemetry/sdk-node";
        import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
        import { OpikExporter } from "opik/vercel";

        // Set up OpenTelemetry with Opik
        const sdk = new NodeSDK({
          traceExporter: new OpikExporter(),
          instrumentations: [getNodeAutoInstrumentations()],
        });
        sdk.start();

        // Your AI SDK calls with telemetry enabled
        const result = await generateText({
          model: openai("gpt-4o"),
          prompt: "What is love?",
          experimental_telemetry: { isEnabled: true },
        });

        console.log(result.text);
        ```

        All AI SDK calls with `experimental_telemetry: { isEnabled: true }` will now be logged to Opik.
      </Step>
    </Steps>

  </Tab>
  <Tab title="ADK" value="adk-python">
    If you are using the ADK, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your ADK agent with the `OpikTracer` decorator:

        ```python
        from opik.integrations.adk import OpikTracer, track_adk_agent_recursive

        opik_tracer = OpikTracer()

        # Define your ADK agent

        # Wrap your ADK agent with the OpikTracer
        track_adk_agent_recursive(agent, opik_tracer)
        ```

        All ADK agent calls will now be logged to Opik.
      </Step>
    </Steps>

  </Tab>
  <Tab title="LangGraph" value="langgraph">
    If you are using LangGraph, you can integrate by:

    <Steps>
      <Step>
        Install the Opik SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik SDK by running the `opik configure` command in your terminal:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your LangGraph graph with the `OpikTracer` decorator:

        ```python
        from opik.integrations.langchain import OpikTracer

        # Create your LangGraph graph
        graph = ...
        app = graph.compile(...)

        # Wrap your LangGraph graph with the OpikTracer
        opik_tracer = OpikTracer(graph=app.get_graph(xray=True))

        # Pass the OpikTracer callback to the invoke functions
        result = app.invoke({"messages": [HumanMessage(content = "How to use LangGraph ?")]},
                      config={"callbacks": [opik_tracer]})
        ```

        All LangGraph calls will now be logged to Opik.
      </Step>
    </Steps>

  </Tab>
  <Tab title="Function Decorators" value="python-function-decorator">
    If you are using the Python function decorator, you can integrate by:

    <Steps>
      <Step>
        Install the Opik Python SDK:

        ```bash
        pip install opik
        ```
      </Step>
      <Step>
        Configure the Opik Python SDK:

        ```bash
        opik configure
        ```
      </Step>
      <Step>
        Wrap your function with the `@track` decorator:

        ```python
        from opik import track

        @track
        def my_function(input: str) -> str:
            return input
        ```

        All calls to the `my_function` will now be logged to Opik. This works well for any function
        even nested ones and is also supported by most integrations (just wrap any parent function
        with the `@track` decorator).
      </Step>
    </Steps>

  </Tab>
  <Tab title="AI Wizard" value="ai-installation">
    The AI integration wizard will get you setup with Opik in just a few minutes with just one
    prompt !

    <Steps>
      <Step title="1. Install the Opik extension for Cursor">
      We will first install the Opik extension for Cursor that will both setup the Opik MCP server
      for you and also log your Cursor conversations to Opik. You can learn more about the extension
      [here](/tracing/integrations/cursor):

      <Button intent="primary" href="cursor:extension/opik.opik">Install Cursor extension</Button>

      </Step>
      <Step title="2. Run the following prompt in the Cursor chat">
        Once it is installed, you can run the following prompt in the Cursor chat (make sure to run
        in agent mode so tools can be called):

        ```text
        Integrate Opik with my project.
        ```

        Once the integration is complete, you will need to setup your Opik Cloud API key or point
        to your self-hosted Opik server. You can learn more about the configuration
        [here](/tracing/sdk_configuration).

      </Step>
      <Step title="3. (Optional) Manually install the Opik MCP server">
            If you are using another AI assistant, first install the Opik MCP server following the
      instructions in the [MCP Server Integration](/prompt_engineering/mcp_server) documentation and
      then run the prompt above.
      </Step>
    </Steps>

  </Tab>
  <Tab title="Other" value="other">
    Opik has more than 30 integrations with the most popular frameworks and libraries, you can find
    a full list of integrations [here](/integrations/overview). For example:

    - [Dify](/integrations/dify)
    - [Agno](/integrations/agno)
    - [Ollama](/integrations/ollama)

    If you are using a framework or library that is not listed, you can still log your traces
    using either the function decorator or the Opik client, check out the
    [Log Traces](/tracing/log_traces) guide for more information.

  </Tab>
</Tabs>

<Tip>
  Opik has more than 40 integrations with the majority of the popular frameworks and libraries. You can find a full list
  of integrations in the integrations [overview page](/integrations/overview).
</Tip>

If you would like more control over the logging process, you can use the low-level SDKs to log
your traces and spans.

## Advanced usage

### Logging traces and spans

You might not always be able to use one of our integrations or you might need more control over the
logging process. In this case, you can use the low level SDKs to log your traces and spans.

#### Using function decorators

Function decorators are a great way to add Opik logging to your existing application. When you add
the `@track` decorator to a function, Opik will create a span for that function call and log the
input parameters and function output for that function. If we detect that a decorated function
is being called within another decorated function, we will create a nested span for the inner
function.

While decorators are most popular in Python, we also support them in our Typescript SDK:

<Tabs>
    <Tab title="Python" value="python">
        You can add the `@track` decorator to any function in your application and track not just
        LLM calls but also any other steps in your application:

        ```python
        import opik
        import openai

        client = openai.OpenAI()

        @opik.track
        def retrieve_context(input_text):
            # Your retrieval logic here, here we are just returning a hardcoded list of strings
            context =[
                "What specific information are you looking for?",
                "How can I assist you with your interests today?",
                "Are there any topics you'd like to explore or learn more about?",
            ]
            return context

        @opik.track
        def generate_response(input_text, context):
            full_prompt = (
                f" If the user asks a question that is not specific, use the context to provide a relevant response.\n"
                f"Context: {', '.join(context)}\n"
                f"User: {input_text}\n"
                f"AI:"
            )

            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": full_prompt}]
            )
            return response.choices[0].message.content

        @opik.track(name="my_llm_application")
        def llm_chain(input_text):
            context = retrieve_context(input_text)
            response = generate_response(input_text, context)

            return response

        # Use the LLM chain
        result = llm_chain("Hello, how are you?")
        print(result)
        ```
    </Tab>
    <Tab title="Typescript" value="typescript">
        TypeScript started supporting decorators from version 5 but it's use is still not widespread.
        The Opik typescript SDK also supports decorators but it's currently considered experimental.

        ```typescript
        import { track } from "opik";

        class TranslationService {
            @track({ type: "llm" })
            async generateText() {
                // Your LLM call here
                return "Generated text";
            }

            @track({ name: "translate" })
            async translate(text: string) {
                // Your translation logic here
                return `Translated: ${text}`;
            }

            @track({ name: "process", projectName: "translation-service" })
            async process() {
                const text = await this.generateText();
                return this.translate(text);
            }
        }
    ```

    <Info>
        You can also specify custom `tags`, `metadata`, and/or a `thread_id` for each trace and/or
        span logged for the decorated function. For more information, see
        [Logging additional data using the opik_args parameter](#logging-additional-data)
    </Info>

    </Tab>

</Tabs>

#### Using the low-level SDKs

If you need full control over the logging process, you can use the low-level SDKs to log your traces and spans:

<Tabs>
    <Tab title="Typescript" value="typescript">
        You can use the [`Opik`](/reference/typescript-sdk/overview) client to log your traces and spans:

        ```typescript
        import { Opik } from "opik";

        const client = new Opik();

        // Log a trace with an LLM span
        const trace = client.trace({
            name: `Trace`,
            input: {
                prompt: `Hello!`,
            },
            output: {
                response: `Hello, world!`,
            },
        });

        const span = trace.span({
            name: `Span`,
            type: "llm",
            input: {
                prompt: `Hello, world!`,
            },
            output: {
                response: `Hello, world!`,
            },
        });

        // Flush the client to send all traces and spans
        await client.flush();
        ```

        <Tip>
            Make sure you define the environment variables for the Opik client in your `.env` file,
            you can find more information about the configuration [here](/tracing/sdk_configuration).
        </Tip>
    </Tab>
    <Tab title="Python" value="python">
        If you want full control over the data logged to Opik, you can use the
        [`Opik`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html) client.


        Logging traces and spans can be achieved by first creating a trace using
        [`Opik.trace`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.trace)
        and then adding spans to the trace using the
        [`Trace.span`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/Trace.html#opik.api_objects.trace.Trace.span)
        method:

        ```python
        from opik import Opik

        client = Opik(project_name="Opik client demo")

        # Create a trace
        trace = client.trace(
            name="my_trace",
            input={"user_question": "Hello, how are you?"},
            output={"response": "Comment ça va?"}
        )

        # Add a span
        trace.span(
            name="Add prompt template",
            input={"text": "Hello, how are you?", "prompt_template": "Translate the following text to French: {text}"},
            output={"text": "Translate the following text to French: hello, how are you?"}
        )

        # Add an LLM call
        trace.span(
            name="llm_call",
            type="llm",
            input={"prompt": "Translate the following text to French: hello, how are you?"},
            output={"response": "Comment ça va?"}
        )

        # End the trace
        trace.end()
        ```

        <Note>
        It is recommended to call `trace.end()` and `span.end()` when you are finished with the trace and span to ensure that
        the end time is logged correctly.
        </Note>

        Opik's logging functionality is designed with production environments in mind. To optimize
        performance, all logging operations are executed in a background thread.

        If you want to ensure all traces are logged to Opik before exiting your program, you can use the `opik.Opik.flush` method:

        ```python
        from opik import Opik

        client = Opik()

        # Log some traces
        client.flush()
        ```

    </Tab>

</Tabs>

### Scoring traces

Feedback scores allow you to annotate your traces based on your own metrics or feedback from your
users.

<Tabs>
    <Tab title="Function decorators (Python)" value="python">
        If you are using the `@track` decorator, you can log feedback scores to traces by using the
        [`opik_context.update_current_trace`](https://www.comet.com/docs/opik/python-sdk-reference/opik_context/update_current_trace.html) and 
        [`opik_context.update_current_span`](https://www.comet.com/docs/opik/python-sdk-reference/opik_context/update_current_span.html)
        methods:

        ```python
        from opik import track, opik_context

        @track
        def llm_chain(input_text):
            # LLM chain code
            # ...

            # Update the trace
            opik_context.update_current_trace(
                feedback_scores=[
                    {"name": "user_feedback", "value": 1.0, "reason": "The response was helpful and accurate."}
                ]
            )
        ```

    </Tab>
    <Tab title="Python low-level SDK" value="python">

        You can log scores to traces and spans using the
        [`log_traces_feedback_scores`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.log_traces_feedback_scores)
        and [`log_spans_feedback_scores`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.log_spans_feedback_scores)
        methods:

        ```python
        from opik import Opik

        client = Opik()

        trace = client.trace(name="my_trace")

        client.log_traces_feedback_scores(
            scores=[
                {"id": trace.id, "name": "overall_quality", "value": 0.85, "reason": "The response was helpful and accurate."},
                {"id": trace.id, "name": "coherence", "value": 0.75}
            ]
        )

        span = trace.span(name="my_span")
        client.log_spans_feedback_scores(
            scores=[
                {"id": span.id, "name": "overall_quality", "value": 0.85, "reason": "The response was helpful and accurate."},
                {"id": span.id, "name": "coherence", "value": 0.75}
            ]
        )
        ```
    </Tab>

</Tabs>

<Tip>
You don't have to manually log feedback scores, you can also define LLM as a Judge metrics in Opik
that will score traces automatically for you.

You can learn more about this feature in the [Online evaluation](/production/rules) guide.

</Tip>
{/* 
#### Logging additional data

As mentioned above, the `@track` decorator only logs the input and output of the decorated function. If you want to log additional data, you can use the
`update_current_span` function and `update_current_trace` function to manually update the span and trace:

```python
from opik import track, opik_context

@track
def llm_chain(input_text):
    # LLM chain code
    # ...

    # Update the trace
    opik_context.update_current_trace(
        tags=["llm_chatbot"],
    )

    # Update the span
    opik_context.update_current_span(
        name="llm_chain"
    )
```

You can learn more about the `opik_context` module in the [opik_context reference docs](https://www.comet.com/docs/opik/python-sdk-reference/opik_context/index.html).

Also, you can use the `opik_args` parameter to pass additional configuration to traced functions, including `tags`, `metadata`, and trace-level settings like `thread_id`.
This parameter can be used in two ways: implicitly or explicitly.

##### Implicit `opik_args` (functions without `opik_args` parameter)

When your function doesn't explicitly declare an `opik_args` parameter, you can still pass `opik_args` when calling the function. The decorator will handle it internally:

```python
import opik

@opik.track
def llm_chain(text: str) -> str:
    # LLM chain code
    # ...
    return f"Processed: {text}"

# Call with opik_args - it won't be passed to the function
result = llm_chain(
    "hello world",
    opik_args={
        "span": {
            "tags": ["llm", "agent"],
            "metadata": {"version": "1.0", "method": "simple"}
        },
        "trace": {
            "thread_id": "conversation-123",
            "tags": ["user-session"],
            "metadata": {"user_id": "user-456"}
        }
    }
)
```

##### Explicit `opik_args` (functions with `opik_args` parameter)

When your function explicitly declares an `opik_args` parameter, it will receive the `opik_args` value directly. This is useful for propagating configuration through nested function calls:

```python
from typing import Optional, Dict, Any
import opik

@opik.track
def child_function(data: str, opik_args: Optional[Dict[str, Any]]=None) -> str:
    # This function can access opik_args directly if needed.
    # All configuration options will be applied to the current span and trace
    return f"Child processed: {data}"

@opik.track
def parent_function(text: str, opik_args: Optional[Dict[str, Any]]=None) -> str:
    # Propagate opik_args to a child function.
    # All configuration options will be applied to the current span and trace.
    intermediate = child_function(text, opik_args=opik_args)
    return f"Parent: {intermediate}"

# Both functions will receive the same opik_args and apply the same configuration
# to the current span and trace of each function.
result = parent_function(
    "hello",
    opik_args={
        "span": {"tags": ["parent-child"]},
        "trace": {"thread_id": "workflow-789"}
    }
)
```

##### The `opik_args` structure

The `opik_args` parameter accepts a dictionary with the following structure:

```python
opik_args = {
    "span": {
        "tags": ["tag1", "tag2"],              # List of tags for the span
        "metadata": {"key": "value"}           # Dictionary of metadata for the span
    },
    "trace": {
        "thread_id": "conversation-id",        # Thread ID for grouping related traces
        "tags": ["trace-tag"],                 # List of tags for the trace
        "metadata": {"trace-key": "value"}     # Dictionary of metadata for the trace
    }
}
```

Both `"span"` and `"trace"` sections are optional - you can include either or both as needed.

#### Configuring the project name

You can configure the project you want the trace to be logged to using the `project_name` parameter of the `@track` decorator:

```python
import opik

@opik.track(project_name="my_project")
def my_function(input):
    # Function code
    return input
```

If you want to configure this globally for all traces, you can also use the environment variable:

```python
import os

os.environ["OPIK_PROJECT_NAME"] = "my_project"
```

This will block the processing until the data is finished being logged.

#### Flushing the trace

You can ensure all data is logged by setting the `flush` parameter of the `@track` decorator to `True`:

```python
import opik

@opik.track(flush=True)
def my_function(input):
    # Function code
    return input
```

#### Disabling automatic logging of function input and output

You can use the `capture_input` and `capture_output` parameters of the [`@track`](https://www.comet.com/docs/opik/python-sdk-reference/track.html) decorator to disable the automatic logging of the function input and output:

```python
import opik

@opik.track(capture_input=False, capture_output=False)
def llm_chain(input_text):
    # LLM chain code
    return input_text
```

You can then use the `opik_context` module to manually log the trace and span attributes.

#### Disable all tracing

You can disable the logging of traces and spans using the environment variable `OPIK_TRACK_DISABLE`, this will turn off the logging for all function decorators:

```python
import os

os.environ["OPIK_TRACK_DISABLE"] = "true"
```

### Configuring the `thread_id`

You can group multiple traces into a single conversational thread by updating each created trace with a specific `thread_id` parameter.
Often, this isn’t possible directly because tracing occurs inside a function you can’t modify. In such cases, you can use the `opik_args` parameter to pass the `thread_id` to the function, as shown below:

```python
from google import genai
from opik.integrations.genai import track_genai

client = genai.Client()
gemini_client = track_genai(client, project_name="opik_args demo")

response = gemini_client.models.generate_content(
    model="gemini-2.0-flash",
    contents="What is Opik?",
    opik_args={"trace": {"thread_id": "f174a"}}
)

print(response.text)

```

Also, you can pass it to the function with `opik.track` decorator:

```python
from google import genai

import opik
from opik.integrations.genai import track_genai

client = genai.Client()
gemini_client = track_genai(client)

@opik.track(project_name="opik_args demo")
def chat_message(input_text):
    response = gemini_client.models.generate_content(
        model="gemini-2.0-flash",
        contents=input_text
    )
    return response.text

opik_args={"trace": {"thread_id": "f174a"}}
chat_message("What is Opik ?", opik_args=opik_args)
chat_message("Who is Opik?", opik_args=opik_args)
```

#### Copying traces to a new project

You can copy traces between projects using the [`copy_traces`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.copy_traces) method. This
method allows you to move traces from one project to another without having to re-log them.

```python
from opik import Opik

client = Opik(_use_batching=True)

client.copy_traces(
    project_name="<name of the project where the traces were created>",
    destination_project_name="<name of the new project>"
)
```

<Note>
By default, the `copy_traces` method will not delete the traces in the source project. You can optionally set
the `delete_original_project` parameter to `true` to delete the traces in the source project after copying them.

This is not recommended, instead we recommend moving the traces and once everything has been migrated you can delete
the source project from the UI.

</Note>

### Using the REST API

The documentation for the Opik REST API is available [here](/reference/rest-api/overview). \*/}
