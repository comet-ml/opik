---
description: Learn how to integrate Helicone with Opik to log and monitor your LLM traffic using the standard Helicone integration setup.
headline: Helicone | Opik Documentation
og:description: Learn to integrate Helicone with Opik using the OpenAI SDK wrapper to log all LLM calls for comprehensive observability.
og:site_name: Opik Documentation
og:title: Integrate Helicone with Opik for LLM Observability
title: Observability for Helicone with Opik
---

[Helicone](https://www.helicone.ai/) is an open-source LLM observability platform that provides monitoring, logging, and analytics for LLM applications. It acts as a proxy layer between your application and LLM providers, offering features like request logging, caching, rate limiting, and cost tracking.

## Gateway Overview

Helicone provides a comprehensive observability layer for LLM applications with features including:

- **Unified API**: OpenAI-compatible API with access to 100+ models through Helicone's model registry
- **Intelligent Routing**: Automatic failures and fallbacks across providers to ensure reliability
- **No Rate Limits**: Skip provider tier restrictions with zero markup on credits
- **Request Logging**: Automatic logging of all LLM requests and responses
- **Caching**: Reduce costs and improve latency with semantic caching
- **Cost Tracking**: Monitor spending across different models and providers with unified observability
- **Multi-Provider Support**: Works with OpenAI, Anthropic, Azure OpenAI, and more

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=helicone&utm_campaign=opik) provides a hosted version of the Opik platform. [Simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=helicone&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=helicone&utm_campaign=opik) for more information.

## Getting Started

### Installation

First, ensure you have both `opik` and `openai` packages installed:

```bash
pip install opik openai
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring Helicone

You'll need a Helicone API key. You can get one by signing up at [Helicone](https://www.helicone.ai/).

Set your API key as an environment variable:

```bash
export HELICONE_API_KEY="YOUR_HELICONE_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "HELICONE_API_KEY" not in os.environ:
    os.environ["HELICONE_API_KEY"] = getpass.getpass("Enter your Helicone API key: ")

```

## Logging LLM Calls

Since Helicone provides an OpenAI-compatible proxy, we can use the [Opik OpenAI SDK wrapper](/docs/opik/integrations/openai) to automatically log Helicone calls as generations in Opik.

### Simple LLM Call

```python
import os
from opik.integrations.openai import track_openai
from openai import OpenAI

# Create an OpenAI client with Helicone's base URL
client = OpenAI(
    api_key=os.environ["HELICONE_API_KEY"],
    base_url="https://ai-gateway.helicone.ai"
)

# Wrap the client with Opik tracking
client = track_openai(client, project_name="helicone-integration-demo")

# Make a chat completion request
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a knowledgeable AI assistant."},
        {"role": "user", "content": "What is the largest city in France?"}
    ]
)

# Print the assistant's reply
print(response.choices[0].message.content)
```

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If Helicone is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
import os
from opik import track
from opik.integrations.openai import track_openai
from openai import OpenAI

# Create and wrap the OpenAI client with Helicone's base URL
client = OpenAI(
    api_key=os.environ["HELICONE_API_KEY"],
    base_url="https://ai-gateway.helicone.ai"
)
client = track_openai(client)

@track
def generate_response(prompt: str):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a knowledgeable AI assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

@track
def refine_response(initial_response: str):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You enhance and polish text responses."},
            {"role": "user", "content": f"Please improve this response: {initial_response}"}
        ]
    )
    return response.choices[0].message.content

@track(project_name="helicone-integration-demo")
def generate_and_refine(prompt: str):
    # First LLM call: Generate initial response
    initial = generate_response(prompt)
    
    # Second LLM call: Refine the response
    refined = refine_response(initial)
    
    return refined

# Example usage
result = generate_and_refine("Explain quantum computing in simple terms.")
```

The trace will show nested LLM calls with hierarchical spans.

## Further Improvements

If you have suggestions for improving the Helicone integration, please let us know by opening an issue on [GitHub](https://github.com/comet-ml/opik/issues).
