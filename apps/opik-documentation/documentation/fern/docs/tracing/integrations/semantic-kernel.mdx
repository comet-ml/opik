---
description: How to send Semantic Kernel telemetry data to Opik using OpenTelemetry
---

# Semantic Kernel Integration via OpenTelemetry

Semantic Kernel is Microsoft's AI orchestration framework designed to simplify the integration of AI services into applications. It provides a unified programming model for working with different AI models, memory systems, and plugins, making it easier to build intelligent applications.

Semantic Kernel's primary advantage is its seamless integration with Microsoft's AI ecosystem and its plugin architecture that allows developers to easily extend functionality with custom skills and connectors.

<Frame>
  <img src="/img/tracing/semantic_kernel_integration.png" alt="Semantic Kernel tracing" />
</Frame>

## Getting started

To use the Semantic Kernel integration with Opik, you will need to have Semantic Kernel and the required OpenTelemetry packages installed.

### Installation

Install the required packages:

```bash
pip install semantic-kernel openinference-instrumentation-semantic-kernel opentelemetry-api opentelemetry-sdk opentelemetry-exporter-otlp-proto-http
```

### Environment configuration

<Tabs>
    <Tab value="Opik Cloud" title="Opik Cloud">
        ```bash wordWrap
        export OTEL_EXPORTER_OTLP_ENDPOINT=https://www.comet.com/opik/api/v1/private/otel
        export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'
        ```
    </Tab>
    <Tab value="Enterprise deployment" title="Enterprise deployment">
        ```bash wordWrap
        export OTEL_EXPORTER_OTLP_ENDPOINT=https://<comet-deployment-url>/opik/api/v1/private/otel
        export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'
        ```
    </Tab>
    <Tab value="Self-hosted instance" title="Self-hosted instance">
        ```bash
        export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5173/api/v1/private/otel
        export OTEL_EXPORTER_OTLP_HEADERS='projectName=<your-project-name>'
        ```
    </Tab>
</Tabs>

## Using Opik with Semantic Kernel

Set up OpenTelemetry instrumentation for Semantic Kernel:

```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from openinference.instrumentation.semantic_kernel import SemanticKernelInstrumentor
import os

# Set up the tracer provider
trace.set_tracer_provider(TracerProvider())

# Configure the OTLP exporter
otlp_exporter = OTLPSpanExporter(
    endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"),
    headers=dict(item.split("=") for item in os.getenv("OTEL_EXPORTER_OTLP_HEADERS", "").split(",") if "=" in item)
)

# Add the span processor
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(otlp_exporter))

# Instrument Semantic Kernel
SemanticKernelInstrumentor().instrument()

# Now use Semantic Kernel normally - all calls will be automatically traced
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion

kernel = sk.Kernel()
kernel.add_chat_service("chat", AzureChatCompletion("gpt-4", "your-endpoint", "your-api-key"))

# Your Semantic Kernel operations will now be traced
```

## Results viewing

After running your code, you can view the traces in the Opik UI. Navigate to your project and look for traces with the Semantic Kernel integration. You'll see:

- Kernel creation and configuration
- Function and skill execution
- Memory operations and retrieval
- Plugin interactions and calls
- Performance metrics and timing
- Error details if any occur

## Further improvements

If you have any feature requests or encounter issues with the Semantic Kernel integration, please open an issue on our [GitHub repository](https://github.com/comet-ml/opik/issues).
