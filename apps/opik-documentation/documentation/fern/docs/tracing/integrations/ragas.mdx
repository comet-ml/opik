---
title: Evaluate LLM Applications with Ragas Metrics in Opik
description: Use Ragas evaluation metrics to assess your LLM application quality and automatically track results in Opik for comprehensive performance monitoring.
pytest_codeblocks_skip: true
---

The Opik SDK provides a simple way to integrate with Ragas, a framework for evaluating RAG systems.

There are two main ways to use Ragas with Opik:

1. Using Ragas to score traces or spans.
2. Using Ragas to evaluate a RAG pipeline.

<div style="display: flex; align-items: center; flex-wrap: wrap; margin: 20px 0;">
  <span style="margin-right: 10px;">
    You can check out the Colab Notebook if you'd like to jump straight to the code:
  </span>
  <a
    className="no-external-icon"
    href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/ragas.ipynb"
    target="_blank"
    rel="noopener noreferrer"
  >
    <img
      src="https://colab.research.google.com/assets/colab-badge.svg"
      alt="Open In Colab"
      style="vertical-align: middle;"
    />
  </a>
</div>

## Getting started

You will first need to install the `opik` and `ragas` packages:

```bash
pip install opik ragas
```

In addition, you can configure Opik using the `opik configure` command which will prompt you for the correct local server address or if you are using the Cloud platform your API key:

```bash
opik configure
```

## Using Ragas to score traces or spans

Ragas provides a set of metrics that can be used to evaluate the quality of a RAG pipeline, a full list of the supported metrics can be found in the [Ragas documentation](https://docs.ragas.io/en/latest/concepts/metrics/available_metrics/).

You can use the `RagasMetricWrapper` to easily integrate Ragas metrics with Opik tracking:

```python
# Import the required dependencies
from ragas.metrics import AnswerRelevancy
from langchain_openai.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from opik.evaluation.metrics import RagasMetricWrapper

# Initialize the Ragas metric
llm = LangchainLLMWrapper(ChatOpenAI())
emb = LangchainEmbeddingsWrapper(OpenAIEmbeddings())
ragas_answer_relevancy = AnswerRelevancy(llm=llm, embeddings=emb)

# Wrap the Ragas metric with RagasMetricWrapper for Opik integration
answer_relevancy_metric = RagasMetricWrapper(
    ragas_answer_relevancy,
    track=True  # This enables automatic tracing in Opik
)
```

Once the metric wrapper is set up, you can use it to score traces or spans:

```python
from opik import track
from opik.opik_context import update_current_trace

@track
def retrieve_contexts(question):
    # Define the retrieval function, in this case we will hard code the contexts
    return ["Paris is the capital of France.", "Paris is in France."]

@track
def answer_question(question, contexts):
    # Define the answer function, in this case we will hard code the answer
    return "Paris"

@track
def rag_pipeline(question):
    # Define the pipeline
    contexts = retrieve_contexts(question)
    answer = answer_question(question, contexts)

    # Score the pipeline using the RagasMetricWrapper
    score_result = answer_relevancy_metric.score(
        user_input=question,
        response=answer,
        retrieved_contexts=contexts
    )

    # Add the score to the current trace
    update_current_trace(
        feedback_scores=[{"name": score_result.name, "value": score_result.value}]
    )

    return answer

print(rag_pipeline("What is the capital of France?"))
```

In the Opik UI, you will be able to see the full trace including the score calculation:

<Frame>
  <img src="/img/tracing/ragas_opik_trace.png" />
</Frame>

## Using Ragas metrics to evaluate a RAG pipeline

The `RagasMetricWrapper` can also be used directly within the Opik evaluation platform. This approach is much simpler than creating custom wrappers:

### 1. Define the Ragas metric

We will start by defining the Ragas metric, in this example we will use `AnswerRelevancy`:

```python
from ragas.metrics import AnswerRelevancy
from langchain_openai.chat_models import ChatOpenAI
from langchain_openai.embeddings import OpenAIEmbeddings
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from opik.evaluation.metrics import RagasMetricWrapper

# Initialize the Ragas metric
llm = LangchainLLMWrapper(ChatOpenAI())
emb = LangchainEmbeddingsWrapper(OpenAIEmbeddings())

ragas_answer_relevancy = AnswerRelevancy(llm=llm, embeddings=emb)
```

### 2. Create the metric wrapper

Simply wrap the Ragas metric with `RagasMetricWrapper`:

```python
# Create the answer relevancy scoring metric
answer_relevancy = RagasMetricWrapper(
    ragas_answer_relevancy,
    track=True  # Enable tracing for the metric computation
)
```

<Tip>

If you are running within a Jupyter notebook, you will need to add the following line to the top of your notebook:

```python
import nest_asyncio
nest_asyncio.apply()
```

</Tip>

### 3. Use the metric wrapper within the Opik evaluation platform

You can now use the metric wrapper directly within the Opik evaluation platform:

```python
from opik.evaluation import evaluate

evaluation_task = evaluate(
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[answer_relevancy],
    nb_samples=10,
)
```

The `RagasMetricWrapper` automatically handles:

- Field mapping between Opik and Ragas (e.g., `input` → `user_input`, `output` → `response`)
- Async execution of Ragas metrics
- Integration with Opik's tracing system when `track=True`
- Proper error handling for missing required fields
