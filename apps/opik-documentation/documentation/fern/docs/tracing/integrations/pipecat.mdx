---
title: Observability for Pipecat with Opik
description: Start here to integrate Opik into your Pipecat-based real-time voice agent application for end-to-end LLM observability, unit testing, and optimization.
---

[Pipecat](https://github.com/pipecat-ai/pipecat) is an open-source Python framework for building real-time voice and multimodal conversational AI agents. Developed by Daily, it enables fully programmable AI voice agents and supports multimodal interactions, positioning itself as a flexible solution for developers looking to build conversational AI systems.

This guide explains how to integrate Opik with Pipecat for observability and tracing of real-time voice agents, enabling you to monitor, debug, and optimize your Pipecat agents in the Opik dashboard.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=pipecat&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=pipecat&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=pipecat&utm_campaign=opik) for more information.

<Frame>
  <img src="/img/tracing/pipecat_integration.png" />
</Frame>

## Getting started

To use the Pipecat integration with Opik, you will need to have Pipecat and the required OpenTelemetry packages installed:

```bash
pip install pipecat-ai[daily,webrtc,silero,cartesia,deepgram,openai,tracing] opentelemetry-exporter-otlp-proto-http websockets
```

<Tabs>
    <Tab value="Opik Cloud" title="Opik Cloud">
        ```bash wordWrap
        export OTEL_EXPORTER_OTLP_ENDPOINT=https://www.comet.com/opik/api/v1/private/otel
        export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'
        ```
    </Tab>
    <Tab value="Enterprise deployment" title="Enterprise deployment">
        ```bash wordWrap
        export OTEL_EXPORTER_OTLP_ENDPOINT=https://<comet-deployment-url>/opik/api/v1/private/otel
        export OTEL_EXPORTER_OTLP_HEADERS='Authorization=<your-api-key>,Comet-Workspace=default'
        ```
    </Tab>
    <Tab value="Self-hosted instance" title="Self-hosted instance">
        ```bash
        export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:5173/api/v1/private/otel
        export OTEL_EXPORTER_OTLP_HEADERS='projectName=<your-project-name>'
        ```
    </Tab>
</Tabs>

## Using Opik with Pipecat

For the basic example, you'll need an OpenAI API key. You can set it as an environment variable:

```bash
export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

Enable tracing in your Pipecat application by setting up OpenTelemetry instrumentation and configuring your pipeline task:

```python
# Initialize OpenTelemetry with the http exporter
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from pipecat.utils.tracing.setup import setup_tracing

# Configured automatically from .env
exporter = OTLPSpanExporter()

setup_tracing(
    service_name="pipecat-demo",
    exporter=exporter,
)

# Enable tracing in your PipelineTask
task = PipelineTask(
    pipeline,
    params=PipelineParams(
        allow_interruptions=True,
        enable_metrics=True,  # Required for some service metrics
    ),
    enable_tracing=True,  # Enables both turn and conversation tracing
    conversation_id="customer-123",  # Optional - will auto-generate if not provided
)
```

## Trace Structure

Traces are organized hierarchically in Opik:

```
Pipeline Task (task-uuid)
├── turn-1
│   ├── llm_openaillmservice
│   └── message_aggregation
└── turn-2
    ├── llm_openaillmservice
    └── message_aggregation
```

This organization helps you track pipeline execution and LLM interactions within your Pipecat application.

## Understanding the Traces

- **Pipeline Task Spans**: The top-level span representing the entire pipeline execution
- **Turn Spans**: Child spans that represent each processing turn in the pipeline
- **Service Spans**: Detailed service operations nested under turns
- **Service Attributes**: Each service includes rich context about its operation:
  - **LLM**: Messages, tokens used, model, service configuration
  - **Aggregators**: Message processing and aggregation details
- **Metrics**: Performance data and processing durations

## Results viewing

Once your Pipecat applications are traced with Opik, you can view them in the Opik UI. Each pipeline execution will contain:

- Complete pipeline flow with turns and service calls
- Detailed service metrics and performance data
- Token usage and cost tracking for LLM services
- Message processing and aggregation details
- Real-time pipeline monitoring

## Advanced Configuration

### Custom Service Tracing

You can add custom tracing to your Pipecat services:

```python
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

tracer = trace.get_tracer(__name__)

class CustomVoiceService:
    @tracer.start_as_current_span("custom_voice_processing")
    async def process_audio(self, audio_data):
        with tracer.start_as_current_span("audio_analysis") as span:
            # Your custom processing logic
            span.set_attribute("audio.duration", len(audio_data))
            span.set_attribute("audio.format", "wav")

            result = await self.analyze_audio(audio_data)

            span.set_attribute("analysis.confidence", result.confidence)
            span.set_status(Status(StatusCode.OK))

            return result
```

## Environment Variables

Make sure to set the following environment variables for the example to work:

```bash
# Required for OpenAI LLM service
export OPENAI_API_KEY="your-openai-api-key"

# Required for Opik tracing (set based on your deployment type above)
export OTEL_EXPORTER_OTLP_ENDPOINT="https://www.comet.com/opik/api/v1/private/otel"
export OTEL_EXPORTER_OTLP_HEADERS="Authorization=<your-opik-api-key>,Comet-Workspace=default"
```

## Troubleshooting

### Common Issues

1. **OTEL 404 Error**: If you see "Failed to export span batch code: 404", check that:
   - Your `OTEL_EXPORTER_OTLP_ENDPOINT` is correct for your deployment type
   - For Opik Cloud: `https://www.comet.com/opik/api/v1/private/otel`
   - For Self-hosted: `http://localhost:5173/api/v1/private/otel`
   - The endpoint should end with `/otel` for trace exports
2. **No Traces in Opik**: Ensure that your OTEL environment variables are correctly set and follow the [troubleshooting guide](/tracing/troubleshooting)
3. **Missing Metrics**: Check that `enable_metrics=True` is set in PipelineParams
4. **Connection Errors**: Verify network connectivity to Opik and that your API key is valid
5. **OpenAI API Errors**: Ensure your `OPENAI_API_KEY` is correctly set and has sufficient credits
6. **Tracing Not Working**: Verify that `enable_tracing=True` is set in your PipelineTask
7. **Context Aggregator Errors**: If you see "missing 1 required positional argument: 'context'", use the OpenAI-specific aggregators:
   - `OpenAIUserContextAggregator()` instead of `LLMUserContextAggregator()`
   - `OpenAIAssistantContextAggregator()` instead of `LLMAssistantContextAggregator()`
8. **Deprecation Warnings**: Update imports to use the latest Pipecat APIs as shown in the example above

### Getting Help

- Check the [Pipecat Documentation](https://github.com/pipecat-ai/pipecat) for framework-specific issues
- Review the [OpenTelemetry Python Documentation](https://opentelemetry.io/docs/instrumentation/python/) for tracing setup
- Contact Pipecat support for framework-specific problems
- Check Opik documentation for tracing features

## Further improvements

If you would like to see us improve this integration, simply open a new feature
request on [Github](https://github.com/comet-ml/opik/issues).
