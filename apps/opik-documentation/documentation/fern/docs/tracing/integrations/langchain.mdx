---
title: Observability for LangChain with Opik
description: Start here to integrate Opik into your LangChain-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

Opik provides seamless integration with LangChain, allowing you to easily log and trace your LangChain-based applications. By using the `OpikTracer` callback, you can automatically capture detailed information about your LangChain runs, including inputs, outputs, metadata, and cost tracking for each step in your chain.

## Key Features

- **Automatic cost tracking** for supported LLM providers (OpenAI, Anthropic, Google AI, AWS Bedrock, and more)
- **Full compatibility** with the `@opik.track` decorator for hybrid tracing approaches
- **Thread support** for conversational applications with `thread_id` parameter
- **Distributed tracing** support for multi-service applications
- **LangGraph compatibility** for complex graph-based workflows

<div style="display: flex; align-items: center; flex-wrap: wrap; margin: 20px 0;">
  <span style="margin-right: 10px;">
    You can check out the Colab Notebook if you'd like to jump straight to the code:
  </span>
  <a
    className="no-external-icon"
    href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/langchain.ipynb"
    target="_blank"
    rel="noopener noreferrer"
  >
    <img
      src="https://colab.research.google.com/assets/colab-badge.svg"
      alt="Open In Colab"
      style="vertical-align: middle;"
    />
  </a>
</div>

## Getting Started

To use the `OpikTracer` with LangChain, you'll need to have both the `opik` and `langchain` packages installed. You can install them using pip:

```bash
pip install opik langchain langchain_openai
```

In addition, you can configure Opik using the `opik configure` command which will prompt you for the correct local server address or if you are using the Cloud platform your API key:

```bash
opik configure
```

## Using OpikTracer

Here's a basic example of how to use the `OpikTracer` callback with a LangChain chain:

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from opik.integrations.langchain import OpikTracer

# Initialize the tracer
opik_tracer = OpikTracer(project_name="langchain-examples")

llm = ChatOpenAI(model="gpt-4o", temperature=0)
prompt = ChatPromptTemplate.from_messages([
    ("human", "Translate the following text to French: {text}")
])
chain = prompt | llm

result = chain.invoke(
    {"text": "Hello, how are you?"}, 
    config={"callbacks": [opik_tracer]}
)
print(result.content)
```

The `OpikTracer` will automatically log the run and its details to Opik, including the input prompt, the output, and metadata for each step in the chain.

For detailed parameter information, see the [OpikTracer SDK reference](https://www.comet.com/docs/opik/python-sdk-reference/integrations/langchain/OpikTracer.html).

## Cost Tracking

The `OpikTracer` automatically tracks token usage and cost for all supported LLM models used within LangChain applications.

Cost information is automatically captured and displayed in the Opik UI, including:
- Token usage details
- Cost per request based on model pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>

For streaming with cost tracking, ensure `stream_usage=True` is set:

```python
from langchain_openai import ChatOpenAI
from opik.integrations.langchain import OpikTracer

llm = ChatOpenAI(
    model="gpt-4o",
    streaming=True,
    stream_usage=True,  # Required for cost tracking with streaming
)

opik_tracer = OpikTracer()

for chunk in llm.stream("Hello", config={"callbacks": [opik_tracer]}):
    print(chunk.content, end="")
```

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>

## Settings tags and metadata

You can customize the `OpikTracer` callback to include additional metadata, logging options, and conversation threading:

```python
from opik.integrations.langchain import OpikTracer

opik_tracer = OpikTracer(
    tags=["langchain", "production"],
    metadata={"use-case": "customer-support", "version": "1.0"},
    thread_id="conversation-123",  # For conversational applications
    project_name="my-langchain-project"
)
```

## Accessing logged traces

You can use the [`created_traces`](https://www.comet.com/docs/opik/python-sdk-reference/integrations/langchain/OpikTracer.html) method to access the traces collected by the `OpikTracer` callback:

```python
from opik.integrations.langchain import OpikTracer

opik_tracer = OpikTracer()

# Calling Langchain object
traces = opik_tracer.created_traces()
print([trace.id for trace in traces])
```

The traces returned by the `created_traces` method are instances of the [`Trace`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/Trace.html#opik.api_objects.trace.Trace) class, which you can use to update the metadata, feedback scores and tags for the traces.

### Accessing the content of logged traces

In order to access the content of logged traces you will need to use the [`Opik.get_trace_content`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.get_trace_content) method:

```python
import opik
from opik.integrations.langchain import OpikTracer
opik_client = opik.Opik()

opik_tracer = OpikTracer()


# Calling Langchain object

# Getting the content of the logged traces
traces = opik_tracer.created_traces()
for trace in traces:
    content = opik_client.get_trace_content(trace.id)
    print(content)
```

### Updating and scoring logged traces

You can update the metadata, feedback scores and tags for traces after they are created. For this you can use the `created_traces` method to access the traces and then update them using the [`update`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/Trace.html#opik.api_objects.trace.Trace.update) method and the [`log_feedback_score`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/Trace.html#opik.api_objects.trace.Trace.log_feedback_score) method:

```python
from opik.integrations.langchain import OpikTracer

opik_tracer = OpikTracer(project_name="langchain-examples")

# ... calling Langchain object

traces = opik_tracer.created_traces()

for trace in traces:
    trace.update(tags=["my-tag"])
    trace.log_feedback_score(name="user-feedback", value=0.5)
```

## Compatibility with @track Decorator

The `OpikTracer` is fully compatible with the `@track` decorator, allowing you to create hybrid tracing approaches:

```python
import opik
from langchain_openai import ChatOpenAI
from opik.integrations.langchain import OpikTracer

@opik.track
def my_langchain_workflow(user_input: str) -> str:
    llm = ChatOpenAI(model="gpt-4o")
    opik_tracer = OpikTracer()
    
    # The LangChain call will create spans within the existing trace
    response = llm.invoke(user_input, config={"callbacks": [opik_tracer]})
    return response.content

result = my_langchain_workflow("What is machine learning?")
```

## Thread Support

Use the `thread_id` parameter to group related conversations or interactions:

```python
from opik.integrations.langchain import OpikTracer

# All traces with the same thread_id will be grouped together
opik_tracer = OpikTracer(thread_id="user-session-123")
```

## Distributed Tracing

For multi-service/thread/process applications, you can use distributed tracing headers to connect traces across services:

```python
from opik import opik_context
from opik.integrations.langchain import OpikTracer
from opik.types import DistributedTraceHeadersDict

# In your service that receives distributed trace headers.
# The distributed_headers dict can be obtained in the "parent" service via `opik_context.get_distributed_trace_headers()`
distributed_headers = DistributedTraceHeadersDict(
    opik_trace_id="trace-id-from-upstream",
    opik_parent_span_id="parent-span-id-from-upstream"
)

opik_tracer = OpikTracer(distributed_headers=distributed_headers)

# LangChain operations will be attached to the existing distributed trace
chain.invoke(input_data, config={"callbacks": [opik_tracer]})
```

<Tip>
  Learn more about distributed tracing in the [Distributed Tracing guide](/tracing/log_distributed_traces).
</Tip>

## LangGraph Integration

For LangGraph applications, Opik provides specialized support. The `OpikTracer` works seamlessly with LangGraph, and you can also visualize graph definitions:

```python
from langgraph.graph import StateGraph
from opik.integrations.langchain import OpikTracer

# Your LangGraph setup
graph = StateGraph(...)
compiled_graph = graph.compile()

opik_tracer = OpikTracer()
result = compiled_graph.invoke(
    input_data,
    config={"callbacks": [opik_tracer]}
)
```

<Tip>
  For detailed LangGraph integration examples, see the [LangGraph Integration guide](/tracing/integrations/langgraph).
</Tip>

## Advanced usage

The `OpikTracer` object has a `flush` method that can be used to make sure that all traces are logged to the Opik platform before you exit a script. This method will return once all traces have been logged or if the timeout is reach, whichever comes first.

```python
from opik.integrations.langchain import OpikTracer

opik_tracer = OpikTracer()
opik_tracer.flush()
```

## Important notes

1. **Asynchronous streaming**: If you are using asynchronous streaming mode (calling `.astream()` method), the `input` field in the trace UI may be empty due to a LangChain limitation for this mode. However, you can find the input data inside the nested spans of this chain.

2. **Streaming with cost tracking**: If you are planning to use streaming with LLM calls and want to calculate LLM call tokens/cost, you need to explicitly set `stream_usage=True`:
