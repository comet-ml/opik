---
description: Start here to integrate Vercel AI Gateway with Opik for unified access to multiple AI providers with edge-optimized performance.
headline: Vercel AI Gateway | Opik Documentation
og:description: Learn to integrate Vercel AI Gateway with Opik using the OpenAI SDK wrapper to log all LLM calls for comprehensive observability.
og:site_name: Opik Documentation
og:title: Integrate Vercel AI Gateway with Opik for Edge-Optimized AI
title: Observability for Vercel AI Gateway with Opik
---

[Vercel AI Gateway](https://vercel.com/docs/ai-gateway) provides a unified interface to access multiple AI providers with edge-optimized performance, built-in caching, and comprehensive analytics. It's designed to work seamlessly with Vercel's edge infrastructure.

## Gateway Overview

Vercel AI Gateway provides enterprise-grade features for managing AI API access, including:

- **Unified API**: Access hundreds of models across providers via Vercel's AI Gateway with minimal code changes
- **Transparent Pricing**: No markup on tokens with "Bring Your Own Key" support
- **Automatic Failover**: High reliability with requests routed to alternate providers if primary is unavailable
- **Low Latency**: Sub-20ms routing latency through Vercel's global edge network
- **Intelligent Caching**: Reduce costs and improve response times with smart caching

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=vercel-ai-gateway&utm_campaign=opik) provides a hosted version of the Opik platform. [Simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=vercel-ai-gateway&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=vercel-ai-gateway&utm_campaign=opik) for more information.

## Getting Started

### Installation

First, ensure you have both `opik` and `openai` packages installed:

```bash
pip install opik openai
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring Vercel AI Gateway

You'll need to set up the Vercel AI Gateway in your Vercel project. Follow the [Vercel AI Gateway setup guide](https://vercel.com/docs/ai-gateway) to configure your gateway.

Set your API keys as environment variables:

```bash
export VERCEL_AI_GATEWAY_URL="YOUR_VERCEL_AI_GATEWAY_URL"
export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
```

Or set them programmatically:

```python
import os
import getpass

if "VERCEL_AI_GATEWAY_URL" not in os.environ:
    os.environ["VERCEL_AI_GATEWAY_URL"] = input("Enter your Vercel AI Gateway URL: ")

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

## Logging LLM Calls

Since Vercel AI Gateway provides an OpenAI-compatible API, we can use the [Opik OpenAI SDK wrapper](/docs/opik/integrations/openai) to automatically log Vercel AI Gateway calls as generations in Opik.

### Simple LLM Call

```python
import os
from opik.integrations.openai import track_openai
from openai import OpenAI

# Create an OpenAI client with Vercel AI Gateway's base URL
client = OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    base_url=os.environ["VERCEL_AI_GATEWAY_URL"]
)

# Wrap the client with Opik tracking
client = track_openai(client, project_name="vercel-ai-gateway-demo")

# Make a chat completion request
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a knowledgeable AI assistant."},
        {"role": "user", "content": "What is the largest city in France?"}
    ]
)

# Print the assistant's reply
print(response.choices[0].message.content)
```

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If Vercel AI Gateway is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
import os
from opik import track
from opik.integrations.openai import track_openai
from openai import OpenAI

# Create and wrap the OpenAI client with Vercel AI Gateway's base URL
client = OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    base_url=os.environ["VERCEL_AI_GATEWAY_URL"]
)
client = track_openai(client)

@track
def generate_response(prompt: str):
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a knowledgeable AI assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response.choices[0].message.content

@track
def refine_response(initial_response: str):
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You enhance and polish text responses."},
            {"role": "user", "content": f"Please improve this response: {initial_response}"}
        ]
    )
    return response.choices[0].message.content

@track(project_name="vercel-ai-gateway-demo")
def generate_and_refine(prompt: str):
    # First LLM call: Generate initial response
    initial = generate_response(prompt)
    
    # Second LLM call: Refine the response
    refined = refine_response(initial)
    
    return refined

# Example usage
result = generate_and_refine("Explain quantum computing in simple terms.")
```

The trace will show nested LLM calls with hierarchical spans.

## Further Improvements

If you have suggestions for improving the Vercel AI Gateway integration, please let us know by opening an issue on [GitHub](https://github.com/comet-ml/opik/issues).
