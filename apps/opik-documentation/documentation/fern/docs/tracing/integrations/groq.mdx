---
description: Start here to integrate Opik into your Groq-based genai application for
  end-to-end LLM observability, unit testing, and optimization.
headline: Groq | Opik Documentation
og:description: Learn to set up your Groq account on Opik and get started with AI
  inference using your API Key.
og:site_name: Opik Documentation
og:title: Fast AI Inference with Groq - Opik Model Providers
title: Observability for Groq with Opik
---

[Groq](https://groq.com/) is Fast AI Inference.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=groq&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=groq&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=groq&utm_campaign=opik) for more information.

## Getting Started

### Installation

To start tracking your Groq LLM calls, you can use our [LiteLLM integration](/integrations/litellm). You'll need to have both the `opik` and `litellm` packages installed. You can install them using pip:

```bash
pip install opik litellm
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

<Info>

If you're unable to use our LiteLLM integration with Groq, please [open an issue](https://github.com/comet-ml/opik/issues/new/choose)

</Info>

### Configuring Groq

In order to configure Groq, you will need to have your Groq API Key. You can create and manage your Groq API Keys on [this page](https://console.groq.com/keys).

You can set it as an environment variable:

```bash
export GROQ_API_KEY="YOUR_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "GROQ_API_KEY" not in os.environ:
    os.environ["GROQ_API_KEY"] = getpass.getpass("Enter your Groq API key: ")
```

## Logging LLM calls

In order to log the LLM calls to Opik, you will need to create the OpikLogger callback. Once the OpikLogger callback is created and added to LiteLLM, you can make calls to LiteLLM as you normally would:

```python
from litellm.integrations.opik.opik import OpikLogger
import litellm
import os

os.environ["OPIK_PROJECT_NAME"] = "groq-integration-demo"

opik_logger = OpikLogger()
litellm.callbacks = [opik_logger]

prompt = """
Write a short two sentence story about Opik.
"""

response = litellm.completion(
    model="groq/llama3-8b-8192",
    messages=[{"role": "user", "content": prompt}]
)

print(response.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/groq_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you are using LiteLLM within a function tracked with the [`@track`](/tracing/log_traces#using-function-decorators) decorator, you will need to pass the `current_span_data` as metadata to the `litellm.completion` call:

```python
from opik import track
from opik.opik_context import get_current_span_data
import litellm

@track
def generate_story(prompt):
    response = litellm.completion(
        model="groq/llama3-8b-8192",
        messages=[{"role": "user", "content": prompt}],
        metadata={
            "opik": {
                "current_span_data": get_current_span_data(),
            },
        },
    )
    return response.choices[0].message.content

@track
def generate_topic():
    prompt = "Generate a topic for a story about Opik."
    response = litellm.completion(
        model="groq/llama3-8b-8192",
        messages=[{"role": "user", "content": prompt}],
        metadata={
            "opik": {
                "current_span_data": get_current_span_data(),
            },
        },
    )
    return response.choices[0].message.content

@track
def generate_opik_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_opik_story()
```

<Frame>
  <img src="/img/cookbook/groq_trace_decorator_cookbook.png" />
</Frame>