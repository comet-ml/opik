---
title: Observability for AWS Bedrock with Opik
description: Start here to integrate Opik into your AWS Bedrock-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

[AWS Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that provides access to high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API.

This guide explains how to integrate Opik with the Bedrock Python SDK. By using the `track_bedrock` method provided by opik, you can easily track and evaluate your Bedrock API calls within your Opik projects as Opik will automatically log the input prompt, model used, token usage, and response generated.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=bedrock&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=bedrock&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=bedrock&utm_campaign=opik) for more information.

## Getting Started

### Installation

To start tracking your Bedrock LLM calls, you'll need to have both the `opik` and `boto3` packages. You can install them using pip:

```bash
pip install opik boto3
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring Bedrock

In order to configure Bedrock, you will need to have:

- Your AWS Credentials configured for boto, see the [following documentation page for how to set them up](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html).
- Access to the model you are planning to use, see the [following documentation page how to do so](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html).

You can request access to models in the [AWS Bedrock console](https://us-east-1.console.aws.amazon.com/bedrock/home?region=us-east-1#/providers?model=meta.llama3-2-3b-instruct-v1:0).

Once you have these, you can create your boto3 client:

```python
import boto3

REGION = "us-east-1"
MODEL_ID = "us.meta.llama3-2-3b-instruct-v1:0"

bedrock = boto3.client(
    service_name="bedrock-runtime",
    region_name=REGION,
    # aws_access_key_id=ACCESS_KEY,
    # aws_secret_access_key=SECRET_KEY,
    # aws_session_token=SESSION_TOKEN,
)
```

## Logging LLM calls

In order to log the LLM calls to Opik, you will need to create the wrap the boto3 client with `track_bedrock`. When making calls with that wrapped client, all calls will be logged to Opik:

```python
from opik.integrations.bedrock import track_bedrock

bedrock_client = track_bedrock(bedrock, project_name="bedrock-integration-demo")

PROMPT = "Why is it important to use a LLM Monitoring like CometML Opik tool that allows you to log traces and spans when working with LLM Models hosted on AWS Bedrock?"

response = bedrock_client.converse(
    modelId=MODEL_ID,
    messages=[{"role": "user", "content": [{"text": PROMPT}]}],
    inferenceConfig={"temperature": 0.5, "maxTokens": 512, "topP": 0.9},
)
print("Response", response["output"]["message"]["content"][0]["text"])
```

<Frame>
  <img src="/img/cookbook/bedrock_trace_cookbook.png" />
</Frame>

## Streaming API

Bedrock supports streaming responses, which is useful for real-time applications. Here's how to use streaming with Opik:

```python
def stream_conversation(
    bedrock_client,
    model_id,
    messages,
    system_prompts,
    inference_config,
):
    """
    Sends messages to a model and streams the response.
    Args:
        bedrock_client: The Boto3 Bedrock runtime client.
        model_id (str): The model ID to use.
        messages (JSON) : The messages to send.
        system_prompts (JSON) : The system prompts to send.
        inference_config (JSON) : The inference configuration to use.

    Returns:
        Nothing.
    """

    response = bedrock_client.converse_stream(
        modelId=model_id,
        messages=messages,
        system=system_prompts,
        inferenceConfig=inference_config,
    )

    stream = response.get("stream")
    if stream:
        for event in stream:
            if "messageStart" in event:
                print(f"\nRole: {event['messageStart']['role']}")

            if "contentBlockDelta" in event:
                print(event["contentBlockDelta"]["delta"]["text"], end="")

            if "messageStop" in event:
                print(f"\nStop reason: {event['messageStop']['stopReason']}")

            if "metadata" in event:
                metadata = event["metadata"]
                if "usage" in metadata:
                    print("\nToken usage")
                    print(f"Input tokens: {metadata['usage']['inputTokens']}")
                    print(f"Output tokens: {metadata['usage']['outputTokens']}")
                    print(f"Total tokens: {metadata['usage']['totalTokens']}")
                if "metrics" in event["metadata"]:
                    print(f"Latency: {metadata['metrics']['latencyMs']} milliseconds")

# Example usage with system prompts
system_prompt = """You are an app that creates playlists for a radio station
  that plays rock and pop music. Only return song names and the artist."""

input_text = "Create a list of 3 pop songs."

message = {"role": "user", "content": [{"text": input_text}]}
messages = [message]

# System prompts.
system_prompts = [{"text": system_prompt}]

# inference parameters to use.
inference_config = {"temperature": 0.5, "topP": 0.9}

stream_conversation(
    bedrock_client,
    MODEL_ID,
    messages,
    system_prompts,
    inference_config,
)
```

<Frame>
  <img src="/img/cookbook/bedrock_trace_streaming_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If Bedrock is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from opik import track
from opik.integrations.bedrock import track_bedrock

os.environ["OPIK_PROJECT_NAME"] = "bedrock-integration-demo"
bedrock_client = track_bedrock(bedrock)

@track
def generate_story(prompt):
    res = bedrock_client.converse(
        modelId=MODEL_ID, messages=[{"role": "user", "content": [{"text": prompt}]}]
    )
    return res["output"]["message"]["content"][0]["text"]

@track
def generate_topic():
    prompt = "Generate a topic for a story about Opik."
    res = bedrock_client.converse(
        modelId=MODEL_ID, messages=[{"role": "user", "content": [{"text": prompt}]}]
    )
    return res["output"]["message"]["content"][0]["text"]

@track
def generate_opik_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_opik_story()
```

The trace can now be viewed in the UI with hierarchical spans showing the relationship between different steps:

<Frame>
  <img src="/img/cookbook/bedrock_trace_decorator_cookbook.png" />
</Frame>

## Cost Tracking

The `track_bedrock` wrapper automatically tracks token usage and cost for all supported AWS Bedrock models.

Cost information is automatically captured and displayed in the Opik UI, including:

- Token usage details
- Cost per request based on Bedrock pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>
