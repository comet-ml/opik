---
description: Start here to integrate Opik into your OpenAI-based genai application
  for end-to-end LLM observability, unit testing, and optimization.
headline: OpenAI | Opik Documentation
og:description: Learn how to seamlessly track and evaluate OpenAI API calls in your
  Opik projects using the track_openai method.
og:site_name: Opik Documentation
og:title: Integrate OpenAI with Opik for Effective Tracking
title: Observability for OpenAI (Python) with Opik
---

<Tip>
  If you are using OpenAI's Agents framework, we recommend using the [OpenAI Agents
  integration](/integrations/openai_agents) instead.
</Tip>

This guide explains how to integrate Opik with the OpenAI Python SDK. By using the `track_openai` method provided by opik, you can easily track and evaluate your OpenAI API calls within your Opik projects as Opik will automatically log the input prompt, model used, token usage, and response generated.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) for more information.

## Getting Started

### Installation

First, ensure you have both `opik` and `openai` packages installed:

```bash
pip install opik openai
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring OpenAI

In order to configure OpenAI, you will need to have your OpenAI API Key. You can [find or create your OpenAI API Key in this page](https://platform.openai.com/settings/organization/api-keys).

You can set it as an environment variable:

```bash
export OPENAI_API_KEY="YOUR_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

## Logging LLM calls

In order to log the LLM calls to Opik, you will need to wrap the OpenAI client with `track_openai`. When making calls with that wrapped client, all calls will be logged to Opik:

```python
from opik.integrations.openai import track_openai
from openai import OpenAI
import os

os.environ["OPIK_PROJECT_NAME"] = "openai-integration-demo"

client = OpenAI()
openai_client = track_openai(client)

prompt = """
Write a short two sentence story about Opik.
"""

completion = openai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}]
)

print(completion.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/openai_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If OpenAI is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from opik import track
from opik.integrations.openai import track_openai
from openai import OpenAI

os.environ["OPIK_PROJECT_NAME"] = "openai-integration-demo"

client = OpenAI()
openai_client = track_openai(client)

@track
def generate_story(prompt):
    res = openai_client.chat.completions.create(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track
def generate_topic():
    prompt = "Generate a topic for a story about Opik."
    res = openai_client.chat.completions.create(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track
def generate_opik_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_opik_story()
```

The trace can now be viewed in the UI with hierarchical spans showing the relationship between different steps:

<Frame>
  <img src="/img/cookbook/openai_trace_decorator_cookbook.png" />
</Frame>

## Using Azure OpenAI

The OpenAI integration also supports Azure OpenAI Services. To use Azure OpenAI, initialize your client with Azure configuration and use it with `track_openai` just like the standard OpenAI client:

```python
from opik.integrations.openai import track_openai
from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
azure_client = AzureOpenAI(
    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)
azure_client = track_openai(azure_client)

completion = azure_client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
```

## Cost Tracking

The `track_openai` wrapper automatically tracks token usage and cost for all supported OpenAI models.

Cost information is automatically captured and displayed in the Opik UI, including:

- Token usage details
- Cost per request based on OpenAI pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>

## Grouping traces into conversational threads using `thread_id`

Threads in Opik are collections of traces that are grouped together using a unique `thread_id`.

The `thread_id` can be passed to the OpenAI client as a parameter, which will be used to group all traces into a single thread.

```python
import openai

from opik.integrations.openai import track_openai

client = openai.OpenAI()
wrapped_client = track_openai(
    openai_client=client,
    project_name="opik_args demo",
)
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is Opik?"},
]

_ = wrapped_client.responses.create(
    model="gpt-4o-mini",
    input=messages,
    opik_args={"trace": {"thread_id": "f174a"}}
)
```

More information on logging chat conversations can be found in the [Log conversations](/tracing/log_chat_conversations) section.

## Video Generation (Sora)

The `track_openai` wrapper also supports OpenAI's video generation API (Sora). When you generate videos, Opik automatically tracks the video creation process and logs the generated video as an attachment when you download it via the client API.

```python
import os
from opik import track
from opik.integrations.openai import track_openai
from openai import OpenAI

os.environ["OPIK_PROJECT_NAME"] = "openai-video-demo"

client = OpenAI()
tracked_client = track_openai(client)


@track
def generate_video(prompt: str) -> dict:
    """Generate a video using OpenAI's Sora model."""
    # Create video and wait for completion
    video = tracked_client.videos.create_and_poll(
        model="sora-2",
        prompt=prompt,
    )

    result = {"id": video.id, "status": video.status}

    # Download the video if generation succeeded
    # If OpenAI moderation rejected your request - you'll see it in the create_and_poll span output
    if video.status == "completed":
        content = tracked_client.videos.download_content(video_id=video.id)
        content.write_to_file("output_video.mp4")
        result["output_path"] = "output_video.mp4"

    return result


# Generate a video
generate_video("A golden retriever playing in the snow")
```

The trace will show the full video generation workflow including the video creation, polling, download, and the generated video as an attachment:

<Frame>
  <img src="/img/tracing/openai_video_integration.png" />
</Frame>

<Tip>
  The `videos.retrieve` method is intentionally not tracked because the `poll` method makes many retrieve calls internally, which would create excessive noise in your traces. If you need to track retrieve calls (e.g. if you don't use `poll` and check the progress manually), you can decorate the method yourself:

  ```python
  tracked_client.videos.retrieve = opik.track(name="videos.retrieve")(
      tracked_client.videos.retrieve
  )
  ```
</Tip>

## Supported OpenAI methods

The `track_openai` wrapper supports the following OpenAI methods:

- `openai_client.chat.completions.create()`, including support for stream=True mode.
- `openai_client.beta.chat.completions.parse()`
- `openai_client.beta.chat.completions.stream()`
- `openai_client.responses.create()`
- `openai_client.videos.create()`
- `openai_client.videos.create_and_poll()`
- `openai_client.videos.poll()`
- `openai_client.videos.download_content()`
- `openai_client.videos.list()`
- `openai_client.videos.delete()`

If you would like to track another OpenAI method, please let us know by opening an issue on [GitHub](https://github.com/comet-ml/opik/issues).