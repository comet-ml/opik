---
title: Observability for OpenAI (Python) with Opik
description: Start here to integrate Opik into your OpenAI-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

<Tip>
  If you are using OpenAI's Agents framework, we recommend using the [OpenAI Agents
  integration](/integrations/openai_agents) instead.
</Tip>

This guide explains how to integrate Opik with the OpenAI Python SDK. By using the `track_openai` method provided by opik, you can easily track and evaluate your OpenAI API calls within your Opik projects as Opik will automatically log the input prompt, model used, token usage, and response generated.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=openai&utm_campaign=opik) for more information.

## Getting Started

### Installation

First, ensure you have both `opik` and `openai` packages installed:

```bash
pip install opik openai
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring OpenAI

In order to configure OpenAI, you will need to have your OpenAI API Key. You can [find or create your OpenAI API Key in this page](https://platform.openai.com/settings/organization/api-keys).

You can set it as an environment variable:

```bash
export OPENAI_API_KEY="YOUR_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

## Logging LLM calls

In order to log the LLM calls to Opik, you will need to wrap the OpenAI client with `track_openai`. When making calls with that wrapped client, all calls will be logged to Opik:

```python
from opik.integrations.openai import track_openai
from openai import OpenAI
import os

os.environ["OPIK_PROJECT_NAME"] = "openai-integration-demo"

client = OpenAI()
openai_client = track_openai(client)

prompt = """
Write a short two sentence story about Opik.
"""

completion = openai_client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": prompt}]
)

print(completion.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/openai_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If OpenAI is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from opik import track
from opik.integrations.openai import track_openai
from openai import OpenAI

os.environ["OPIK_PROJECT_NAME"] = "openai-integration-demo"

client = OpenAI()
openai_client = track_openai(client)

@track
def generate_story(prompt):
    res = openai_client.chat.completions.create(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track
def generate_topic():
    prompt = "Generate a topic for a story about Opik."
    res = openai_client.chat.completions.create(
        model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track
def generate_opik_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_opik_story()
```

The trace can now be viewed in the UI with hierarchical spans showing the relationship between different steps:

<Frame>
  <img src="/img/cookbook/openai_trace_decorator_cookbook.png" />
</Frame>

## Using Azure OpenAI

The OpenAI integration also supports Azure OpenAI Services. To use Azure OpenAI, initialize your client with Azure configuration and use it with `track_openai` just like the standard OpenAI client:

```python
from opik.integrations.openai import track_openai
from openai import AzureOpenAI

# gets the API Key from environment variable AZURE_OPENAI_API_KEY
azure_client = AzureOpenAI(
    # https://learn.microsoft.com/azure/ai-services/openai/reference#rest-api-versioning
    api_version="2023-07-01-preview",
    # https://learn.microsoft.com/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource
    azure_endpoint="https://example-endpoint.openai.azure.com",
)
azure_client = track_openai(azure_client)

completion = azure_client.chat.completions.create(
    model="deployment-name",  # e.g. gpt-35-instant
    messages=[
        {
            "role": "user",
            "content": "How do I output all files in a directory using Python?",
        },
    ],
)
```

## Cost Tracking

The `track_openai` wrapper automatically tracks token usage and cost for all supported OpenAI models.

Cost information is automatically captured and displayed in the Opik UI, including:

- Token usage details
- Cost per request based on OpenAI pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>

## Grouping traces into conversational threads using `thread_id`

Threads in Opik are collections of traces that are grouped together using a unique `thread_id`.

The `thread_id` can be passed to the OpenAI client as a parameter, which will be used to group all traces into a single thread.

```python
import openai

from opik.integrations.openai import track_openai

client = openai.OpenAI()
wrapped_client = track_openai(
    openai_client=client,
    project_name="opik_args demo",
)
messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is Opik?"},
]

_ = wrapped_client.responses.create(
    model="gpt-4o-mini",
    input=messages,
    opik_args={"trace": {"thread_id": "f174a"}}
)
```

More information on logging chat conversations can be found in the [Log conversations](/tracing/log_chat_conversations) section.

## Supported OpenAI methods

The `track_openai` wrapper supports the following OpenAI methods:

- `openai_client.chat.completions.create()`, including support for stream=True mode.
- `openai_client.beta.chat.completions.parse()`
- `openai_client.beta.chat.completions.stream()`
- `openai_client.responses.create()`

If you would like to track another OpenAI method, please let us know by opening an issue on [GitHub](https://github.com/comet-ml/opik/issues).
