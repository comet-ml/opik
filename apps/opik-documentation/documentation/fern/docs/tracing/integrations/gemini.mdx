---
description: Start here to integrate Opik into your Google Gemini-based genai application
  for end-to-end LLM observability, unit testing, and optimization.
headline: Gemini | Opik Documentation
og:description: Explore how to integrate Gemini models with Opik using Google VertexAI
  for enhanced AI development and model tracing capabilities.
og:site_name: Opik Documentation
og:title: Gemini Models in VertexAI - Opik Integration
title: Observability for Google Gemini (Python) with Opik
---

[Gemini](https://aistudio.google.com/welcome) is a family of multimodal large language models developed by Google DeepMind.

## VertexAI Support

Opik also supports [Google VertexAI](https://cloud.google.com/vertex-ai?hl=en), Google's fully-managed AI development platform that provides access to Gemini models through the `google-genai` package. When using VertexAI, you can leverage the same `track_genai` wrapper with the `google-genai` client configured for VertexAI, allowing you to trace and monitor your Gemini model calls whether you're using the direct Google AI API or through VertexAI's enterprise platform.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=gemini&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=gemini&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=gemini&utm_campaign=opik) for more information.

## Getting Started

### Installation

First, ensure you have both `opik` and `google-genai` packages installed:

```bash
pip install opik google-genai
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring Gemini

In order to configure Gemini, you will need to have your Gemini API Key. See the [following documentation page](https://ai.google.dev/gemini-api/docs/api-key) how to retrieve it.

You can set it as an environment variable:

```bash
export GOOGLE_API_KEY="YOUR_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "GOOGLE_API_KEY" not in os.environ:
    os.environ["GOOGLE_API_KEY"] = getpass.getpass("Enter your Gemini API key: ")
```

## Logging LLM calls

In order to log the LLM calls to Opik, you will need to wrap the Gemini client with `track_genai`. When making calls with that wrapped client, all calls will be logged to Opik:

```python
from google import genai
from opik.integrations.genai import track_genai

os.environ["OPIK_PROJECT_NAME"] = "gemini-integration-demo"

client = genai.Client()
gemini_client = track_genai(client)

prompt = """
Write a short two sentence story about Opik.
"""

response = gemini_client.models.generate_content(
    model="gemini-2.0-flash-001", contents=prompt
)
print(response.text)
```

<Frame>
  <img src="/img/cookbook/gemini_trace_cookbook.png" />
</Frame>

## Using with VertexAI

To use Opik with VertexAI, configure the `google-genai` client for VertexAI and wrap it with `track_genai`:

```python
from google import genai
from opik.integrations.genai import track_genai

# Configure for VertexAI
PROJECT_ID = "your-project-id"
LOCATION = "us-central1"

client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)
vertexai_client = track_genai(client)

# Set project name for organization
os.environ["OPIK_PROJECT_NAME"] = "vertexai-integration-demo"

# Use the wrapped client
response = vertexai_client.models.generate_content(
    model="gemini-2.0-flash-001",
    contents="Write a short story about AI observability."
)
print(response.text)
```

<Frame>
  <img src="/img/cookbook/vertexai_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If Gemini is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from opik import track

@track
def generate_story(prompt):
    response = gemini_client.models.generate_content(
        model="gemini-2.0-flash-001", contents=prompt
    )
    return response.text

@track
def generate_topic():
    prompt = "Generate a topic for a story about Opik."
    response = gemini_client.models.generate_content(
        model="gemini-2.0-flash-001", contents=prompt
    )
    return response.text

@track
def generate_opik_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_opik_story()
```

The trace can now be viewed in the UI with hierarchical spans showing the relationship between different steps:

<Frame>
  <img src="/img/cookbook/gemini_trace_decorator_cookbook.png" />
</Frame>

## Multimodal Content Attachments

The `track_genai` wrapper automatically logs multimodal content parts (images, audio, video) as attachments in your traces. When you send images or other media to Gemini models, they are captured and viewable directly in the Opik UI alongside your trace data.

This makes it easy to:
- Review the exact media content sent to the model
- Debug multimodal prompts
- Audit model inputs for compliance

## Video Generation (Veo)

The `track_genai` wrapper also supports Google's Veo video generation API. When you generate videos, Opik automatically tracks the video creation process and logs the generated video as an attachment when you save it.

```python
import os
import time
import opik
from opik import track, opik_context
from opik.integrations.genai import track_genai
import google.genai as genai
from google.genai.types import HttpOptions, GenerateVideosConfig

os.environ["OPIK_PROJECT_NAME"] = "genai-video-demo"

# Configure for VertexAI (required for Veo)
client = genai.Client(
    vertexai=True,
    http_options=HttpOptions(api_version="v1"),
)
genai_client = track_genai(client)


@track
def generate_video(
    prompt: str,
    number_of_videos: int = 1,
    duration_seconds: int = 4,
    resolution: str = "720p",
    generate_audio: bool = False,
) -> dict:
    """Generate a video using Google's Veo model."""
    # Create video
    operation = genai_client.models.generate_videos(
        model="veo-3.1-fast-generate-preview",
        prompt=prompt,
        config=GenerateVideosConfig(
            duration_seconds=duration_seconds,
            resolution=resolution,
            generate_audio=generate_audio,
            number_of_videos=number_of_videos,
        ),
    )

    # Wait for completion
    with opik.start_as_current_span(name="wait_for_completion") as span:
        while not operation.done:
            time.sleep(10)
            operation = genai_client.operations.get(operation)
            result = {"name": operation.name, "done": operation.done}
            opik_context.update_current_span(output=result)

    # Download all videos if generation succeeded
    if operation.response and operation.response.generated_videos:
        output_paths = []
        for i, generated_video in enumerate(operation.response.generated_videos):
            output_path = f"output_video_{i}.mp4"
            generated_video.video.save(output_path)
            output_paths.append(output_path)
        result["output_paths"] = output_paths

    return result


# Generate videos
generate_video("A golden retriever playing in the snow", number_of_videos=2)
```

The trace will show the full video generation workflow including the video creation, polling, and the generated video as an attachment:

<Frame>
  <img src="/img/tracing/google_genai_veo_trace.png" />
</Frame>

## Cost Tracking

The `track_genai` wrapper automatically tracks token usage and cost for all supported Google AI models.

Cost information is automatically captured and displayed in the Opik UI, including:

- Token usage details
- Cost per request based on Google AI pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>