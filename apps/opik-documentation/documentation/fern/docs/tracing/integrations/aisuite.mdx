---
description: Start here to integrate Opik into your AISuite-based genai application
  for end-to-end LLM observability, unit testing, and optimization.
headline: AISuite | Opik Documentation
og:description: Learn to integrate Opik with aisuite SDK to track API calls, log prompts,
  model usage, and responses effectively.
og:site_name: Opik Documentation
og:title: Integrate AISuite with Opik for Seamless Tracking
title: Observability for AISuite with Opik
---

This guide explains how to integrate Opik with the aisuite Python SDK. By using the `track_aisuite` method provided by opik, you can easily track and evaluate your aisuite API calls within your Opik projects as Opik will automatically log the input prompt, model used, token usage, and response generated.

## Account Setup

[Comet](https://www.comet.com/site?from=llm&utm_source=opik&utm_medium=colab&utm_content=aisuite&utm_campaign=opik) provides a hosted version of the Opik platform, [simply create an account](https://www.comet.com/signup?from=llm&utm_source=opik&utm_medium=colab&utm_content=aisuite&utm_campaign=opik) and grab your API Key.

> You can also run the Opik platform locally, see the [installation guide](https://www.comet.com/docs/opik/self-host/overview/?from=llm&utm_source=opik&utm_medium=colab&utm_content=aisuite&utm_campaign=opik) for more information.

## Getting Started

### Installation

First, ensure you have both `opik` and `aisuite` packages installed:

```bash
pip install opik "aisuite[openai]"
```

### Configuring Opik

Configure the Opik Python SDK for your deployment type. See the [Python SDK Configuration guide](/tracing/sdk_configuration) for detailed instructions on:

- **CLI configuration**: `opik configure`
- **Code configuration**: `opik.configure()`
- **Self-hosted vs Cloud vs Enterprise** setup
- **Configuration files** and environment variables

### Configuring AISuite

In order to configure AISuite, you will need to have your OpenAI API Key. You can [find or create your OpenAI API Key in this page](https://platform.openai.com/settings/organization/api-keys).

You can set it as an environment variable:

```bash
export OPENAI_API_KEY="YOUR_API_KEY"
```

Or set it programmatically:

```python
import os
import getpass

if "OPENAI_API_KEY" not in os.environ:
    os.environ["OPENAI_API_KEY"] = getpass.getpass("Enter your OpenAI API key: ")
```

## Logging LLM calls

In order to log the LLM calls to Opik, you will need to wrap the AISuite client with `track_aisuite`. When making calls with that wrapped client, all calls will be logged to Opik:

```python
from opik.integrations.aisuite import track_aisuite
import aisuite as ai

client = track_aisuite(ai.Client(), project_name="aisuite-integration-demo")

messages = [
    {"role": "user", "content": "Write a short two sentence story about Opik."},
]

response = client.chat.completions.create(
    model="openai:gpt-4o",
    messages=messages,
    temperature=0.75
)
print(response.choices[0].message.content)
```

<Frame>
  <img src="/img/cookbook/aisuite_trace_cookbook.png" />
</Frame>

## Advanced Usage

### Using with the `@track` decorator

If you have multiple steps in your LLM pipeline, you can use the `@track` decorator to log the traces for each step. If AISuite is called within one of these steps, the LLM call will be associated with that corresponding step:

```python
from opik import track
from opik.integrations.aisuite import track_aisuite
import aisuite as ai

client = track_aisuite(ai.Client(), project_name="aisuite-integration-demo")

@track
def generate_story(prompt):
    res = client.chat.completions.create(
        model="openai:gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track
def generate_topic():
    prompt = "Generate a topic for a story about Opik."
    res = client.chat.completions.create(
        model="openai:gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}]
    )
    return res.choices[0].message.content

@track(project_name="aisuite-integration-demo")
def generate_opik_story():
    topic = generate_topic()
    story = generate_story(topic)
    return story

# Execute the multi-step pipeline
generate_opik_story()
```

The trace can now be viewed in the UI with hierarchical spans showing the relationship between different steps:

<Frame>
  <img src="/img/cookbook/aisuite_trace_decorator_cookbook.png" />
</Frame>

## Supported aisuite methods

The `track_aisuite` wrapper supports the following aisuite methods:

- `aisuite.Client.chat.completions.create()`

If you would like to track another aisuite method, please let us know by opening an issue on [GitHub](https://github.com/comet-ml/opik/issues).