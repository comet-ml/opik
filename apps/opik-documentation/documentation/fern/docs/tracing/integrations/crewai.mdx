---
title: Observability for CrewAI with Opik
description: Start here to integrate Opik into your CrewAI-based genai application for end-to-end LLM observability, unit testing, and optimization.
---

[CrewAI](https://www.crewai.com/) is a cutting-edge framework for orchestrating autonomous AI agents.

Opik integrates with CrewAI to log traces for all CrewAI activity, including both classic Crew/Agent/Task pipelines and the new CrewAI Flows API.

<div style="display: flex; align-items: center; flex-wrap: wrap; margin: 20px 0;">
  <span style="margin-right: 10px;">
    You can check out the Colab Notebook if you'd like to jump straight to the code:
  </span>
  <a
    href="https://colab.research.google.com/github/comet-ml/opik/blob/main/apps/opik-documentation/documentation/docs/cookbook/crewai.ipynb"
    target="_blank"
    rel="noopener noreferrer"
  >
    <img
      src="https://colab.research.google.com/assets/colab-badge.svg"
      alt="Open In Colab"
      style="vertical-align: middle;"
    />
  </a>
</div>

<Frame>
  <img src="/img/tracing/crewai/crewai_crew_kickoff_trace_example.png" />
</Frame>

## Getting started

First, ensure you have both `opik` and `crewai` installed:

```bash
pip install opik crewai crewai-tools
```

In addition, you can configure Opik using the `opik configure` command which will prompt you for the correct local server address or if you are using the Cloud platform your API key:

```bash
opik configure
```

## Logging CrewAI calls

To log a CrewAI pipeline run, you can use the [`track_crewai`](https://www.comet.com/docs/opik/python-sdk-reference/integrations/crewai/track_crewai.html). This callback will log each CrewAI call to Opik:

```python
from crewai import Agent, Crew, Task, Process


class YourCrewName:
    def agent_one(self) -> Agent:
        return Agent(
            role="Data Analyst",
            goal="Analyze data trends in the market",
            backstory="An experienced data analyst with a background in economics",
            verbose=True,
        )

    def agent_two(self) -> Agent:
        return Agent(
            role="Market Researcher",
            goal="Gather information on market dynamics",
            backstory="A diligent researcher with a keen eye for detail",
            verbose=True
        )

    def task_one(self) -> Task:
        return Task(
            name="Collect Data Task",
            description="Collect recent market data and identify trends.",
            expected_output="A report summarizing key trends in the market.",
            agent=self.agent_one()
        )

    def task_two(self) -> Task:
        return Task(
            name="Market Research Task",
            description="Research factors affecting market dynamics.",
            expected_output="An analysis of factors influencing the market.",
            agent=self.agent_two()
        )

    def crew(self) -> Crew:
        return Crew(
            agents=[self.agent_one(), self.agent_two()],
            tasks=[self.task_one(), self.task_two()],
            process=Process.sequential,
            verbose=True
        )


from opik.integrations.crewai import track_crewai

track_crewai(project_name="crewai-integration-demo")

my_crew = YourCrewName().crew()
result = my_crew.kickoff()

print(result)
```

Each run will now be logged to the Opik platform.

## Logging CrewAI Flows

Opik also supports the CrewAI Flows API. When you enable tracking with `track_crewai`, Opik automatically:

- Tracks `Flow.kickoff()` and `Flow.kickoff_async()` as the root span/trace with inputs and outputs
- Tracks flow step methods decorated with `@start` and `@listen` as nested spans
- Captures any LLM calls (via LiteLLM) within those steps with token usage
- Flow methods are compatible with other Opik integrations (e.g., OpenAI, Anthropic, LangChain) and the `@opik.track` decorator. Any spans created inside flow steps are correctly attached to the flow's span tree.

Example:

```python
from crewai.flow.flow import Flow, start, listen
from opik.integrations.crewai import track_crewai

track_crewai(project_name="crewai-integration-demo")

class ExampleFlow(Flow):
    model = "gpt-4o-mini"

    @start()
    def generate_city(self):
        from litellm import completion
        response = completion(
            model=self.model,
            messages=[{"role": "user", "content": "Return the name of a random city."}],
        )
        return response["choices"][0]["message"]["content"]

    @listen(generate_city)
    def generate_fun_fact(self, random_city):
        from litellm import completion
        response = completion(
            model=self.model,
            messages=[{"role": "user", "content": f"Tell me a fun fact about {random_city}"}],
        )
        return response["choices"][0]["message"]["content"]

flow = ExampleFlow()
result = flow.kickoff()
```

## Cost Tracking

The `track_crewai` integration automatically tracks token usage and cost for all supported LLM models used during CrewAI agent execution.

Cost information is automatically captured and displayed in the Opik UI, including:
- Token usage details
- Cost per request based on model pricing
- Total trace cost

<Tip>
  View the complete list of supported models and providers on the [Supported Models](/tracing/supported_models) page.
</Tip>
