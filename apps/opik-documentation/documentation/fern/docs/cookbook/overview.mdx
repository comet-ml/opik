---
title: Opik Cookbooks
subtitle: An open-source collection of notebooks and guides for using the Opik platform.
---

## Guides

<Card title="Quickstart Notebook" icon="link" href="/cookbook/quickstart_notebook">
  If you are looking at learning more about the Opik platform, the quickstart notebook is a comprehensive overview of
  the full platform covering both the tracing and the evaluation functionality.
</Card>

## Advanced guides

The advanced guides cover more advanced usage of the Opik platform.

<CardGroup cols={2}>
  <Card title="Evaluate Hallucination Metric" href="/docs/opik/cookbook/evaluate_hallucination_metric">
    In this guide, we evaluate the hallucination metric that is included with the Opik platform.
  </Card>
  <Card title="Evaluate Moderation Metric" href="/docs/opik/cookbook/evaluate_moderation_metric">
    In this guide, we evaluate the moderation metric that is included with the Opik platform.
  </Card>
</CardGroup>

## Integration examples

Opik provides first-class support for many popular LLM frameworks and providers. Choose your integration below to get started:

### LLM Providers

<CardGroup cols={3}>
  <Card title="OpenAI" href="/docs/opik/cookbook/openai">
    Log all OpenAI LLM calls to Opik
  </Card>
  <Card title="Anthropic" href="/docs/opik/cookbook/anthropic">
    Log all Anthropic LLM calls to Opik
  </Card>
  <Card title="Bedrock" href="/docs/opik/cookbook/bedrock">
    AWS Bedrock is a managed service for high performing foundational models
  </Card>
  <Card title="Gemini" href="/docs/opik/cookbook/gemini">
    Gemini is a family of multimodal large language models developed by Google DeepMind
  </Card>
  <Card title="Groq" href="/docs/opik/cookbook/groq">
    Groq provides fast LLM inference for Open Source models
  </Card>
  <Card title="Ollama" href="/docs/opik/cookbook/ollama">
    Ollama allows you to run open-source LLM models on your local machine
  </Card>
  <Card title="watsonx" href="/docs/opik/cookbook/watsonx">
    IBM's platform for deploying ML models
  </Card>
</CardGroup>

### Frameworks & Tools

<CardGroup cols={3}>
  <Card title="LangChain" href="/docs/opik/cookbook/langchain">
    LangChain is a framework for developing applications powered by LLMs
  </Card>
  <Card title="LlamaIndex" href="/docs/opik/cookbook/llama-index">
    LlamaIndex is a framework for building agentic applications
  </Card>
  <Card title="Haystack" href="/docs/opik/cookbook/haystack">
    Build production-ready LLM applications
  </Card>
  <Card title="Instructor" href="/docs/opik/cookbook/instructor">
    The Instructor library helps you work with structured outputs from LLMs
  </Card>
  <Card title="LiteLLM" href="/docs/opik/cookbook/litellm">
    LiteLLM allows you to call all LLM APIs using the OpenAI format
  </Card>
  <Card title="CrewAI" href="/docs/opik/cookbook/crewai">
    CrewAi can be used to create AI agent teams that work together to tackle complex tasks
  </Card>
  <Card title="DSPy" href="/docs/opik/cookbook/dspy">
    DSPy is an LLM optimization framework for prompt engineering
  </Card>
  <Card title="Guardrails" href="/docs/opik/cookbook/guardrails-ai">
    Guardrails is a framework for detecting and preventing errors in LLM applications
  </Card>
  <Card title="LangGraph" href="/docs/opik/cookbook/langgraph">
    LangGraph is a framework for building agentic applications built by the LangChain team
  </Card>
  <Card title="aisuite" href="/docs/opik/cookbook/aisuite">
    Simple, unified interface to multiple Generative AI providers
  </Card>
  <Card title="Predibase" href="/docs/opik/cookbook/predibase">
    Predibase provides the fastest way to fine-tune and serve open-source LLMs
  </Card>
  <Card title="Ragas" href="/docs/opik/cookbook/ragas">
    Ragas is a framework for evaluating Retrieval Augmented Generation (RAG) pipelines
  </Card>
</CardGroup>

Don't see your preferred framework or tool? [Open an issue](https://github.com/comet-ml/opik/issues) to request it! In the meantime, you can use our SDK's core logging functions to track your LLM interactions - check out our [tracing documentation](/docs/opik/tracing/log_traces) for details.
