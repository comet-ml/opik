{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pydantic AI Integration via OpenTelemetry\n",
    "\n",
    "Opik offers an [OpenTelemetry backend](https://www.comet.com/docs/opik/tracing/opentelemetry/overview) that ingests trace data from a variety of OpenTelemetry instrumentation libraries. In this guide, we demonstrate how to use the Pydantic Logfire instrumentation library to instrument your Pydantic AI agents.\n",
    "\n",
    "> **About PydanticAI:** [PydanticAI](https://pydantic-ai.readthedocs.io/en/latest/) is a Python agent framework designed to simplify the development of production-grade generative AI applications. It brings the same type-safety, ergonomic API design, and developer experience found in FastAPI to the world of GenAI app development. \n",
    "\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "Before you begin, install the necessary Python packages. The command below will install the `pydantic-ai` package along with Logfire support, which is required for trace ingestion via Opik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "p7zqVoQVAzii",
    "outputId": "69c960b8-7645-42ee-ce2c-a615653678e1"
   },
   "outputs": [],
   "source": [
    "%pip install pydantic-ai[logfire]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configure Environment Variables\n",
    "\n",
    "To forward trace data to Opik, you must set up the required environment variables. This includes providing your Opik API keys and the proper OpenTelemetry exporter endpoint. Additionally, you need to specify your OpenAI API key if you are using OpenAI for your generative tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhj-dTnXBii3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "OPIK_API_KEY = None\n",
    "OPIK_PROJECT_NAME = \"pydantic-ai-integration\"\n",
    "OPIK_WORKSPACE = \"lothiraldan\"\n",
    "\n",
    "if OPIK_API_KEY is None and \"OPIK_API_KEY\" not in os.environ:\n",
    "    OPIK_API_KEY = getpass.getpass(\"Enter your OPIK API key: \")\n",
    "elif OPIK_API_KEY is None:\n",
    "    OPIK_API_KEY = os.environ[\"OPIK_API_KEY\"]\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = (\n",
    "    \"https://www.comet.com/opik/api/v1/private/otel\"  # Opik Cloud\n",
    ")\n",
    "# os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = \"https://<YOUR-OPIK-INSTANCE>/api/v1/private/otel\" # Opik self-hosted\n",
    "\n",
    "headers = [f\"Authorization={OPIK_API_KEY}\"]\n",
    "\n",
    "if OPIK_PROJECT_NAME is not None:\n",
    "    headers.append(f\"projectName={OPIK_PROJECT_NAME}\")\n",
    "\n",
    "if OPIK_WORKSPACE is not None:\n",
    "    headers.append(f\"Comet-Workspace={OPIK_WORKSPACE}\")\n",
    "\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \",\".join(headers)\n",
    "\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\",\".join(headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Initialize Instrumentation\n",
    "\n",
    "Now, initialize Logfire’s instrumentation and define a sample Pydantic AI agent that makes use of dependency injection and tool registration. In this example, we create a \"roulette\" agent. The agent is configured to call a tool function (`roulette_wheel`), which checks if a given square is a winner. The agent is type-safe, ensuring that the dependency (`deps_type`) and the output (`result_type`) have defined types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYU7iIWACUyC"
   },
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysE9xblmBv2V",
    "outputId": "80168afd-6470-4cb1-8cb4-d4a0ae59c85e"
   },
   "outputs": [],
   "source": [
    "import logfire\n",
    "\n",
    "logfire_ = logfire.configure(\n",
    "    service_name=\"my_logfire_service\",\n",
    "    # Sending to Logfire is on by default regardless of the OTEL env vars.\n",
    "    send_to_logfire=False,\n",
    ")\n",
    "\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor, ConsoleSpanExporter\n",
    "\n",
    "tracer_provider = logfire_.config.get_tracer_provider()\n",
    "\n",
    "processor = BatchSpanProcessor(ConsoleSpanExporter())\n",
    "tracer_provider.add_span_processor(processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozWO44i8AxV3"
   },
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "\n",
    "roulette_agent = Agent(\n",
    "    \"openai:gpt-4o\",\n",
    "    deps_type=int,\n",
    "    result_type=bool,\n",
    "    system_prompt=(\n",
    "        \"Use the `roulette_wheel` function to see if the \"\n",
    "        \"customer has won based on the number they provide.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "@roulette_agent.tool\n",
    "async def roulette_wheel(ctx: RunContext[int], square: int) -> str:\n",
    "    \"\"\"check if the square is a winner\"\"\"\n",
    "    return \"winner\" if square == ctx.deps else \"loser\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run the Agent\n",
    "\n",
    "Finally, run your Pydantic AI agent and generate trace data that will be sent to Opik. In the example below, the agent is executed with a dependency value (the winning square) and natural language input. The output from the tool function is then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4Ic6U7DB7vO",
    "outputId": "756da737-7c58-4c46-a578-941815ca5d2a"
   },
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "\n",
    "\n",
    "async def main():\n",
    "    success_number = 18\n",
    "    result = await roulette_agent.run(\n",
    "        \"Put my money on square eighteen\", deps=success_number\n",
    "    )\n",
    "    print(result.data)\n",
    "\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider.force_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations as _annotations\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any\n",
    "\n",
    "import logfire\n",
    "from httpx import AsyncClient\n",
    "\n",
    "from pydantic_ai import Agent, ModelRetry, RunContext\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Deps:\n",
    "    client: AsyncClient\n",
    "    weather_api_key: str | None\n",
    "    geo_api_key: str | None\n",
    "\n",
    "\n",
    "weather_agent = Agent(\n",
    "    \"openai:gpt-4o\",\n",
    "    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n",
    "    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n",
    "    system_prompt=(\n",
    "        \"Be concise, reply with one sentence.\"\n",
    "        \"Use the `get_lat_lng` tool to get the latitude and longitude of the locations, \"\n",
    "        \"then use the `get_weather` tool to get the weather.\"\n",
    "    ),\n",
    "    deps_type=Deps,\n",
    "    retries=2,\n",
    "    instrument=True,\n",
    ")\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_lat_lng(\n",
    "    ctx: RunContext[Deps], location_description: str\n",
    ") -> dict[str, float]:\n",
    "    \"\"\"Get the latitude and longitude of a location.\n",
    "\n",
    "    Args:\n",
    "        ctx: The context.\n",
    "        location_description: A description of a location.\n",
    "    \"\"\"\n",
    "    if ctx.deps.geo_api_key is None:\n",
    "        # if no API key is provided, return a dummy response (London)\n",
    "        return {\"lat\": 51.1, \"lng\": -0.1}\n",
    "\n",
    "    params = {\n",
    "        \"q\": location_description,\n",
    "        \"api_key\": ctx.deps.geo_api_key,\n",
    "    }\n",
    "    with logfire.span(\"calling geocode API\", params=params) as span:\n",
    "        r = await ctx.deps.client.get(\"https://geocode.maps.co/search\", params=params)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        span.set_attribute(\"response\", data)\n",
    "\n",
    "    if data:\n",
    "        return {\"lat\": data[0][\"lat\"], \"lng\": data[0][\"lon\"]}\n",
    "    else:\n",
    "        raise ModelRetry(\"Could not find the location\")\n",
    "\n",
    "\n",
    "@weather_agent.tool\n",
    "async def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n",
    "    \"\"\"Get the weather at a location.\n",
    "\n",
    "    Args:\n",
    "        ctx: The context.\n",
    "        lat: Latitude of the location.\n",
    "        lng: Longitude of the location.\n",
    "    \"\"\"\n",
    "    if ctx.deps.weather_api_key is None:\n",
    "        # if no API key is provided, return a dummy response\n",
    "        return {\"temperature\": \"21 °C\", \"description\": \"Sunny\"}\n",
    "\n",
    "    params = {\n",
    "        \"apikey\": ctx.deps.weather_api_key,\n",
    "        \"location\": f\"{lat},{lng}\",\n",
    "        \"units\": \"metric\",\n",
    "    }\n",
    "    with logfire.span(\"calling weather API\", params=params) as span:\n",
    "        r = await ctx.deps.client.get(\n",
    "            \"https://api.tomorrow.io/v4/weather/realtime\", params=params\n",
    "        )\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        span.set_attribute(\"response\", data)\n",
    "\n",
    "    values = data[\"data\"][\"values\"]\n",
    "    # https://docs.tomorrow.io/reference/data-layers-weather-codes\n",
    "    code_lookup = {\n",
    "        1000: \"Clear, Sunny\",\n",
    "        1100: \"Mostly Clear\",\n",
    "        1101: \"Partly Cloudy\",\n",
    "        1102: \"Mostly Cloudy\",\n",
    "        1001: \"Cloudy\",\n",
    "        2000: \"Fog\",\n",
    "        2100: \"Light Fog\",\n",
    "        4000: \"Drizzle\",\n",
    "        4001: \"Rain\",\n",
    "        4200: \"Light Rain\",\n",
    "        4201: \"Heavy Rain\",\n",
    "        5000: \"Snow\",\n",
    "        5001: \"Flurries\",\n",
    "        5100: \"Light Snow\",\n",
    "        5101: \"Heavy Snow\",\n",
    "        6000: \"Freezing Drizzle\",\n",
    "        6001: \"Freezing Rain\",\n",
    "        6200: \"Light Freezing Rain\",\n",
    "        6201: \"Heavy Freezing Rain\",\n",
    "        7000: \"Ice Pellets\",\n",
    "        7101: \"Heavy Ice Pellets\",\n",
    "        7102: \"Light Ice Pellets\",\n",
    "        8000: \"Thunderstorm\",\n",
    "    }\n",
    "    return {\n",
    "        \"temperature\": f'{values[\"temperatureApparent\"]:0.0f}°C',\n",
    "        \"description\": code_lookup.get(values[\"weatherCode\"], \"Unknown\"),\n",
    "    }\n",
    "\n",
    "\n",
    "async def main():\n",
    "    async with AsyncClient() as client:\n",
    "        # create a free API key at https://www.tomorrow.io/weather-api/\n",
    "        weather_api_key = os.getenv(\"WEATHER_API_KEY\")\n",
    "        # create a free API key at https://geocode.maps.co/\n",
    "        geo_api_key = os.getenv(\"GEO_API_KEY\")\n",
    "        deps = Deps(\n",
    "            client=client, weather_api_key=weather_api_key, geo_api_key=geo_api_key\n",
    "        )\n",
    "        result = await weather_agent.run(\n",
    "            \"What is the weather like in Strasbourg?\", deps=deps\n",
    "        )\n",
    "        print(\"Response:\", result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer_provider.force_flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Explore Traces in Opik\n",
    "\n",
    "With the instrumentation in place, all trace data generated by the agent will be sent to Opik. You can view detailed trace logs—including operation timings, debugging information, and performance metrics—by accessing your Opik dashboard. For example, check out a [sample trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0194c8b3c1fbb67529f717d4009a310b?timestamp=2025-02-02T22%3A06%3A51.387Z) to see the flow of a Pydantic AI request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXjZqGq6tUso"
   },
   "source": [
    "[Example trace in Langfuse](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/0194c8b3c1fbb67529f717d4009a310b?timestamp=2025-02-02T22%3A06%3A51.387Z)\n",
    "\n",
    "![Pydantic AI OpenAI Trace](https://langfuse.com/images/cookbook/otel-integration-pydantic-ai/pydanticai-openai-trace-tree.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "851sPORkCKQl",
    "outputId": "327666e0-5f34-48d6-b2f4-2e9f773f8c1f"
   },
   "outputs": [],
   "source": [
    "# result = roulette_agent.run_sync(\"I bet five is the winner\", deps=success_number)\n",
    "# print(result.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
