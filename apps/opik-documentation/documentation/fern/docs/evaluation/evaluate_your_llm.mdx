---
subtitle: Step by step guide on how to evaluate your LLM application
---

Evaluating your LLM application allows you to have confidence in the performance of your LLM application. In this guide, we will walk through the process of evaluating complex applications like LLM chains or agents.

<Tip>
  In this guide, we will focus on evaluating complex LLM applications. If you are looking at evaluating single prompts
  you can refer to the [Evaluate A Prompt](/evaluation/evaluate_prompt) guide.
</Tip>

The evaluation is done in five steps:

1. Add tracing to your LLM application
2. Define the evaluation task
3. Choose the `Dataset` that you would like to evaluate your application on
4. Choose the metrics that you would like to evaluate your application with
5. Create and run the evaluation experiment

## 1. Add tracking to your LLM application

While not required, we recommend adding tracking to your LLM application. This allows you to have full visibility into each evaluation run. In the example below we will use a combination of the `track` decorator and the `track_openai` function to trace the LLM application.

```python
from opik import track
from opik.integrations.openai import track_openai
import openai

openai_client = track_openai(openai.OpenAI())

# This method is the LLM application that you want to evaluate
# Typically this is not updated when creating evaluations
@track
def your_llm_application(input: str) -> str:
    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": input}],
    )

    return response.choices[0].message.content
```

<Tip>
  Here we have added the `track` decorator so that this trace and all its nested steps are logged to the platform for
  further analysis.
</Tip>

## 2. Define the evaluation task

Once you have added instrumentation to your LLM application, we can define the evaluation task. The evaluation task takes in as an input a dataset item and needs to return a dictionary with keys that match the parameters expected by the metrics you are using. In this example we can define the evaluation task as follows:

```python
def evaluation_task(x):
    return {
        "output": your_llm_application(x['user_question'])
    }
```

<Warning>
  If the dictionary returned does not match with the parameters expected by the metrics, you will get inconsistent
  evaluation results.
</Warning>

## 3. Choose the evaluation Dataset

In order to create an evaluation experiment, you will need to have a Dataset that includes all your test cases.

If you have already created a Dataset, you can use the [`Opik.get_or_create_dataset`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.get_or_create_dataset) function to fetch it:

```python
from opik import Opik

client = Opik()
dataset = client.get_or_create_dataset(name="Example dataset")
```

If you don't have a Dataset yet, you can insert dataset items using the [`Dataset.insert`](https://www.comet.com/docs/opik/python-sdk-reference/evaluation/Dataset.html#opik.Dataset.insert) method. You can call this method multiple times as Opik performs data deduplication before ingestion:

```python
from opik import Opik

client = Opik()
dataset = client.get_or_create_dataset(name="Example dataset")

dataset.insert([
    {"input": "Hello, world!", "expected_output": "Hello, world!"},
    {"input": "What is the capital of France?", "expected_output": "Paris"},
])
```

## 4. Choose evaluation metrics

Opik provides a set of built-in evaluation metrics that you can choose from. These are broken down into two main categories:

1. Heuristic metrics: These metrics that are deterministic in nature, for example `equals` or `contains`
2. LLM-as-a-judge: These metrics use an LLM to judge the quality of the output; typically these are used for detecting `hallucinations` or `context relevance`

In the same evaluation experiment, you can use multiple metrics to evaluate your application:

```python
from opik.evaluation.metrics import Hallucination

hallucination_metric = Hallucination()
```

<Tip>
  Each metric expects the data in a certain format. You will need to ensure that the task you have defined in step 1
  returns the data in the correct format.
</Tip>

## 5. Run the evaluation

Now that we have the task we want to evaluate, the dataset to evaluate on, and the metrics we want to evaluate with, we can run the evaluation:

```python
from opik import Opik, track
from opik.evaluation import evaluate
from opik.evaluation.metrics import Equals, Hallucination
from opik.integrations.openai import track_openai
import openai

# Define the task to evaluate
openai_client = track_openai(openai.OpenAI())

MODEL = "gpt-3.5-turbo"

@track
def your_llm_application(input: str) -> str:
    response = openai_client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": input}],
    )
    return response.choices[0].message.content

# Define the evaluation task
def evaluation_task(x):
    return {
        "output": your_llm_application(x['input'])
    }

# Create a simple dataset
client = Opik()
dataset = client.get_or_create_dataset(name="Example dataset")
dataset.insert([
    {"input": "What is the capital of France?"},
    {"input": "What is the capital of Germany?"},
])

# Define the metrics
hallucination_metric = Hallucination()

evaluation = evaluate(
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[hallucination_metric],
    experiment_config={
        "model": MODEL
    }
)
```

<Tip>
  You can use the `experiment_config` parameter to store information about your evaluation task. Typically we see teams
  store information about the prompt template, the model used and model parameters used to evaluate the application.
</Tip>

## Advanced usage

### Using async evaluation tasks

The `evaluate` function does not support `async` evaluation tasks, if you pass
an async task you will get an error similar to:

```python wordWrap
Input should be a valid dictionary [type=dict_type, input_value='<coroutine object kyc_qu...ng_task at 0x3336d0a40>', input_type=str]
```

As it might not always be possible to convert all your LLM logic to not rely on async logic,
we recommend using `asyncio.run` within the evaluation task:

```python
import asyncio

async def your_llm_application(input: str) -> str:
    return "Hello, World"

def evaluation_task(x):
    # your_llm_application here is an async function
    result = asyncio.run(your_llm_application(x['input']))
    return {
        "output": result
    }
```

This should solve the issue and allow you to run the evaluation.

<Tip>
If you are running in a Jupyter notebook, you will need to add the following line to the top of your notebook:

```python
import nest_asyncio
nest_asyncio.apply()
```

otherwise you might get the error `RuntimeError: asyncio.run() cannot be called from a running event loop`

</Tip>

<Warning>
  The `evaluate` function uses multi-threading under the hood to speed up the evaluation run. Using both
  `asyncio` and multi-threading can lead to unexpected behavior and hard to debug errors.

If you run into any issues, you can disable the multi-threading in the SDK by setting `task_threads` to 1:

```python
evaluation = evaluate(
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[hallucination_metric],
    task_threads=1
)
```

</Warning>

### Missing arguments for scoring methods

When you face the `opik.exceptions.ScoreMethodMissingArguments` exception, it means that the dataset item and task output dictionaries do not contain all the arguments expected by the scoring method. The way the evaluate function works is by merging the dataset item and task output dictionaries and then passing the result to the scoring method. For example, if the dataset item contains the keys `user_question` and `context` while the evaluation task returns a dictionary with the key `output`, the scoring method will be called as `scoring_method.score(user_question='...', context= '...', output= '...')`. This can be an issue if the scoring method expects a different set of arguments.

You can solve this by either updating the dataset item or evaluation task to return the missing arguments or by using the `scoring_key_mapping` parameter of the `evaluate` function. In the example above, if the scoring method expects `input` as an argument, you can map the `user_question` key to the `input` key as follows:

```python
evaluation = evaluate(
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[hallucination_metric],
    scoring_key_mapping={"input": "user_question"},
)
```

### Linking prompts to experiments

The [Opik prompt library](/prompt_engineering/prompt_management) can be used to version your prompt templates.

When creating an Experiment, you can link the Experiment to a specific prompt version:

```python
import opik

# Create a prompt
prompt = opik.Prompt(
    name="My prompt",
    prompt="..."
)

# Run the evaluation
evaluation = evaluate(
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[hallucination_metric],
    prompt=prompt,
)
```

The experiment will now be linked to the prompt allowing you to view all experiments that use a specific prompt:

<Frame>
  <img src="/img/evaluation/linked_prompt.png" />
</Frame>

### Logging traces to a specific project

You can use the `project_name` parameter of the `evaluate` function to log evaluation traces to a specific project:

```python
evaluation = evaluate(
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[hallucination_metric],
    project_name="hallucination-detection",
)
```

### Evaluating a subset of the dataset

You can use the `nb_samples` parameter to specify the number of samples to use for the evaluation. This is useful if you only want to evaluate a subset of the dataset.

```python
evaluation = evaluate(
    experiment_name="My experiment",
    dataset=dataset,
    task=evaluation_task,
    scoring_metrics=[hallucination_metric],
    nb_samples=10,
)
```

### Disabling threading

In order to evaluate datasets more efficiently, Opik uses multiple background threads to evaluate the dataset. If this is causing issues, you can disable these by setting `task_threads` and `scoring_threads` to `1` which will lead Opik to run all calculations in the main thread.

### Passing additional arguments to `evaluation_task`

Sometimes your evaluation task needs extra context besides the dataset item (commonly referred to as `x`). For example, you may want to pass a model name, a system prompt, or a pre-initialized client. Since `evaluate` calls the task as `task(x)` for each dataset item, the recommended pattern is to create a wrapper (or use `functools.partial`) that closes over any additional arguments.

Using a wrapper function:

```python
# Extra dependencies you want to provide to the task
MODEL = "gpt-4o"
IMAGE_TYPE = "thumbnail"

def evaluation_task(x, model, image_type, client, prompt):
    full_response = client.get_answer(
        x["question"],
        x["image_paths"][image_type],
        prompt.format(),
        model=model,
    )
    response = full_response["response"]
    return {
        "response": response,
        "bbox": full_response.get("bounding_boxes"),
        "image_url": full_response.get("image_url"),
    }

def make_task(model, image_type, client, prompt):
    # Return a unary function that evaluate() can call as task(x)
    def _task(x):
        return evaluation_task(x, model, image_type, client, prompt)
    return _task

task = make_task(MODEL, IMAGE_TYPE, bot, system_prompt)

evaluation = evaluate(
    dataset=dataset,
    task=task,  # evaluate will call task(x) for each item
    scoring_metrics=[levenshteinratio_metric],
    scoring_key_mapping={
        "input": "question",
        "output": "response",
        "reference": "expected_answer",
    },
)
```

Using `functools.partial`:

```python
from functools import partial

def evaluation_task(x, model, image_type, client, prompt):
    full_response = client.get_answer(
        x["question"],
        x["image_paths"][image_type],
        prompt.format(),
        model=model,
    )
    response = full_response["response"]
    return {
        "response": response,
        "bbox": full_response.get("bounding_boxes"),
        "image_url": full_response.get("image_url"),
    }

task = partial(
    evaluation_task,
    model=MODEL,
    image_type=IMAGE_TYPE,
    client=bot,
    prompt=system_prompt,
)

evaluation = evaluate(
    dataset=dataset,
    task=task,  # evaluate will call task(x)
    scoring_metrics=[levenshteinratio_metric],
    scoring_key_mapping={
        "input": "question",
        "output": "response",
        "reference": "expected_answer",
    },
)
```

Notes:
- `evaluate` merges keys from the dataset item and the task output to satisfy metric inputs. Passing extra args via closures/partial ensures `evaluation_task` remains unary (`task(x)`), while still having access to additional context.
- This pattern is standard for callback-style APIs and avoids modifying `evaluate`’s calling convention.

### Accessing logged experiments

You can access all the experiments logged to the platform from the SDK with the [`Opik.get_experiments_by_name`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.get_experiment_by_name) and [`Opik.get_experiment_by_id`](https://www.comet.com/docs/opik/python-sdk-reference/Opik.html#opik.Opik.get_experiment_by_id) methods:

```python {pytest_codeblocks_skip=true}
import opik

# Get the experiment
opik_client = opik.Opik()
experiments = opik_client.get_experiments_by_name("My experiment")

# Access the first experiment content
items = experiments[0].get_items()
print(items)
```
