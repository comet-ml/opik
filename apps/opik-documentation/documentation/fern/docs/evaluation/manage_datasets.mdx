---
headline: Manage datasets | Opik Documentation
og:description: Evaluate your LLM using datasets. Learn to create and manage them
  via Python SDK, TypeScript SDK, or the Traces table.
og:site_name: Opik Documentation
og:title: Manage datasets effectively with Opik
subtitle: Guides you through the process of creating and managing datasets
title: Manage datasets
---

Use datasets to store test cases for evaluating your LLM. Each dataset is a collection of items—dictionaries with whatever key-value pairs you need. We recommend an `input` field and, optionally, `expected_output`.

You use datasets to [run evaluations and experiments](#run-evaluation-and-experiments-on-a-dataset) in the [Playground](/prompt_engineering/playground) or via the [SDK](/evaluation/overview), and when [evaluating your LLM](/evaluation/evaluate_your_llm) or [evaluating prompts](/evaluation/evaluate_prompt). Each run is tied to a dataset version so results are reproducible.

## Quickstart

1. **[Create a dataset](#create-a-dataset)** — In the UI (Evaluation > Datasets) or via the SDK with `get_or_create_dataset`.
2. **[Add data](#add-data-to-your-dataset)** — Add traces from the Traces table, or insert items via the SDK (dictionaries, JSONL, or Pandas).
3. **[Run an evaluation or experiment](#run-evaluation-and-experiments-on-a-dataset)** — In the Playground or via the SDK; each run is tied to a dataset version.

## Create a dataset

### Through the UI

If you're starting from a CSV, the UI is the fastest way to bootstrap.

1. Go to **Evaluation > Datasets**.
2. Click **Create new dataset**.
3. Give it a name (and optional description), and optionally upload a CSV.
4. Click **Create dataset**.

<Frame>
  <img src="/img/evaluation/create_dataset.png" />
</Frame>

<Tip>
  The UI supports up to 1,000 rows per CSV and no nested JSON. For larger or richer data, use the SDK (below).
</Tip>

### Through the SDK

Create or reuse a dataset with `get_or_create_dataset`; if the name already exists, you get the existing dataset.

<CodeBlocks>
```typescript title="TypeScript SDK" language="typescript"
import { Opik } from "opik";

const client = new Opik();
const dataset = await client.getOrCreateDataset("My dataset");
```

```python title="Python SDK" language="python"
from opik import Opik

client = Opik()
dataset = client.get_or_create_dataset(name="My dataset")
```

</CodeBlocks>


## Add data to your dataset

### From traces (UI)

Turn production traces into dataset items for realistic test cases.

1. Go to the traces page, select one or more traces, and click **Add to dataset**.
2. In the dialog: pick an existing dataset (or create one), then choose which metadata to include (nested spans, tags, feedback scores, comments, usage, metadata). Input and output are always included; all options are on by default—uncheck what you don't need.
3. Click the dataset name to add the selected traces.

<Frame>
  <img src="/img/evaluation/add_traces_to_dataset.png" alt="Add traces to dataset modal" />
</Frame>

#### What gets added

Each trace becomes a dataset item with: **input**, **expected_output** (the trace output, for evaluation), and optionally **spans**, **tags**, **feedback_scores**, **comments**, **usage**, and **metadata**.

### From code (SDK)

<Tabs>
    <Tab value="Dictionary" title="Dictionary items">

Use `insert` to add items:

<CodeBlocks>
```typescript title="TypeScript" language="typescript"
import { Opik } from "opik";
const client = new Opik();
const dataset = await client.getOrCreateDataset("My dataset");

await dataset.insert([
  { user_question: "Hello, world!", expected_output: { assistant_answer: "Hello, world!" } },
  { user_question: "What is the capital of France?", expected_output: { assistant_answer: "Paris" } },
]);
```

```python title="Python" language="python"
import opik

# Get or create a dataset
client = opik.Opik()
dataset = client.get_or_create_dataset(name="My dataset")

# Add dataset items to it
dataset.insert([
    {"user_question": "Hello, world!", "expected_output": {"assistant_answer": "Hello, world!"}},
    {"user_question": "What is the capital of France?", "expected_output": {"assistant_answer": "Paris"}},
])
```

</CodeBlocks>

</Tab>
    <Tab value="JSONL" title="From a JSONL file (Python)">

<CodeBlocks>
```python title="Python" language="python"
import opik

client = opik.Opik()
dataset = client.get_or_create_dataset(name="My dataset")

dataset.read_jsonl_from_file("path/to/file.jsonl")
```
</CodeBlocks>

</Tab>
    <Tab value="Pandas" title="From a Pandas DataFrame (Python)">

<CodeBlocks>
```python title="Python" language="python"
import opik

client = opik.Opik()
dataset = client.get_or_create_dataset(name="My dataset")

dataset.insert_from_pandas(dataframe=df)

# You can also specify an optional keys_mapping parameter
dataset.insert_from_pandas(dataframe=df, keys_mapping={"Expected output": "expected_output"})
```
</CodeBlocks>

</Tab>
</Tabs>

<Tip>
  A new dataset version is created automatically whenever you insert or delete items (or run other mutating operations)—there's no separate save step. See [Iterate safely: versioning](#iterate-safely-versioning).
</Tip>

<Tip>
  The Python SDK deduplicates inserts—you can insert the same item multiple times without duplicates.
</Tip>

You can view inserted items in the Opik UI:

<Frame>
  <img src="/img/evaluation/dataset_items_page.png" />
</Frame>

## Expanding a dataset with AI

Use AI to generate extra synthetic samples from your existing data—handy when you have a small dataset and want more diverse test cases. The AI follows patterns in your data and produces similar-but-varied items (including edge cases and different difficulty levels). New samples go into your **draft**; review and save when ready (see [Draft mode (UI)](#draft-mode-ui)).

### How to expand

1. Open your dataset (Evaluation > Datasets > [Your Dataset]) and click **Expand with AI**.
2. Configure: **Model** (GPT-4, GPT-5, Claude, etc.), **Sample count** (1–100), **Preserve fields** (which columns to keep fixed), **Variation instructions** (e.g. "Create edge cases" or "Different complexity levels"), and optionally a **Custom prompt**.
3. Run the expansion, then review the generated items in the draft and save to create a new version.

<Frame>
  <img src="/img/evaluation/dataset_expansion_modal.png" />
</Frame>

<Tip>
  Expansion works best with at least 5–10 strong examples. Start with 10–20 samples to check quality before scaling up.
</Tip>

## Run evaluation and experiments on a dataset

Experiments are tied to a dataset version so results are reproducible. Opik records which version was used; the results page shows it and you can click through to the data. If you don't pick a version, `latest` is used. That link is permanent—later changes to the dataset don't affect past results.

### In the Playground

In the Playground: choose your dataset in the dataset selector, then use the nested version dropdown to pick a specific version or `latest`.

<Frame>
  <img src="/img/evaluation/dataset_version_select.png" />
</Frame>

<Tip>
  When comparing experiments or A/B testing, use the same dataset version so differences in results come from your prompt or model, not from data changes.
</Tip>

### In the SDK

When you run experiments via the SDK (`evaluate()` in Python or TypeScript), the **latest** dataset version is always used. The SDK does not accept a dataset version parameter.

<CodeBlocks>
```python title="Python" language="python"
from opik import Opik
from opik.evaluation import evaluate

client = Opik()
dataset = client.get_dataset(name="My dataset")

result = evaluate(
    experiment_name="baseline-experiment",
    dataset=dataset,
    task=my_task_function,
    scoring_metrics=[my_metric],
)
```

```typescript title="TypeScript" language="typescript"
import { Opik, evaluate } from "opik";

const client = new Opik();
const dataset = await client.getDataset("My dataset");

const result = await evaluate({
  experimentName: "baseline-experiment",
  dataset: dataset,
  task: myTaskFunction,
  scoringMetrics: [myMetric],
});
```
</CodeBlocks>

## Iterate safely: versioning

Opik versions datasets as **immutable snapshots**. Every change—add, edit, or delete—creates a new version. You get reproducibility, an audit trail, and the ability to roll back. **Version tags** (e.g. `latest`, `baseline`) label snapshots; **item tags** label individual rows for filtering (see [Item tags](#item-tags)). The `latest` tag always points to the most recent version; experiments default to `latest` if you don't specify one. Restoring a version creates a *new* version with that data; history is never overwritten.

<Tip>
  **When versions are created:** Uploading a CSV at creation (v1), saving changes in the UI (draft → saved), insert/delete/clear via the SDK, or bulk deletes in the UI—each of these creates a new version. Each version has an auto-generated name (v1, v2, …), optional change note and tags (e.g. `production`, `baseline`), item stats, and timestamp.
</Tip>

### Draft mode (UI)

In the UI, changes live in a **draft** until you save. The draft is only visible to you; "Expand with AI" samples go there too. When there are unsaved changes, an orange **Draft** tag and **Save changes** / **Discard changes** appear. Items get colored borders: green for new, amber for modified.

To save: click **Save changes**, enter a version note, optionally add tags, then **Save**. To discard: **Discard changes** and confirm. Navigating away with unsaved changes triggers a warning.

<Frame>
  <img src="/img/evaluation/dataset_draft_mode.png" />
</Frame>

<Frame>
  <img src="/img/evaluation/save_version_dialog.png" />
</Frame>

### Version history (UI)

Open your dataset and click **Version history** to see the full timeline (name, change summary, note, tags, item count, timestamp). From a version row you can **View items**, **Restore this version** (creates a new version with that data), or **Edit** metadata (note/tags; data stays immutable).

<Frame>
  <img src="/img/evaluation/version_history_tab.png" />
</Frame>

### Read items: latest vs a specific version

**Latest (default)** — Use the dataset helpers: `get_items()`, `to_pandas()`, and `to_json()` in Python; `getItems()` in TypeScript. They always return the latest version.

**A specific version** — The helpers don't accept a version. To read a tagged version (e.g. `"baseline"`) or a version hash, use the REST client and pass `version`:

<CodeBlocks>
```typescript title="TypeScript" language="typescript"
import { Opik } from "opik";

const client = new Opik();
const dataset = await client.getDataset("My dataset");

// Latest version
const items = await dataset.getItems();

// Specific version (tag or version hash from version history)
const page = await client.api.datasets.getDatasetItems(dataset.id, {
  version: "baseline",
});
const itemsFromVersion = page.content ?? [];
```

```python title="Python" language="python"
from opik import Opik

client = Opik()
dataset = client.get_dataset(name="My dataset")

# Latest version
items = dataset.get_items()
dataset.to_pandas()
dataset.to_json()

# Specific version (tag or version hash from version history)
page = client.rest_client.datasets.get_dataset_items(
    dataset.id,
    version="baseline",
)
items_from_version = page.content or []
```
</CodeBlocks>


### Listing and restoring versions (API)

- **List:** `GET /v1/private/datasets/{id}/versions` (paginated, newest first).
- **Restore:** `POST /v1/private/datasets/{id}/versions/restore` with `version_ref` (hash or tag)—creates a new version with that data.

Available on the generated REST client (e.g. `client.datasets.list_dataset_versions(id)` and `client.datasets.restore_dataset_version(id, version_ref=...)` in Python).

## Maintain and organize

### Item tags

**Item tags** label individual rows (for filtering and grouping); **version tags** label snapshots. Use item tags to categorize test cases (e.g. `edge-case`, `production`), track sources (`synthetic`, `real-world`), mark review status, or filter for evaluation. Each item can have multiple tags; tags are case-sensitive and can use letters, numbers, hyphens, and underscores.

From your dataset (Evaluation > Datasets > [Your Dataset]):

- **Add a tag to one item:** Open the item's details panel, click **+** in the Tags section, type the tag and press Enter. Remove with the **×** next to the tag.
- **Add a tag to many items:** Select the items with the checkboxes, click **Add tags** in the toolbar, enter the tag and confirm.
- **Filter by tag:** Click **Filters** next to the search bar, choose **Tags** as the column, operator **contains**, and enter the tag name. The filter is stored in the URL so you can bookmark or share it.

### Bulk operations

Select items with the table header checkbox; a banner lets you **Select all items** across pages (respecting any active filter). With items selected, the toolbar offers **Add tags**, **Delete** (creates a new version), and **Export** (CSV or JSON). Large operations run in the background with a progress indicator; the UI stays responsive.

<Frame>
  <img src="/img/evaluation/dataset_bulk_select.png" />
</Frame>

### Delete items (SDK)

To remove items, use `delete` (or `clear` to remove everything). Each delete or clear creates a new dataset version (see [Iterate safely: versioning](#iterate-safely-versioning)).

<CodeBlocks>
```typescript title="TypeScript" language="typescript"
import { Opik } from "opik";

// Get a dataset
const client = new Opik();
const dataset = await client.getDataset("My dataset");

await dataset.delete(["123", "456"]);

// Or delete all items
await dataset.clear();
```

```python title="Python" language="python"
from opik import Opik

# Get a dataset
client = Opik()
dataset = client.get_dataset(name="My dataset")

dataset.delete(items_ids=["123", "456"])

# Or delete all items
dataset.clear()
```
</CodeBlocks>