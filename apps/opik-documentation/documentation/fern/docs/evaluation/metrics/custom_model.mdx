---
description: Describes how to use a custom model for Opik's built-in LLM as a Judge metrics

toc_max_heading_level: 4
pytest_codeblocks_execute_previous: true
---

Opik provides a set of LLM as a Judge metrics that are designed to be model-agnostic and can be used with any LLM. In order to achieve this, we use the [LiteLLM library](https://github.com/BerriAI/litellm) to abstract the LLM calls.

By default, Opik will use the `gpt-4o` model. However, you can change this by setting the `model` parameter when initializing your metric to any model supported by [LiteLLM](https://docs.litellm.ai/docs/providers):

```python
from opik.evaluation.metrics import Hallucination

hallucination_metric = Hallucination(
    model="gpt-4-turbo"
)
```

## Using a model supported by LiteLLM

In order to use many models supported by LiteLLM, you also need to pass additional parameters. For this, you can use the [LiteLLMChatModel](https://www.comet.com/docs/opik/python-sdk-reference/Objects/LiteLLMChatModel.html) class and passing it to the metric:

```python
from opik.evaluation.metrics import Hallucination
from opik.evaluation import models

model = models.LiteLLMChatModel(
    name="<model_name>",
    base_url="<base_url>"
)

hallucination_metric = Hallucination(
    model=model
)
```

## Creating Your Own Custom Model Class

Opik's LLM-as-a-Judge metrics, such as `Hallucination`, are designed to work with various language models. While Opik supports many models out-of-the-box via LiteLLM, you can integrate any LLM by creating a custom model class. This involves subclassing `opik.evaluation.models.OpikBaseModel` and implementing its required methods.

### The `OpikBaseModel` Interface

`OpikBaseModel` is an abstract base class that defines the interface Opik metrics use to interact with LLMs. To create a compatible custom model, you must implement the following methods:

1.  `__init__(self, model_name: str)`:
    Initializes your custom model. Call `super().__init__(model_name)` and add any custom initialization, such as setting API keys or endpoints.
2.  `generate_string(self, input: str, **kwargs: Any) -> str`:
    Generates a simple string output from the model given a string input.
3.  `generate_provider_response(self, **kwargs: Any) -> Any`:
    The primary method used by LLM-as-a-Judge metrics. It receives the judge's prompt (typically as `kwargs['prompt']`) and should return the model's raw response. For judge metrics, this response is expected to be a JSON string containing the evaluation (e.g., `{"score": 0.0, "reason": ["..."]}`).

### Implementing a Custom Model for an OpenAI-like API

Here's an example of a custom model class that interacts with an LLM service exposing an OpenAI-compatible API endpoint.

```python
import requests
from typing import Any

from opik.evaluation.models import OpikBaseModel

class CustomOpenAICompatibleModel(OpikBaseModel):
    def __init__(self, model_name: str, api_key: str, base_url: str):
        super().__init__(model_name)
        self.api_key = api_key
        self.base_url = base_url # e.g., "https://api.openai.com/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }

    def generate_string(self, input: str, **kwargs: Any) -> str:
        """
        This method is used as part of LLM as a Judge metrics to take a string prompt, pass it to
        the model as a user message and return the model's response as a string.
        """
        messages = [
            {
                "content": input,
                "role": "user",
            },
        ]

        provider_response = self.generate_provider_response(messages=request, **kwargs)
        return provider_response["choices"][0]["message"]["content"]

    def generate_provider_response(self, messages: List[Dict[str, Any]], **kwargs: Any) -> Any:
        """
        This method is used as part of LLM as a Judge metrics to take a list of AI messages, pass it to
        the model and return the full model response.
        """

        if not messages:
            raise ValueError("'messages' must be provided in kwargs for LLM-as-a-Judge.")

        payload = {
            "model": self.model_name,
            "messages": messages,
        }

        response = requests.post(self.base_url, headers=self.headers, json=payload)

        response.raise_for_status()
        return response.json()
```

**Key considerations for the implementation:**

- **API Endpoint and Payload**: Adjust `base_url` and the JSON payload in `_make_request` (and `_amake_request`) to match your specific LLM provider's requirements if they deviate from the common OpenAI structure.
- **Model Name/ID**: The `model_id` passed to `__init__` is used as the `model` parameter in the API call. Ensure this matches an available model on your LLM service.
- **JSON Output**: The LLM-as-a-Judge prompt (provided by the `Hallucination` metric) instructs the model to return a JSON string. Your chosen LLM must be capable of following these instructions. The custom class then just returns this JSON string.
- **Error Handling**: The example includes basic error handling. Robust applications may require more sophisticated error management and logging.

### Using the Custom Model with the `Hallucination` Metric

Instantiate your custom model and pass it to the `Hallucination` metric:

```python
import os
from opik.evaluation.metrics import Hallucination

# Ensure these are set securely, e.g., via environment variables
API_KEY = os.getenv("MY_CUSTOM_LLM_API_KEY")
BASE_URL = "YOUR_LLM_CHAT_COMPLETIONS_ENDPOINT" # e.g., "https://api.openai.com/v1/chat/completions"
MODEL_NAME = "your-model-id" # e.g., "gpt-3.5-turbo"

if not API_KEY or not BASE_URL:
    raise ValueError("API_KEY and BASE_URL must be set for the custom model.")

# Initialize your custom model
my_custom_model = CustomOpenAICompatibleModel(
    model_name=MODEL_NAME,
    api_key=API_KEY,
    base_url=BASE_URL
)

# Initialize the Hallucination metric with the custom model
hallucination_metric = Hallucination(
    model=my_custom_model
)

# Example usage:
try:
    evaluation = hallucination_metric.score(
        input="What is the capital of Mars?",
        output="The capital of Mars is Ares City, a bustling metropolis.",
        context=["Mars is a planet in our solar system. It does not currently have any established cities or a designated capital."]
    )
    print(f"Hallucination Score: {evaluation.value}") # Expected: 1.0 (hallucination detected)
    print(f"Reason: {evaluation.reason}")
except Exception as e:
    print(f"An error occurred during metric scoring: {e}")
```
