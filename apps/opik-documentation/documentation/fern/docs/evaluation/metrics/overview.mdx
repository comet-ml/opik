---
description: Describes all the built-in evaluation metrics provided by Opik
---

# Overview

Opik provides a set of built-in evaluation metrics that can be used to evaluate the output of your LLM calls. These metrics are broken down into two main categories:

1. Heuristic metrics
2. LLM as a Judge metrics

Heuristic metrics are deterministic and are often statistical in nature. LLM as a Judge metrics are non-deterministic and are based on the idea of using an LLM to evaluate the output of another LLM.

Opik provides the following built-in evaluation metrics:

| Metric                       | Type           | Description                                                                                       | Documentation                                                                                             |
| ---------------------------- | -------------- | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |
| Equals                       | Heuristic      | Checks if the output exactly matches an expected string                                           | [Equals](/evaluation/metrics/heuristic_metrics#equals)                                                    |
| Contains                     | Heuristic      | Check if the output contains a specific substring, can be both case sensitive or case insensitive | [Contains](/evaluation/metrics/heuristic_metrics#contains)                                                |
| RegexMatch                   | Heuristic      | Checks if the output matches a specified regular expression pattern                               | [RegexMatch](/evaluation/metrics/heuristic_metrics#regexmatch)                                            |
| IsJson                       | Heuristic      | Checks if the output is a valid JSON object                                                       | [IsJson](/evaluation/metrics/heuristic_metrics#isjson)                                                    |
| Levenshtein                  | Heuristic      | Calculates the Levenshtein distance between the output and an expected string                     | [Levenshtein](/evaluation/metrics/heuristic_metrics#levenshteinratio)                                     |
| Hallucination                | LLM as a Judge | Check if the output contains any hallucinations                                                   | [Hallucination](/evaluation/metrics/hallucination)                                                        |
| G-Eval                       | LLM as a Judge | Task agnostic LLM as a Judge metric                                                               | [G-Eval](/evaluation/metrics/g_eval)                                                                      |
| Moderation                   | LLM as a Judge | Check if the output contains any harmful content                                                  | [Moderation](/evaluation/metrics/moderation)                                                              |
| AnswerRelevance              | LLM as a Judge | Check if the output is relevant to the question                                                   | [AnswerRelevance](/evaluation/metrics/answer_relevance)                                                   |
| Usefulness                   | LLM as a Judge | Check if the output is useful to the question                                                     | [Usefulness](/evaluation/metrics/usefulness)                                                              |
| ContextRecall                | LLM as a Judge | Check if the output contains any hallucinations                                                   | [ContextRecall](/evaluation/metrics/context_recall)                                                       |
| ContextPrecision             | LLM as a Judge | Check if the output contains any hallucinations                                                   | [ContextPrecision](/evaluation/metrics/context_precision)                                                 |
| Conversational Coherence     | LLM as a Judge | Calculates the conversational coherence score for a given conversation thread.                    | [ConversationalCoherence](/evaluation/metrics/conversation_threads_metrics#conversationalcoherencemetric) |
| Session Completeness Quality | LLM as a Judge | Evaluates the completeness of a session within a conversational thread.                           | [SessionCompleteness](/evaluation/metrics/conversation_threads_metrics#sessioncompletenessquality)        |
| User Frustration             | LLM as a Judge | Calculates the user frustration score for a given conversational thread.                          | [UserFrustration](/evaluation/metrics/conversation_threads_metrics#userfrustrationmetric)                 |

You can also create your own custom metric, learn more about it in the [Custom Metric](/evaluation/metrics/custom_metric) section.

## Customizing LLM as a Judge metrics

By default, Opik uses GPT-4o from OpenAI as the LLM to evaluate the output of other LLMs. However, you can easily switch to another LLM provider by specifying a different `model` parameter.

<CodeBlocks>
```python title="Python" language="python"
from opik.evaluation.metrics import Hallucination

metric = Hallucination(model="bedrock/anthropic.claude-3-sonnet-20240229-v1:0")

metric.score(
input="What is the capital of France?",
output="The capital of France is Paris. It is famous for its iconic Eiffel Tower and rich cultural heritage.",
)

````

```typescript title="TypeScript" language="typescript"
import { Hallucination } from 'opik';
import { openai } from '@ai-sdk/openai';

// Using model ID string (simplest approach)
const metric1 = new Hallucination({ model: 'gpt-4o' });
const metric2 = new Hallucination({ model: 'claude-3-5-sonnet-latest' });
const metric3 = new Hallucination({ model: 'gemini-2.0-flash' });

// With generation parameters (temperature, seed, maxTokens)
const metric4 = new Hallucination({
  model: 'gpt-4o',
  temperature: 0.3,
  seed: 42
});

// Using custom LanguageModel instance for provider-specific configuration
const customModel = openai('gpt-4o', {
  structuredOutputs: true
});
const metric5 = new Hallucination({ model: customModel });

// Score using the metric
await metric4.score({
    input: "What is the capital of France?",
    output: "The capital of France is Paris. It is famous for its iconic Eiffel Tower and rich cultural heritage.",
});
````

</CodeBlocks>

For **Python**, this functionality is based on LiteLLM framework. You can find a full list of supported LLM providers and how to configure them in the [LiteLLM Providers](https://docs.litellm.ai/docs/providers) guide.

For **TypeScript**, the SDK integrates with the Vercel AI SDK. You can use model ID strings for simplicity or LanguageModel instances for advanced configuration. See the [Models documentation](/reference/typescript-sdk/evaluation/models) for more details.
