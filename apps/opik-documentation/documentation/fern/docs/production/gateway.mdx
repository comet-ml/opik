---
headline: Gateway | Opik Documentation
og:description: Build a centralized LLM gateway with Opik to streamline access to
  multiple LLM providers using a consistent API format.
og:site_name: Opik Documentation
og:title: LLM Gateway for Seamless Integration - Opik
subtitle: Overview of LLM gateway integrations available with Opik
title: Gateway
---

An LLM gateway is a proxy server that forwards requests to an LLM API and returns the response. This is useful for when you want to centralize the access to LLM providers or when you want to be able to query multiple LLM providers from a single endpoint using a consistent request and response format.

## Gateway Integrations

Opik supports several LLM gateway solutions to help you centralize and manage your LLM provider access:

<CardGroup cols={3}>
  <Card title="Opik LLM Gateway" href="/docs/opik/integrations/opik-llm-gateway" />
  <Card title="Kong AI Gateway" href="/docs/opik/integrations/kong-ai-gateway" />
  <Card title="AISuite" href="/docs/opik/integrations/aisuite" />
  <Card title="LiteLLM" href="/docs/opik/integrations/litellm" />
  <Card title="OpenRouter" href="/docs/opik/integrations/openrouter" />
</CardGroup>

### Choosing the Right Gateway

- **Opik LLM Gateway**: Light-weight proxy server for development and testing purposes. Supports OpenAI-compatible API format.
- **Kong AI Gateway**: Production-ready enterprise gateway with authentication, load balancing, caching, and advanced monitoring features.
- **AISuite**: Python SDK gateway for unified LLM provider access.
- **LiteLLM**: Unified interface for multiple LLM providers with automatic fallback and retry logic.
- **OpenRouter**: Unified API for accessing multiple LLM providers through a single endpoint.

For production applications, we recommend using the **Kong AI Gateway** or **LiteLLM** for their enterprise-grade features and reliability.