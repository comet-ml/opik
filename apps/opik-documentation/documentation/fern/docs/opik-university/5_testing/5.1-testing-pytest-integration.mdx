---
title: PyTest Integration
---

<div style={{
    position: 'relative',
    paddingBottom: '56.25%', // 16:9 aspect ratio
    height: 0,
    overflow: 'hidden',
    maxWidth: '100%',
    marginBottom: '20px'
}}>
    <iframe
        src="https://www.loom.com/embed/03831837e44f49a6b7a9bbbe407c29e2?sid=3cb6fbde-7d26-4d0f-a142-820c9e2da651"
        frameborder="0"
        webkitallowfullscreen
        mozallowfullscreen
        allowfullscreen
        style={{
            position: 'absolute',
            top: 0,
            left: 0,
            width: '100%',
            height: '100%',
        }}
    />
</div>

## Integrating LLM Testing into Your Development Workflow

This video demonstrates how to integrate Opik's [PyTest](https://www.comet.com/docs/opik/testing/pytest_integration) functionality into your development workflow, bridging traditional software [testing](https://www.comet.com/docs/opik/testing/pytest_integration) with LLM application testing. Using a real-world call summarizer Streamlit application example, you'll learn how to write regression tests that ensure new features don't break existing LLM functionality, while creating comprehensive [datasets](https://www.comet.com/docs/opik/evaluation/manage_datasets) from your test cases for ongoing [evaluation](https://www.comet.com/docs/opik/evaluation/overview).

## Key Highlights

- **LLM Unit [Testing](https://www.comet.com/docs/opik/testing/pytest_integration)**: Use the [`@llm_unit`](https://www.comet.com/docs/opik/testing/pytest_integration#advanced-usage) decorator to transform any [PyTest](https://www.comet.com/docs/opik/testing/pytest_integration) function into an LLM test that automatically captures [traces](https://www.comet.com/docs/opik/tracing/log_traces) and sends them to Opik projects
- **Flexible [Integration](https://www.comet.com/docs/opik/integrations/overview)**: Works with both [`@track`](https://www.comet.com/docs/opik/tracing/log_traces#decorators) decorated functions and [integration](https://www.comet.com/docs/opik/integrations/overview)-wrapped clients (like [`track_openai`](https://www.comet.com/docs/opik/integrations/openai)) for comprehensive test coverage
- **Mixed [Testing](https://www.comet.com/docs/opik/testing/pytest_integration) Strategies**: Combine traditional unit tests, mocked LLM calls, and real API calls within the same test suite based on your specific needs
- **Cost-Conscious CI/CD**: Be mindful of API costs when using real LLM calls in CI/CD pipelines - consider running expensive tests only locally or selectively
- **Cumulative [Dataset](https://www.comet.com/docs/opik/evaluation/manage_datasets) Creation**: All test cases automatically contribute to a centralized "tests" [dataset](https://www.comet.com/docs/opik/evaluation/manage_datasets), providing comprehensive test coverage documentation
- **Regression Prevention**: Write tests that ensure new code doesn't break existing LLM functionality, maintaining application stability as you iterate
- **Real-World Example**: Practical demonstration using a call summarizer application with actual Streamlit integration and multiple test scenarios
- **[Trace](https://www.comet.com/docs/opik/tracing/log_traces) Integration**: Test [traces](https://www.comet.com/docs/opik/tracing/log_traces) integrate seamlessly with Opik's [experiment](https://www.comet.com/docs/opik/evaluation/evaluate_your_llm) and [dataset](https://www.comet.com/docs/opik/evaluation/manage_datasets) system, providing [feedback scores](https://www.comet.com/docs/opik/tracing/annotate_traces) and success [metrics](https://www.comet.com/docs/opik/evaluation/metrics/overview)
- **File Path Tracking**: Each test result references the exact test function file path, making debugging and maintenance straightforward
