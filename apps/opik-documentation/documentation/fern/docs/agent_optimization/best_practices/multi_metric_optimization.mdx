---
title: Multi-Metric Optimization
description: Learn how to optimize your agents using multiple evaluation metrics simultaneously with composite objectives.
---

When optimizing AI agents, you often need to balance multiple quality dimensions simultaneously. Multi-metric optimization allows you to combine several evaluation metrics with customizable weights to create a composite objective function.

## Why Use Multi-Metric Optimization?

While you can implement metric combinations within a custom metric function, using Opik Optimizer's `MultiMetricObjective` API provides additional benefits:

- **Automatic logging** of all component metrics to the Opik platform
- **Individual tracking** of each sub-metric alongside the composite score
- **Detailed visibility** into how each metric contributes to optimization
- **Trial-level insights** for both aggregate and individual trace performance

This visibility helps you understand trade-offs between different quality dimensions during optimization.

## Basic Setup

In this guide, we'll use the **HotPot QA dataset** to demonstrate multi-metric optimization. The example optimizes a question-answering agent that uses Wikipedia search to balance both accuracy and relevance.

To use multi-metric optimization, you need to:

1. Define multiple metric functions
2. Create a `MultiMetricObjective` class instance using your functions and weights
3. Pass it to your optimizer as the metric to optimize for

### Step 1: Define Your Metrics

Create individual metric functions that evaluate different aspects of your agent's output:

```python
from typing import Any, Dict
from opik.evaluation.metrics import LevenshteinRatio, AnswerRelevance
from opik.evaluation.metrics.score_result import ScoreResult

def levenshtein_ratio(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
    """Measures string similarity between output and reference answer."""
    metric = LevenshteinRatio()
    return metric.score(reference=dataset_item["answer"], output=llm_output)

def answer_relevance_score(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
    """Evaluates how relevant the answer is to the question and context."""
    metric = AnswerRelevance()
    return metric.score(
        context=[dataset_item["answer"]], 
        output=llm_output, 
        input=dataset_item["question"]
    )
```

### Step 2: Create a Multi-Metric Objective

Combine your metrics with weights using `MultiMetricObjective`:

```python
import opik_optimizer

multi_metric_objective = opik_optimizer.MultiMetricObjective(
    weights=[0.4, 0.6],
    metrics=[levenshtein_ratio, answer_relevance_score],
    name="my_composite_metric",
)
```

**Understanding Weights:**

The `weights` parameter controls the relative importance of each metric:

- `weights=[0.4, 0.6]` → First metric contributes 40%, second contributes 60%
- Higher weights emphasize those metrics during optimization
- Weights don't need to sum to 1—use any values that represent your priorities

### Step 3: Use with Optimizer

Pass the multi-metric objective to your optimizer:

```python
from opik_optimizer.gepa_optimizer import GepaOptimizer

optimizer = GepaOptimizer(
    model="openai/gpt-4o-mini",
    reflection_model="openai/gpt-4o",
    project_name="GEPA-Hotpot",
    temperature=0.7,
    max_tokens=400,
)

result = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,
    metric=multi_metric_objective,  # Use the composite metric
    max_metric_calls=60,
    reflection_minibatch_size=5,
    candidate_selection_strategy="best",
    n_samples=12,
)

result.display()
```

## Complete Example

Here's a full working example using multi-metric optimization for a question-answering task with tool usage:

```python
from typing import Any, Dict
import opik
import opik_optimizer
from opik_optimizer import ChatPrompt
from opik_optimizer.gepa_optimizer import GepaOptimizer
from opik_optimizer.datasets import hotpot_300
from opik_optimizer.utils import search_wikipedia
from opik.evaluation.metrics import LevenshteinRatio, AnswerRelevance
from opik.evaluation.metrics.score_result import ScoreResult

# Load dataset
dataset = hotpot_300()

# Define metric functions
def levenshtein_ratio(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
    """Measures string similarity between output and reference answer."""
    metric = LevenshteinRatio()
    return metric.score(reference=dataset_item["answer"], output=llm_output)

def answer_relevance_score(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
    """Evaluates how relevant the answer is to the question and context."""
    metric = AnswerRelevance()
    return metric.score(
        context=[dataset_item["answer"]], 
        output=llm_output, 
        input=dataset_item["question"]
    )

# Define prompt template with Wikipedia search tool
prompt = ChatPrompt(
    system="Answer the question",
    user="{question}",
    tools=[
        {
            "type": "function",
            "function": {
                "name": "search_wikipedia",
                "description": "This function is used to search wikipedia abstracts.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The query parameter is the term or phrase to search for.",
                        },
                    },
                    "required": ["query"],
                },
            },
        },
    ],
    function_map={"search_wikipedia": opik.track(type="tool")(search_wikipedia)},
)

# Create optimizer
optimizer = GepaOptimizer(
    model="openai/gpt-4o-mini",
    reflection_model="openai/gpt-4o",
    project_name="GEPA-Hotpot",
    temperature=0.7,
    max_tokens=400,
)

# Create multi-metric objective
multi_metric_objective = opik_optimizer.MultiMetricObjective(
    weights=[0.4, 0.6],
    metrics=[levenshtein_ratio, answer_relevance_score],
    name="my_composite_metric",
)

# Optimize with multi-metric objective
result = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,
    metric=multi_metric_objective,
    max_metric_calls=60,
    reflection_minibatch_size=5,
    candidate_selection_strategy="best",
    n_samples=12,
)

result.display()
```

## Viewing Results

When you run multi-metric optimization, Opik tracks and displays both the composite metric and individual sub-metrics throughout the optimization process.

### Progress Chart

The optimization progress chart shows how your composite metric and individual metrics evolve over trials:

<Frame>
  <img src="/img/agent_optimization/multi_metric_optimization/progress_chart.png" alt="Multi-metric optimization progress chart" />
</Frame>

**What you'll see:**

- **Composite metric** (`my_composite_metric`) — The weighted combination of all metrics
- **Individual metrics** (`levenshtein_ratio`, `answer_relevance_score`) — Each component tracked separately
- **Trial progression** — Metric evolution over time

This lets you see not just overall optimization progress, but how each metric contributes to the final score.

### Trial Items View

View individual trial items with detailed metric breakdowns:

<Frame>
  <img src="/img/agent_optimization/multi_metric_optimization/trial_items.png" alt="Trial items with multi-metric scores" />
</Frame>

**What you'll see:**

- Composite metric score for each trial
- Individual scores for each component metric
- Performance comparison across different prompts

**Insights you'll gain:**

- Which metrics are improving or degrading
- Trade-offs between different quality dimensions
- Whether your weight balance is appropriate

## Next Steps

- Explore [available evaluation metrics](/docs/opik/evaluation/metrics)
- Understand [optimization strategies](/docs/opik/agent_optimization/overview)

