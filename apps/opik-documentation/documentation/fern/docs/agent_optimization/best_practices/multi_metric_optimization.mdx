---
title: Multi-Metric Optimization
description: Learn how to optimize your agents using multiple evaluation metrics simultaneously with composite objectives.
---

When optimizing AI agents, you often need to balance multiple quality dimensions simultaneously.
Multi-metric optimization allows you to combine several evaluation metrics with customizable
weights to create a composite objective function.

## Why Use Multi-Metric Optimization?

While you can implement metric combinations within a custom metric function, using Opik Optimizer's
`MultiMetricObjective` API provides additional benefits:

- **Automatic logging** of all component metrics to the Opik platform
- **Individual tracking** of each sub-metric alongside the composite score
- **Detailed visibility** into how each metric contributes to optimization
- **Trial-level insights** for both aggregate and individual trace performance

This visibility helps you understand trade-offs between different quality dimensions during
optimization.

## Quickstart

You can use `MultiMetricObjective` to create a composite metric from multiple metrics:

```python
from opik_optimizer import MultiMetricObjective

multi_metric_objective = MultiMetricObjective(
    weights=[0.4, 0.6],
    metrics=[metric_1, metric_2],
)
```

## End to end example

In this guide, we'll demonstrate multi-metric optimization with a simple question-answering task.
The example optimizes a basic Q&A agent to balance both accuracy and relevance without requiring
complex tool usage.

To use multi-metric optimization, you need to:

1. Define multiple metric functions
2. Create a `MultiMetricObjective` class instance using your functions and weights
3. Pass it to your optimizer as the metric to optimize for

<Steps>
    <Step title="Define Your Metrics">
        Create individual metric functions that evaluate different aspects of your agent's output:

        ```python
        from typing import Any, Dict
        from opik.evaluation.metrics import LevenshteinRatio, AnswerRelevance
        from opik.evaluation.metrics.score_result import ScoreResult

        def levenshtein_ratio(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
            """Measures string similarity between output and reference answer."""
            metric = LevenshteinRatio()
            return metric.score(reference=dataset_item["answer"], output=llm_output)

        def answer_relevance_score(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
            """Evaluates how relevant the answer is to the question and context."""
            metric = AnswerRelevance()
            return metric.score(
                context=[dataset_item["answer"]], 
                output=llm_output, 
                input=dataset_item["question"]
            )
        ```
    </Step>
    <Step title="Create a Multi-Metric Objective">
        Combine your metrics with weights using `MultiMetricObjective`:

        ```python
        import opik_optimizer

        multi_metric_objective = opik_optimizer.MultiMetricObjective(
            weights=[0.4, 0.6],
            metrics=[levenshtein_ratio, answer_relevance_score],
            name="my_composite_metric",
        )
        ```

        **Understanding Weights:**

        The `weights` parameter controls the relative importance of each metric:

        - `weights=[0.4, 0.6]` → First metric contributes 40%, second contributes 60%
        - Higher weights emphasize those metrics during optimization
        - Weights don't need to sum to 1—use any values that represent your priorities
    </Step>
    <Step title="Use with Optimizer">
        ```python
        import opik
        import opik_optimizer
        from opik_optimizer import ChatPrompt
        from opik_optimizer.gepa_optimizer import GepaOptimizer

        # Create a simple dataset
        client = opik.Opik()
        dataset = client.get_or_create_dataset(name='multi_metric_example')
        dataset.insert([
            {"question": "What is the capital of France?", "answer": "Paris"},
            {"question": "What is 2+2?", "answer": "4"},
        ])

        # Define a simple prompt
        prompt = ChatPrompt(
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "{question}"},
            ],
            model="gpt-4o-mini"
        )

        optimizer = GepaOptimizer(
            model="openai/gpt-4o-mini",
            reflection_model="openai/gpt-4o",
            project_name="Multi-Metric-Example",
            temperature=0.7,
            max_tokens=100,
        )

        result = optimizer.optimize_prompt(
            prompt=prompt,
            dataset=dataset,
            metric=multi_metric_objective,  # Use the composite metric
            n_samples=6,
        )

        result.display()
        ```
    </Step>
    <Step title="View Results">
        You can view the results of the optimization in the Opik dashboard:

        <Frame>
            <img src="/img/agent_optimization/multi_metric_optimization/results.png" alt="Multi-metric optimization results" />
        </Frame>

        **What you'll see:**

            - **Composite metric** (`my_composite_metric`) — The weighted combination of all metrics
            - **Individual metrics** (`levenshtein_ratio`, `answer_relevance_score`) — Each component tracked separately
            - **Trial progression** — Metric evolution over time

        This lets you see not just overall optimization progress, but how each metric contributes to the final score.
    </Step>
</Steps>

## Next Steps

- Explore [available evaluation metrics](/docs/opik/evaluation/metrics)
- Understand [optimization strategies](/docs/opik/agent_optimization/overview)

