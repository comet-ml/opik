---
description: Create reliable metrics and composite objectives for Agent Optimizer
  runs.
headline: Define metrics | Opik Documentation
og:description: Learn to leverage metrics for optimizer decisions with Opik. Discover
  presets and extend them for your specific use case.
og:site_name: Opik Documentation
og:title: Define Metrics for Optimization - Opik
title: Define metrics
---

Metrics drive optimizer decisions. This guide highlights the fastest way to pick proven presets from Opik’s evaluation catalog, then shows how to extend them when your use case demands it. If you need the full theory, see [Evaluation concepts](/evaluation/concepts) and the [metrics overview](/evaluation/metrics/overview).

## Metric anatomy

A metric is a callable with the signature `(dataset_item, llm_output) -> ScoreResult | float`. Use `ScoreResult` to attach names and reasons.

```python
from opik.evaluation.metrics.score_result import ScoreResult

def short_answer(item, output):
    is_short = len(output) <= 200
    return ScoreResult(
        name="short_answer",
        value=1.0 if is_short else 0.0,
        reason="Answer under 200 chars" if is_short else "Answer too long"
    )
```

## Compose metrics

Use `MultiMetricObjective` to balance multiple goals (accuracy, style, safety).

```python
from opik_optimizer import MultiMetricObjective
from opik.evaluation.metrics import LevenshteinRatio, AnswerRelevance

objective = MultiMetricObjective(
    weights=[0.6, 0.4],
    metrics=[
        lambda item, output: LevenshteinRatio().score(reference=item["answer"], output=output),
        lambda item, output: AnswerRelevance().score(
            context=[item["answer"]], output=output, input=item["question"]
        ),
    ],
    name="accuracy_and_relevance",
)
```

<Tip>
  Weights do not need to sum to 1; choose numbers that highlight the most critical metric to your use case.
</Tip>

### Include cost and duration metrics

You can optimize for efficiency alongside quality by including span-based metrics like cost and duration in your composite objective. These metrics require access to the `task_span` parameter:

```python
from opik_optimizer import MultiMetricObjective
from opik.evaluation.metrics import AnswerRelevance
from opik_optimizer.metrics import SpanCost, SpanDuration

# Regular metric without task_span
def answer_relevance(dataset_item, llm_output):
    metric = AnswerRelevance()
    return metric.score(
        context=[dataset_item["answer"]], 
        output=llm_output, 
        input=dataset_item["question"]
    )

# Built-in span metrics can be normalized with target= for clean multi-metric weighting.
# invert=True (default) means lower raw value -> higher score.
cost = SpanCost(target=0.01, invert=True, name="cost_score")
duration = SpanDuration(target=6.0, invert=True, name="duration_score")

# Combine quality, cost, and speed metrics on a common [0, 1] scale
objective = MultiMetricObjective(
    metrics=[answer_relevance, cost, duration],
    weights=[0.33, 0.33, 0.33],  # equally optimize for accuracy, cost and duration/latency
    name="quality_cost_speed",
)
```

For a working end-to-end example in the repository, see:
[multi_metric_cost_duration_example.py](https://github.com/comet-ml/opik/blob/main/sdks/opik_optimizer/scripts/multi_metric_cost_duration_example.py)

<Note>
  Span-based metrics like `SpanCost` and `SpanDuration` automatically receive the `task_span` parameter during evaluation, which contains execution information about the agent's run. When using raw (non-normalized) cost or duration values, use negative weights in `MultiMetricObjective` to minimize them. When using target-normalized metrics (`target=`), use positive weights because those scores are mapped to a "higher is better" scale.
</Note>

<Note>
  Direction control is explicit:
  - `invert=True` (default) for efficiency metrics where lower raw values should score higher.
  - `invert=False` if your objective should reward higher raw values.
  When `target` is omitted, these metrics return raw values (not normalized scores).
</Note>

## Recommended presets

| Scenario | Metric | Notes |
| --- | --- | --- |
| Factual QA | `LevenshteinRatio` or `ExactMatch` | Works with text-only datasets; deterministic and low cost. |
| Retrieval / grounding | `AnswerRelevance` | Pass reference context via `context=[item["answer"]]` or retrieved docs. |
| Safety | `Moderation` or custom LLM-as-a-judge | Combine with `MultiMetricObjective` to gate unsafe answers. |
| Multi-turn trajectories | [Agent trajectory evaluator](/evaluation/evaluate_agent_trajectory) | Scores complete conversations, not just final outputs. |

Reuse these heuristics before writing custom metrics—most are already imported in `opik.evaluation.metrics`.

## Optimizer built-in metrics

Opik Optimizer also ships built-in metric helpers for common optimization setups:

| Metric | Import | When to use |
| --- | --- | --- |
| `LevenshteinAccuracyMetric` | `from opik_optimizer.metrics import LevenshteinAccuracyMetric` | Quick string-similarity accuracy using dataset keys like `answer` or `highlights`. |
| `SpanCost` | `from opik_optimizer.metrics import SpanCost` | Cost metric with `target=` normalization and `invert=` direction control. |
| `SpanDuration` | `from opik_optimizer.metrics import SpanDuration` | Duration metric with `target=` normalization and `invert=` direction control. |

Example with built-ins:

```python
from opik_optimizer import MultiMetricObjective
from opik_optimizer.metrics import LevenshteinAccuracyMetric, SpanCost, SpanDuration

accuracy = LevenshteinAccuracyMetric(reference_key="answer")
cost = SpanCost(target=0.01, invert=True, name="cost_score")
duration = SpanDuration(target=6.0, invert=True, name="duration_score")

objective = MultiMetricObjective(
    metrics=[accuracy, cost, duration],
    weights=[0.5, 0.25, 0.25],  # all metrics already normalized to [0, 1]
    name="accuracy_cost_duration",
)
```

## Checklist for great metrics

- **Return explanations** – populate `reason` so reflective optimizers can group failure modes.
- **Avoid randomness** – deterministic metrics keep optimizers from chasing noise.
- **Bound runtime** – use cached references or lightweight models where possible; heavy metrics slow down trials.
- **Log metadata** – include `details` in the `ScoreResult` if you want to visualize per-sample attributes later.

When you outgrow presets, move to [Custom metrics](/agent_optimization/advanced/custom_metrics) for LLM-as-a-judge flows or domain-specific scoring.

## Testing metrics

1. Dry-run against a handful of dataset rows before launching an optimization.
2. Use `optimizer.task_evaluator.evaluate_prompt` to evaluate a single prompt with your metric.
3. Inspect the per-sample reasons in the Opik dashboard to ensure they match expectations.

## Related resources

- Deep dive: [Multi-metric optimization guide](/agent_optimization/best_practices/multi_metric_optimization)
- API reference: [`ScoreResult`](https://www.comet.com/docs/opik/python-sdk-reference/Objects/ScoreResult.html)
- Advanced topic: [Custom metrics](/agent_optimization/advanced/custom_metrics)
