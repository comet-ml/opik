---
description: Compare Agent Optimizer algorithms and pick the right one for your workload.
headline: Overview | Opik Documentation
og:description: Explore and select the right optimizer in Opik to enhance your pipeline
  with a seamless API for efficient performance.
og:site_name: Opik Documentation
og:title: Overview of Optimization Algorithms - Opik
title: Optimization algorithms overview
---

The Opik Optimizer SDK wraps a mix of in-house algorithms (MetaPrompt, HRPO) and external research projects (e.g., GEPA). Each optimizer follows the same API (`optimize_prompt`, `OptimizationResult`) so you can swap them without rewriting your pipeline. Use this page to quickly decide which optimizer to run before diving into the detailed guides.

## How optimizers run

1. **Input** – you pass a `ChatPrompt` definition, dataset, and metric. Many optimizers also accept additional parameters to set which model to use, number of optimization rounds, and even tool use (MCP and function calling) definitions.
2. **Candidate generation** – each algorithm proposes new prompts (MetaPrompt via reasoning LLMs, Evolutionary via mutation/crossover, GEPA via its genetic-Pareto search).
3. **Evaluation** – Opik runs the candidate against your dataset/metric and logs trials to the dashboard. The steps 2-to-3 continue to loop until such time a best prompt is found or the search has been exhausted.
4. **Result delivery** – every optimizer returns an `OptimizationResult` with the best prompt, history, scores, and metadata which is passed back and also available in the UI.

## Selection matrix

| Optimizer | Origin | Best for | Key inputs | Notes |
| --- | --- | --- | --- | --- |
| [MetaPrompt](/agent_optimization/algorithms/metaprompt_optimizer) | Opik | General prompt refinement | Prompt + dataset + metric | Reasoning LLM critiques and rewrites prompts, supports MCP workflows and tool schemas. |
| [HRPO](/agent_optimization/algorithms/hierarchical_adaptive_optimizer) | Opik | Root-cause analysis on complex prompts | Metrics with detailed reasons | Batches failures, synthesizes themes, proposes targeted fixes. |
| [Few-Shot Bayesian](/agent_optimization/algorithms/fewshot_bayesian_optimizer) | Opik | Optimizing few-shot example sets | Dataset with demonstrations | Uses Optuna to pick count/order of examples for chat prompts. |
| [Evolutionary](/agent_optimization/algorithms/evolutionary_optimizer) | Opik + DEAP | Exploring diverse prompt structures | Mutation/crossover params | Multi-objective optimization (score vs. length) and LLM-driven operators. |
| [GEPA](/agent_optimization/algorithms/gepa_optimizer) | External (GEPA) | Single-turn, reflection-heavy tasks | `gepa` dependency + reflection minibatches | We provide a wrapper so GEPA consumes Opik datasets/metrics while preserving its Pareto search. |
| [Parameter](/agent_optimization/algorithms/parameter_optimizer) | Opik | Temperature / top_p tuning | Prompt + parameter search space | Leaves prompt untouched; focuses on sampling parameters via Bayesian search. |

## How to choose

1. **Identify the constraint** (e.g., wording vs. tool usage vs. parameters).
2. **Check dataset readiness** – reflective optimizers need detailed metric reasons. Consider splitting your data into training and validation sets to prevent overfitting.
3. **Estimate budget** – evolutionary/GEPA runs consume more tokens than MetaPrompt.
4. **Plan follow-up** – you can chain optimizers (MetaPrompt → Parameter) when needed.

## Next steps

- Follow the individual optimizer guides for configuration details.
- Learn how to [chain optimizers](/agent_optimization/advanced/chaining_optimizers) for complex workflows.
