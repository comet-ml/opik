---
title: "MetaPrompt Optimizer"
subtitle: "Refine and improve LLM prompts with systematic analysis."
description: "Learn how to use the MetaPrompt Optimizer to refine and improve your LLM prompts through systematic analysis and iterative refinement."
---

The MetaPrompter is a specialized optimizer designed for meta-prompt optimization. It focuses on
improving the structure and effectiveness of prompts through systematic analysis and refinement of
prompt templates, instructions, and examples.

<Note>
  The `MetaPromptOptimizer` is a strong choice when you have an initial instruction prompt and want to
  iteratively refine its wording, structure, and clarity using LLM-driven suggestions. It excels at
  general-purpose prompt improvement where the core idea of your prompt is sound but could be
  phrased better for the LLM, or when you want to explore variations suggested by a reasoning model.
</Note>

## How it works

The `MetaPromptOptimizer` automates the process of prompt refinement by using a "reasoning" LLM to
critique and improve your initial prompt. Here's a conceptual breakdown:

<Frame>
  <img src="/img/agent_optimization/metaprompt_optimizer.png" alt="MetaPrompt Optimizer" />
</Frame>

<Tip>
  The optimizer is open-source, you can check out the code in the
  [Opik repository](https://github.com/comet-ml/opik/tree/main/sdks/opik_optimizer/src/opik_optimizer/metaprompt_optimizer).
</Tip>


## Quickstart

You can use the `MetaPromptOptimizer` to optimize a prompt by following these steps:

```python maxLines=1000
from opik_optimizer import MetaPromptOptimizer
from opik.evaluation.metrics import LevenshteinRatio
from opik_optimizer import datasets, ChatPrompt

# Initialize optimizer
optimizer = MetaPromptOptimizer(
    model="openai/gpt-4",
    model_parameters={
        "temperature": 0.1,
        "max_tokens": 5000
    },
    n_threads=8,
    seed=42
)

# Prepare dataset
dataset = datasets.hotpot_300()

# Define metric and task configuration (see docs for more options)
def levenshtein_ratio(dataset_item, llm_output):
    return LevenshteinRatio().score(reference=dataset_item['answer'], output=llm_output)

prompt = ChatPrompt(
    messages=[
        {"role": "system", "content": "Provide an answer to the question."},
        {"role": "user", "content": "{question}"}
    ]
)

# Run optimization
results = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,
    metric=levenshtein_ratio,
    n_samples=100
)

# Access results
results.display()
```

## Configuration Options

### Optimizer parameters

The optimizer has the following parameters:

<ParamField path="model" type="str" optional={true} default="gpt-4o">LiteLLM model name for optimizer's internal reasoning/generation calls</ParamField>
<ParamField path="model_parameters" type="dict[str, typing.Any] | None" optional={true}>Optional dict of LiteLLM parameters for optimizer's internal LLM calls. Common params: temperature, max_tokens, max_completion_tokens, top_p.</ParamField>
<ParamField path="prompts_per_round" type="int" optional={true} default="4">Number of candidate prompts to generate per optimization round</ParamField>
<ParamField path="enable_context" type="bool" optional={true} default="True">Whether to include task-specific context when reasoning about improvements</ParamField>
<ParamField path="n_threads" type="int" optional={true} default="12">Number of parallel threads for prompt evaluation</ParamField>
<ParamField path="verbose" type="int" optional={true} default="1">Controls internal logging/progress bars (0=off, 1=on)</ParamField>
<ParamField path="seed" type="int" optional={true} default="42">Random seed for reproducibility</ParamField>

### `optimize_prompt` parameters

The `optimize_prompt` method has the following parameters:

<ParamField path="prompt" type="ChatPrompt">The prompt to optimize</ParamField>
<ParamField path="dataset" type="Dataset">Opik Dataset to optimize on</ParamField>
<ParamField path="metric" type="Callable">Metric function to evaluate on</ParamField>
<ParamField path="experiment_config" type="dict | None" optional={true}>Optional configuration for the experiment, useful to log additional metadata</ParamField>
<ParamField path="n_samples" type="int | None" optional={true}>Optional number of items to test in the dataset</ParamField>
<ParamField path="auto_continue" type="bool" optional={true} default="False">Whether to auto-continue optimization</ParamField>
<ParamField path="agent_class" type="type[opik_optimizer.optimizable_agent.OptimizableAgent] | None" optional={true}>Optional agent class to use</ParamField>
<ParamField path="project_name" type="str" optional={true} default="Optimization">Opik project name for logging traces (default: "Optimization")</ParamField>
<ParamField path="max_trials" type="int" optional={true} default="10">Number of trials for Bayesian Optimization (default: 10)</ParamField>
<ParamField path="args" type="Any" />
<ParamField path="kwargs" type="Any" />


### Model Support

There are two models to consider when using the `MetaPromptOptimizer`:
- `MetaPromptOptimizer.model`: The model used for the reasoning and candidate generation.
- `ChatPrompt.model`: The model used to evaluate the prompt.

The `model` parameter accepts any LiteLLM-supported model string (e.g., `"gpt-4o"`, `"azure/gpt-4"`,
`"anthropic/claude-3-opus"`, `"gemini/gemini-1.5-pro"`). You can also pass in extra model parameters
using the `model_parameters` parameter:

```python
optimizer = MetaPromptOptimizer(
    model="anthropic/claude-3-opus-20240229",
    model_parameters={
        "temperature": 0.7,
        "max_tokens": 4096
    }
)
```

## MCP Tool Calling Support

The MetaPrompt Optimizer is the only optimizer that currently supports **MCP (Model Context
Protocol) tool calling optimization**. This means you can optimize prompts that include MCP tools
and function calls.

<Note>
  MCP tool calling optimization is a specialized feature that allows the optimizer to understand and optimize prompts
  that use external tools and functions through the Model Context Protocol. This is particularly useful for complex
  agent workflows that require tool usage.
</Note>

For comprehensive information about tool optimization, see the [Tool Optimization Guide](/agent_optimization/algorithms/tool_optimization).

## Research and References

- [Meta-Prompting for Language Models](https://arxiv.org/abs/2401.12954)
