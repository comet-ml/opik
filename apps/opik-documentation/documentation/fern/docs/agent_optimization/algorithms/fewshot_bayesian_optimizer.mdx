---
title: "Few-Shot Bayesian Optimizer"
subtitle: "Optimize few-shot examples for chat prompts with Bayesian techniques."
description: "Learn how to use the Few-Shot Bayesian Optimizer to find optimal few-shot examples for your chat-based prompts using Bayesian optimization techniques."
pytest_codeblocks_skip: true
---

The FewShotBayesianOptimizer is a sophisticated prompt optimization tool that combines few-shot learning with Bayesian optimization techniques. It's designed to iteratively improve prompts by learning from examples and systematically exploring the optimization space, specifically for chat-based prompts.

<Note>
  **When to Use This Optimizer:**
  Use `FewShotBayesianOptimizer` when your primary goal is to find the optimal number and combination of few-shot examples (demonstrations) to accompany your main instruction prompt, particularly for **chat models**. If your task performance heavily relies on the quality and relevance of in-context examples, this optimizer is ideal.

**Key Trade-offs:**

- Focuses exclusively on selecting examples from the provided dataset; it does _not_ optimize the `prompt` text itself.
- The search space can be very large if the dataset for example selection is massive; `n_iterations` needs to be sufficient for adequate exploration.
- Effectiveness depends on the quality and diversity of the examples within your dataset.

</Note>

<Info>
  Have questions about `FewShotBayesianOptimizer`? Our [Optimizer & SDK FAQ](/agent_optimization/opik_optimizer/faq)
  answers common questions, including when to use this optimizer, how parameters like `min_examples` and `n_iterations`
  work, and whether it modifies the main instruction prompt.
</Info>

## How It Works

This optimizer focuses on finding the optimal set and number of few-shot examples to include with
your base instruction prompt for chat models. It uses [Optuna](https://optuna.org/), a hyperparameter
optimization framework, to guide this search:

1.  **Problem Definition**: The core task is to select the best combination of few-shot examples
    from your provided dataset to append to the `prompt`.
    The number of examples to select can range from `min_examples` to `max_examples`.

2.  (Optional) Create the few-shot prompt template: As a first step the few-shot optimizer updates the
    `prompt` with the few-shot examples placeholder and creates a few-shot example template.

3.  **Search Space**: For each trial, Optuna suggests:

    - `n_examples`: The number of few-shot examples to use for this particular trial (within the `min_examples` and `max_examples` range).
    - `example_indices`: A list of specific indices pointing to items in your dataset that will serve as these `n_examples`.

4.  **Prompt Construction**: For each trial, the few-shot example placeholder is replaced with the
    examples selected in the search space.

5.  **Evaluation**: This constructed few-shot prompt is then evaluated against a portion (or all) of
    your dataset using the specified `metric`. The evaluation measures how well this particular
    combination of few-shot examples helps the model perform the task.

6.  **Bayesian Optimization (via Optuna)**:

    - Optuna uses a Bayesian optimization algorithm (typically TPESampler - Tree-structured Parzen
      Estimator Sampler) to decide which combinations of `n_examples` and `example_indices` to try
      next.
    - It builds a probabilistic model of how different example choices affect the evaluation score
      and uses this model to balance exploration (trying new, uncertain combinations) and exploitation
      (focusing on combinations likely to yield high scores).
    - This process is repeated for `n_iterations` (number of trials).

7.  **Result**: After all trials, the optimizer returns the `prompt` with the best few-shot examples
    that achieved the best score.

Essentially, the `FewShotBayesianOptimizer` intelligently searches through the vast space of
possible few-shot example combinations to find the set that best primes your chat model for the
task.

<Tip>
  The core of this optimizer relies on robust evaluation (Step 4) where each combination of few-shot examples is
  assessed using your `metric` against the `dataset`. Understanding Opik's evaluation platform is key to effective use:
  - [Evaluation Overview](/evaluation/overview) - [Evaluate Prompts](/evaluation/evaluate_prompt) - [Metrics
  Overview](/evaluation/metrics/overview)
</Tip>

## Configuration Options

### Basic Configuration

```python
from opik_optimizer import FewShotBayesianOptimizer

optimizer = FewShotBayesianOptimizer(
    model="openai/gpt-4",  # or "azure/gpt-4"
    project_name="my-project",
    temperature=0.1,
    max_tokens=5000,
    num_threads=8,
    seed=42
)
```

### Advanced Configuration

The `FewShotBayesianOptimizer` relies on the [Optuna](https://optuna.org/) library, specifically
using its `TPESampler` for Bayesian optimization. The primary configurable parameters are those
exposed directly in its constructor:

- `model`: The LLM used for evaluating prompts with different few-shot example combinations.
- `min_examples` / `max_examples`: Define the range for the number of few-shot examples to be
  selected by Optuna during optimization.
- `n_iterations`: The total number of optimization iterations (trials) Optuna will run.
- `n_initial_prompts`: The number of initial random configurations Optuna will evaluate before
  starting its TPE sampling.

LLM call parameters (e.g., `temperature`, `max_tokens`, `seed`) are passed via keyword arguments
(`**model_kwargs`) to the constructor.

The optimizer works by letting Optuna suggest the number of examples (`n_examples`) and which
specific examples from the dataset to use for constructing few-shot prompts. These prompts are
then evaluated, and Optuna uses the scores to guide its search for the best combination of
examples and their count.

## Example Usage

```python
from opik_optimizer import FewShotBayesianOptimizer
from opik.evaluation.metrics import LevenshteinRatio
from opik_optimizer import datasets, ChatPrompt

# Initialize optimizer
optimizer = FewShotBayesianOptimizer(
    model="openai/gpt-4",
    temperature=0.1,
    max_tokens=5000
)

# Prepare dataset
dataset = datasets.hotpot_300()

# Define metric and prompt (see docs for more options)
def levenshtein_ratio(dataset_item, llm_output):
    return LevenshteinRatio().score(reference=dataset_item["answer"], output=llm_output)

prompt = ChatPrompt(
    messages=[
        {"role": "system", "content": "Provide an answer to the question."},
        {"role": "user", "content": "{question}"}
    ]
)

# Run optimization
# Note: `num_trials` is an argument for older versions or a conceptual representation.
# For current FewShotBayesianOptimizer, `n_iterations` is set at initialization.
# The optimize_prompt method itself doesn't take num_trials.
# We'll use a representative call here.
results = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,
    metric=levenshtein_ratio
)

# Access results
results.display()
```

## Model Support

The FewShotBayesianOptimizer supports all models available through LiteLLM. This provides broad
compatibility with providers like OpenAI, Azure OpenAI, Anthropic, Google, and many others,
including locally hosted models.

For detailed instructions on how to specify different models and configure providers, please refer
to the main [LiteLLM Support for Optimizers documentation page](/agent_optimization/opik_optimizer/litellm_support).

### Configuration Example using LiteLLM model string

```python
optimizer = FewShotBayesianOptimizer(
    model="openai/gpt-4o-mini",  # Example: Using OpenAI via LiteLLM
    project_name="my-project",
    temperature=0.1,
    max_tokens=5000
)
```

## Best Practices

1. **Dataset Preparation**

   - Minimum 50 examples recommended
   - Diverse and representative samples
   - Clear input-output pairs

2. **Parameter Tuning**

   - Start with default parameters
   - Adjust based on problem complexity
   - Monitor convergence metrics

3. **Evaluation Strategy**

   - Use separate validation set
   - Track multiple metrics
   - Document optimization history

4. **Performance Optimization**
   - Adjust num_threads based on resources
   - Balance min_examples and max_examples
   - Monitor memory usage

## Research and References

- [Bayesian Optimization for Hyperparameter Tuning](https://arxiv.org/abs/1206.2944)
- [Few-shot Learning with Bayesian Optimization](https://arxiv.org/abs/1904.04232)
- [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/)

## Next Steps

- Explore other [Optimization Algorithms](/agent_optimization/overview#optimization-algorithms)
- Explore [Dataset Requirements](/agent_optimization/opik_optimizer/datasets)
- Try the [Example Projects & Cookbooks](/agent_optimization/cookbook) for runnable Colab notebooks using this optimizer
