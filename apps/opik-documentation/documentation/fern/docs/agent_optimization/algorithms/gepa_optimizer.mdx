---
title: "GEPA Optimizer"
subtitle: "Single-turn system prompt optimization with reflection"
description: "Use the external GEPA package through Opik's `GepaOptimizer` to optimize a single system prompt for single-turn tasks with a reflection model."
---

`GepaOptimizer` wraps the external [GEPA](https://github.com/gepa-ai/gepa) package to optimize a
single system prompt for single-turn tasks. It maps Opik datasets and metrics into GEPA’s expected
format, runs GEPA’s optimization using a task model and a reflection model, and returns a standard
`OptimizationResult` compatible with the Opik SDK.

<Warning>
  GEPA integration is currently in <strong>Beta</strong>. APIs and defaults may evolve, and performance characteristics
  can change as we iterate. Please report issues or feedback on GitHub.
</Warning>

<Note>
  When to use: Choose `GepaOptimizer` when you have a single-turn task (one user input → one model
  response) and you want to optimize the system prompt using a reflection-driven search.

Key trade-offs:

- GEPA (DefaultAdapter) focuses on a single system prompt; multi-turn or tool-heavy flows are not the target.
- Requires both a task model and a reflection model (can be the same; often the reflection model is stronger).
- GEPA runs additional reflection passes; expect extra LLM calls compared to simple optimizers.
  </Note>

## Requirements

- Python: `pip install opik-optimizer gepa`
- Models: Uses LiteLLM-style model names for both the task model and reflection model (e.g.,
  `openai/gpt-4o-mini`, `openai/gpt-4o`). Set the appropriate environment variables (e.g.,
  `OPENAI_API_KEY`). See [LiteLLM Support for Optimizers](/agent_optimization/opik_optimizer/litellm_support).

## Quick Example (Tiny Test)

```python
"""
Optimize a simple system prompt on the tiny_test dataset.
Requires: pip install gepa, and a valid OPENAI_API_KEY for LiteLLM-backed models.
"""
from typing import Any, Dict

from opik.evaluation.metrics import LevenshteinRatio
from opik.evaluation.metrics.score_result import ScoreResult

from opik_optimizer import ChatPrompt, datasets
from opik_optimizer.gepa_optimizer import GepaOptimizer

def levenshtein_ratio(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
    return LevenshteinRatio().score(reference=dataset_item["label"], output=llm_output)

dataset = datasets.tiny_test()

prompt = ChatPrompt(
    system="You are a helpful assistant. Answer concisely with the exact answer.",
    user="{text}",
)

optimizer = GepaOptimizer(
    model="openai/gpt-4o-mini",
    reflection_model="openai/gpt-4o",  # stronger reflector is often helpful
    project_name="GEPA_TinyTest",
    temperature=0.2,
    max_tokens=200,
)

result = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,
    metric=levenshtein_ratio,
    max_metric_calls=12,
    reflection_minibatch_size=2,
    n_samples=5,
)

result.display()
```

> Reference implementation: `sdks/opik_optimizer/scripts/litellm_gepa_tiny_test_example.py`

## Example (Hotpot-style with LiteLLM)

The following mirrors our example that uses a Hotpot-style dataset and LiteLLM models.

```python
from typing import Any, Dict

import opik
from opik_optimizer import ChatPrompt
from opik_optimizer.gepa_optimizer import GepaOptimizer
from opik_optimizer.datasets import hotpot_300
from opik_optimizer.utils import search_wikipedia

from opik.evaluation.metrics import LevenshteinRatio
from opik.evaluation.metrics.score_result import ScoreResult

dataset = hotpot_300()

def levenshtein_ratio(dataset_item: Dict[str, Any], llm_output: str) -> ScoreResult:
    metric = LevenshteinRatio()
    return metric.score(reference=dataset_item["answer"], output=llm_output)

prompt = ChatPrompt(
    system="Answer the question",
    user="{question}",
    tools=[
        {
            "type": "function",
            "function": {
                "name": "search_wikipedia",
                "description": "This function is used to search wikipedia abstracts.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "query": {
                            "type": "string",
                            "description": "The query parameter is the term or phrase to search for.",
                        },
                    },
                    "required": ["query"],
                },
            },
        },
    ],
    function_map={"search_wikipedia": opik.track(type="tool")(search_wikipedia)},
)

optimizer = GepaOptimizer(
    model="openai/gpt-4o-mini",   # task model
    reflection_model="openai/gpt-4o",  # reflection model
    project_name="GEPA-Hotpot",
    temperature=0.7,
    max_tokens=400,
)

result = optimizer.optimize_prompt(
    prompt=prompt,
    dataset=dataset,
    metric=levenshtein_ratio,
    max_metric_calls=60,
    reflection_minibatch_size=5,
    candidate_selection_strategy="best",
    n_samples=12,
)

result.display()
```

> Reference implementation: `sdks/opik_optimizer/scripts/litellm_gepa_hotpot_example.py`

## Usage Notes

- Single-turn focus: GEPA’s default adapter targets optimizing a single system message for single-turn tasks.
- Dataset mapping: The optimizer heuristically infers input/output keys when not provided. Ensure your dataset items
  contain a clear input field (e.g., `text`, `question`) and a reference label/answer field.
- Metrics: Any metric that returns an Opik `ScoreResult` can be used (e.g., `LevenshteinRatio`).
- Budget: `max_metric_calls` determines the optimization budget; higher budgets can find better prompts at more cost.
- Reflection batching: `reflection_minibatch_size` controls how many items are considered per reflection step.

## API Summary

Constructor:

```python
GepaOptimizer(
    model: str,
    project_name: str | None = None,
    reflection_model: str | None = None,  # defaults to model if None
    verbose: int = 1,
    **model_kwargs,
)
```

Common `model_kwargs` include LiteLLM parameters such as `temperature`, `max_tokens`, and others.

Optimization:

```python
opt_result = optimizer.optimize_prompt(
    prompt: ChatPrompt,
    dataset: opik.Dataset,
    metric: Callable[[dict, str], ScoreResult],
    max_metric_calls: int,
    reflection_minibatch_size: int = 3,
    candidate_selection_strategy: str = "pareto",  # e.g., "pareto" or "best"
    n_samples: int | None = None,
)
```

The returned `OptimizationResult` includes the best prompt, score, and useful `details` such as
GEPA’s best candidate, validation scores, and the evolution history.

## Comparisons & Benchmarks

- Comparisons: GEPA targets optimizing a single system prompt for single-turn tasks, similar in scope
  to MetaPrompt on simple flows. Unlike Evolutionary or MIPRO, it does not aim to explore widely different
  prompt structures or multi-step agent graphs. For chat prompts heavily reliant on few-shot examples,
  the Few-shot Bayesian optimizer may be more appropriate.
- Benchmarks: TBD. GEPA is newly integrated; we will add results as we complete our evaluation runs.

## Troubleshooting

- Install error: Ensure `pip install gepa` completes successfully in the same environment as `opik-optimizer`.
- API keys: Set provider keys (e.g., `OPENAI_API_KEY`) for LiteLLM to load your chosen models.
- Low improvements: Increase `max_metric_calls`, try a stronger `reflection_model`, or adjust the starting system prompt.
- Mismatch fields: Confirm your dataset’s input/output field names align with your prompt’s placeholders and metric’s reference.
