---
title: "MIPRO Optimizer: Agent & Complex Prompt Optimization"
pytest_codeblocks_skip: true
---
The MiproOptimizer is a specialized prompt optimization tool that implements the MIPRO (Multi-agent Interactive Prompt Optimization) algorithm. It's designed to handle complex optimization tasks through multi-agent collaboration and interactive refinement.

<Note>
  **When to Use This Optimizer:**
  `MiproOptimizer` is the preferred choice for complex tasks, especially those involving **tool use** or requiring **multi-step reasoning**. If you are building an agent that needs to interact with external tools or follow a sophisticated chain of thought, and you want to optimize the underlying prompts and agent structure, `MiproOptimizer` (which leverages DSPy) is highly suitable.

  **Key Trade-offs:**
  - The optimization process can be more involved than single-prompt optimizers due to its reliance on the DSPy framework and compilation process.
  - Understanding basic DSPy concepts (like Modules, Signatures, Teleprompters) can be beneficial for advanced customization and debugging, though Opik abstracts much of this.
  - Debugging can sometimes be more complex as you are optimizing a program/agent structure, not just a single string.
</Note>

<Info>
  Got questions about `MiproOptimizer` or its use with DSPy? 
  The [Optimizer & SDK FAQ](/agent_optimization/opik_optimizer/faq) covers topics such as when to use `MiproOptimizer`, its relationship with DSPy, the role of `num_candidates`, and using it without deep DSPy knowledge.
</Info>

## How It Works

The `MiproOptimizer` leverages the [DSPy](https://dspy.ai/) library, specifically an internal version of its MIPRO (Modular Instruction Programming and Optimization) teleprompter, to optimize potentially complex prompt structures, including those for tool-using agents.

Here's a simplified overview of the process:

1.  **DSPy Program Representation**: Your task, as defined by `TaskConfig` (including the `instruction_prompt`, input/output fields, and any `tools`), is translated into a DSPy program structure. If tools are provided, this often involves creating a `dspy.ReAct` or similar agent module.

2.  **Candidate Generation (Compilation)**: The core of MIPRO involves compiling this DSPy program. This "compilation" is an optimization process itself:
    *   It explores different ways to formulate the instructions within the DSPy program's modules (e.g., the main instruction, tool usage instructions if applicable).
    *   It also optimizes the selection of few-shot demonstrations to include within the prompts for these modules, if the DSPy program structure uses them.
    *   This is guided by an internal DSPy teleprompter algorithm (like `MIPROv2` in the codebase) which uses techniques like bootstrapping demonstrations and proposing instruction variants.

3.  **Evaluation**: Each candidate program (representing a specific configuration of instructions and/or demonstrations) is evaluated on your training dataset (`dataset`) using the specified `metric_config`. The `MiproOptimizer` uses DSPy's evaluation mechanisms, which can handle complex interactions, especially for tool-using agents.

4.  **Iterative Refinement**: The teleprompter iteratively refines the program based on these evaluations, aiming to find a program configuration that maximizes the metric score. The `num_candidates` parameter in `optimize_prompt` influences how many of these configurations are explored.

5.  **Result**: The `MiproOptimizer` returns an `OptimizationResult` containing the best-performing DSPy program structure found. This might be a single optimized prompt string or, for more complex agents, a collection of optimized prompts that make up the agent's internal logic (e.g., `tool_prompts` in the `OptimizationResult`).

Essentially, `MiproOptimizer` uses MIPRO algorithm to not just optimize a single string prompt, but potentially a whole system of prompts and few-shot examples that define how an LLM (or an LLM-based agent) should behave to accomplish a task.

<Tip>
  The evaluation of each candidate program (Step 3) is crucial. It uses your `MetricConfig` and `dataset` to score how well a particular set of agent instructions or prompts performs. Since `MiproOptimizer` often deals with agentic behavior, understanding Opik's broader evaluation tools is beneficial:
  - [Evaluation Overview](/evaluation/overview)
  - [Evaluate Agents](/evaluation/evaluate_agents) (particularly relevant)
  - [Evaluate Prompts](/evaluation/evaluate_prompt)
  - [Metrics Overview](/evaluation/metrics/overview)
</Tip>

## Configuration Options

### Basic Configuration

```python
from opik_optimizer import MiproOptimizer

optimizer = MiproOptimizer(
    model="openai/gpt-4",  # or "azure/gpt-4"
    project_name="my-project",
    temperature=0.1,
    max_tokens=5000,
    num_threads=8,
    seed=42
)
```

### Advanced Configuration

The `MiproOptimizer` leverages the [DSPy](https://dspy.ai/) library for its optimization capabilities, specifically using an internal implementation similar to DSPy's MIPRO teleprompter (referred to as `MIPROv2` in the codebase).

The constructor for `MiproOptimizer` is simple (`model`, `project_name`, `**model_kwargs`). The complexity of the optimization is managed within the DSPy framework when `optimize_prompt` is called.

Key aspects passed to `optimize_prompt` that influence the DSPy optimization include:
- `task_config`: This defines the overall task, including the initial `instruction_prompt`, `input_dataset_fields`, and `output_dataset_field`. If `task_config.tools` are provided, `MiproOptimizer` will attempt to build and optimize a DSPy agent that uses these tools.
- `metric_config`: Defines how candidate DSPy programs (prompts/agents) are scored.
- `num_candidates`: This parameter from `optimize_prompt`

## Research and References

- [MIPRO Paper - Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs](https://arxiv.org/abs/2406.11695)