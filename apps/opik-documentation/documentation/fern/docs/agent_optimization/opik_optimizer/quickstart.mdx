---
title: "Opik Optimizer Quickstart: Get Started with Prompt Optimization"
pytest_codeblocks_skip: true
---
This guide will help you get started with Opik Optimizer for improving your LLM
prompts through systematic optimization.

## Prerequisites

- Python 3.9 or higher
- An Opik API key ([sign up here](https://www.comet.com/signup?from=llm) if you don't have one)

## Installation

Install Opik and the optimizer package:

```bash
# Using pip
pip install opik opik-optimizer

# Using uv (recommended for faster installation)
uv pip install opik-optimizer
```

Configure your Opik environment:

```bash
# Install the Opik CLI if not already installed
pip install opik

# Configure your API key
opik configure
```

## Basic Usage

Here's a complete example of optimizing a prompt using the Few-Shot Bayesian Optimizer, broken down into individual steps:

<Steps>
  <Step title="1. Import necessary modules">
    First, we import all the required classes and functions from `opik` and `opik_optimizer`.
    ```python
    from opik.evaluation.metrics import LevenshteinRatio
    from opik_optimizer import FewShotBayesianOptimizer, MetricConfig, TaskConfig, from_llm_response_text, from_dataset_field
    from opik_optimizer.demo import get_or_create_dataset
    ```
  </Step>
  <Step title="2. Define your evaluation dataset">
    You can use a demo dataset for testing, or create/load your own. `get_or_create_dataset` is a helper for demo purposes.
    ```python
    # You can use a demo dataset for testing, or your own dataset
    dataset = get_or_create_dataset("tiny-test")
    print(f"Using dataset: {dataset.name}, with {len(dataset.items)} items.")
    ```
  </Step>
  <Step title="3. Configure the evaluation metric">
    `MetricConfig` tells the optimizer how to score the LLM's outputs. Here, `LevenshteinRatio` measures the similarity between the model's response and a "label" field in the dataset.
    ```python
    # This example uses Levenshtein distance to measure output quality
    metric_config = MetricConfig(
        metric=LevenshteinRatio(),
        inputs={
            "output": from_llm_response_text(),  # Model's output
            "reference": from_dataset_field(name="label"),  # Ground truth
        }
    )
    print("MetricConfig created.")
    ```
  </Step>
  <Step title="4. Define your base prompt">
    This is the initial instruction that the `FewShotBayesianOptimizer` will try to enhance by adding few-shot examples.
    ```python
    # Following best practices, provide clear instructions and context
    system_prompt = """You are an expert assistant. Your task is to answer questions
    accurately and concisely. Consider the context carefully before responding."""
    print("System prompt defined.")
    ```
  </Step>
  <Step title="5. Choose and configure an optimizer">
    Instantiate `FewShotBayesianOptimizer`, specifying the model, project name, and parameters like `min_examples`, `max_examples`, and `n_iterations` which control the few-shot search.
    ```python
    optimizer = FewShotBayesianOptimizer(
        model="gpt-4",  # Choose your preferred model, ensure it's configured in LiteLLM if not OpenAI
        project_name="Prompt Optimization Quickstart",
        min_examples=2,  # Minimum number of examples to try
        max_examples=5,  # Maximum number of examples to try (kept small for quick demo)
        n_iterations=3   # Number of optimization iterations (kept small for quick demo)
    )
    print(f"Optimizer configured: {type(optimizer).__name__}")
    ```
  </Step>
  <Step title="6. Configure the task">
    `TaskConfig` ties everything together: the `instruction_prompt`, the dataset fields to use for input and output, and whether to format for chat models.
    ```python
    task_config = TaskConfig(
        instruction_prompt=system_prompt,
        input_dataset_fields=["text"],  # Input field(s) from your dataset
        output_dataset_field="label",   # Output field from your dataset
        use_chat_prompt=True,  # Use chat format for modern LLMs
    )
    print("TaskConfig created.")
    ```
  </Step>
  <Step title="7. Run the optimization">
    The `optimizer.optimize_prompt(...)` method is called with the dataset, metric configuration, and task configuration to start the optimization process.
    ```python
    print("Starting optimization...")
    result = optimizer.optimize_prompt(
        dataset=dataset,
        metric_config=metric_config,
        task_config=task_config,
        n_samples=10 # Use a subset of the dataset for faster evaluation during optimization
    )
    print("Optimization finished.")
    ```
  </Step>
  <Step title="8. View the results">
    Finally, `result.display()` is called to show a summary of the optimization, including the best prompt found and its score.
    ```python
    print("Optimization Results:")
    result.display() 
    ```
    The `OptimizationResult` object also contains more details in `result.history` and `result.details`.
  </Step>
</Steps>

The optimization results will be displayed in your console and are also available in the
Opik Agent Optimization dashboard:

<Frame>
  <img src="/img/agent_optimization/trial_dashboard.png" />
</Frame>

## Next Steps

1. Learn about different [optimization algorithms](/agent_optimization/overview#optimization-algorithms) to choose the best one for your use case
2. Explore [prompt engineering best practices](/agent_optimization/best_practices/prompt_engineering)
3. Set up your own [evaluation datasets](/agent_optimization/opik_optimizer/datasets)
4. Review the [API reference](/agent_optimization/opik_optimizer/reference) for detailed configuration options

<Info>
  ðŸš€ Want to see Opik Optimizer in action? Check out our [Example Projects & Cookbooks](/agent_optimization/cookbook) for runnable Colab notebooks covering real-world optimization workflows, including HotPotQA and synthetic data generation.
</Info>
