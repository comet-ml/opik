---
description: Learn about the core concepts of Opik Agent Optimizer, including key
  terms, evaluation processes, optimization workflows, and best practices for effective
  LLM optimization.
headline: Concepts | Opik Documentation
og:description: Learn to refine agent and prompt optimization in Opik for enhanced
  LLM evaluation and performance.
og:site_name: Opik Documentation
og:title: Master Optimization Concepts with Opik
subtitle: Common terminology and how we optimize
title: Opik Agent Optimizer Core Concepts
---

## Overview

Understanding the core concepts of the Opik Agent Optimizer is essential for unlocking its full
potential in LLM evaluation and optimization. This section explains the foundational terms,
processes, and strategies that underpin effective agent and prompt optimization within Opik.

## What is Agent Optimization (and Prompt Optimization)?

In Opik, **Agent Optimization** refers to the systematic process of refining and evaluating the
prompts, configurations, and overall design of language model-based applications to maximize their
performance. This is an iterative approach leveraging continuous testing, data-driven refinement,
and advanced evaluation techniques.

**Prompt Optimization** is a crucial subset of Agent Optimization. It focuses specifically on
improving the instructions (prompts) given to Large Language Models (LLMs) to achieve desired
outputs more accurately, consistently, and efficiently. Since prompts are the primary way to
interact with and guide LLMs, optimizing them is fundamental to enhancing any LLM-powered agent or
application.

`Opik Agent Optimizer` provides tools for both: directly optimizing individual prompt strings and
also for optimizing more complex agentic structures that might involve multiple prompts, few-shot
examples, or tool interactions.

## Key Terms

<AccordionGroup>
  <Accordion title="Optimizer">
    A specialized algorithm within the Opik Agent Optimizer SDK designed to enhance prompt effectiveness. Each optimizer
    (e.g., [`MetaPromptOptimizer`](/agent_optimization/algorithms/metaprompt_optimizer),
    [`FewShotBayesianOptimizer`](/agent_optimization/algorithms/fewshot_bayesian_optimizer),
    [`EvolutionaryOptimizer`](/agent_optimization/algorithms/evolutionary_optimizer),
    [`HRPO`](/agent_optimization/algorithms/hierarchical_adaptive_optimizer))
    employs unique strategies and configurable parameters to address specific optimization goals.
  </Accordion>
  <Accordion title="ChatPrompt">
    The object to optimize which contains your chat messages with placeholders for variables that change on each example.
    See the [API Reference](/agent_optimization/api-reference#chatprompt).
  </Accordion>
  <Accordion title="Metric">
    An object defining how to measure the performance of a prompt. The metric functions should accept two parameters:
    - `dataset_item`: A dictionary with the dataset item keys
    - `llm_output`: This will be populated with the LLM response

    It should return either a [ScoreResult](https://www.comet.com/docs/opik/python-sdk-reference/Objects/ScoreResult.html) object or a float.
  </Accordion>
  <Accordion title="Dataset (for Optimization)">
    A collection of data items, typically with inputs and expected outputs (ground truth), used to guide and evaluate
    the prompt optimization process. For best results, split your data into separate training and validation datasetsâ€”the optimizer uses the training dataset to analyze failures and generate improvements, then evaluates candidates on the validation dataset to prevent overfitting. See [Datasets](/evaluation/manage_datasets) and [Define datasets](/agent_optimization/optimization/define_datasets) for more information.
  </Accordion>
  <Accordion title="Optimization Run">
    A single execution of a prompt optimization process using a specific configuration. For example,
    calling `optimizer.optimize_prompt(...)` once constitutes a Run. Each Run is typically logged to
    the Opik platform for tracking.
  </Accordion>
  <Accordion title="Optimization Trial">
    Each optimization run is made up of one or more optimization trials. A trial corresponds to a
    single evaluation of a candidate prompt or prompt configuration. You can view each trial's prompt,
    score, and reasoning history inside the Opik UI to understand optimizer progress.
  </Accordion>
  <Accordion title="Prompt model">
  The model that is used to evaluate the prompt. This is the model that you use the prompt with,
  should be the same as the model you use in your application. Configure it via `ChatPrompt(model="provider/model-name")`.
  
  See [Configure LLM Providers](/agent_optimization/optimization/configure_models) for setup instructions.
  </Accordion>
  <Accordion title="Optimizer model">
    The model that is used to optimize the prompt. This is the model that the optimizer uses to improve your prompt,
    you will get the best performance by using the most powerful model for the optimization. Configure it via the optimizer's `model` parameter.
    
    See [Configure LLM Providers](/agent_optimization/optimization/configure_models) for setup instructions.
  </Accordion>
</AccordionGroup>

## Next Steps

- Explore specific [Optimizers](/agent_optimization/overview#optimization-algorithms) for algorithm details.
- Refer to the [FAQ](/agent_optimization/faq) for common questions and troubleshooting.
- Refer to the [API Reference](/agent_optimization/api-reference) for detailed configuration options.

<Info>
  ðŸ““ Want to see these concepts in action? Check out our [Example Projects & Cookbooks](/agent_optimization/optimizer-cookbooks/optimizer_introduction_cookbook)
  for step-by-step, runnable Colab notebooks.
</Info>
