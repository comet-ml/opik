---
description: Learn about Opik's automated LLM prompt and agent optimization SDK. Discover
  MetaPrompt, Few-shot Bayesian, and Evolutionary optimization algorithms for enhanced
  performance.
headline: Opik Agent Optimizer | Opik Documentation
keywords: LLM optimization, prompt engineering, agent optimization, MetaPrompt, Few-shot
  learning, Bayesian optimization, Evolutionary algorithms, DSPy
og:description: Learn to automate prompt and agent workflow tuning using Opik's optimization
  SDK for improved performance and efficiency.
og:image: /img/agent_optimization/introducing.png
og:site_name: Opik Documentation
og:title: Optimize Prompts with Opik Agent Optimizer
title: Agent Optimization
---

**Opik Agent Optimizer** is a turnkey, open-source agent and prompt optimization SDK. It automatically tunes prompts, tools, and agent workflows using the datasets, metrics, and traces you already log to Opik. Instead of hand-editing instructions and re-running evaluations, pick an optimizer (MetaPrompt, HRPO, Evolutionary, GEPA, etc.) and let it iterate for you online or fully offline inside Docker and Kubernetes.

<Frame>
  <img src="/img/agent_optimization/dashboard.png" alt="Opik Agent Optimizer Dashboard showing optimization progress" />
</Frame>

## Why teams choose Opik Agent Optimizer

- **Automatic prompt optimization** â€“ end-to-end workflow that installs in minutes and runs locally or in your  stack.
- **Open-source and framework agnostic** â€“ no lock-in, use Opikâ€™s first-party optimizers or community favorites like GEPA in the same SDK.
- **Agent-aware** â€“ optimize beyond system prompts, including MCP tool signatures, function-calling schemas, and full multi-agent systems.
- **Deep observability** â€“ every trial logs prompts, tool calls, traces, and metric reasons to Opik so you can explain and ship changes confidently.

## Key capabilities

<Cards>
  <Card title="Optimizers" body="MetaPrompt, HRPO (Hierarchical Reflective Prompt Optimizer), Few-Shot Bayesian, Evolutionary, GEPA, Parameter tuningâ€”all open-source, mixing in-house algorithms with popular public projects." />
  <Card title="Tool & MCP support" body="Optimize beyond system prompts: MCP tool signatures, OpenAI function calling, and traditional prompt text all benefit from the same workflows." />
  <Card title="Agent integrations" body="Ready-made adapters for LangGraph, Google ADK, MCP workflows, and custom agents, plus the ability to build your own `OptimizableAgent`." />
  <Card title="Dashboard analytics" body="Optimization runs show progress charts, per-trial prompts, dataset coverage, and traces for root-cause analysis." />
  <Card title="Secure & offline" body="Run the SDK locally or inside Opikâ€™s Docker compose to keep data inside your network." />
</Cards>

## How it works

<Steps>
  <Step title="1. Prepare data & metrics">
    Use Opik datasets (CSV upload, API, or trace exports) plus deterministic metrics/`ScoreResult` functions. See [Define datasets](/agent_optimization/optimization/define_datasets) and [Define metrics](/agent_optimization/optimization/define_metrics).
  </Step>
  <Step title="2. Pick an optimizer">
    Choose the best algorithm for your task (see [Optimization algorithms](/agent_optimization/algorithms/overview)). All optimizers expose the same API, so you can swap them easily or chain runs.
  </Step>
  <Step title="3. Inspect & ship">
    Results land in the Opik dashboard under **Evaluation â†’ Optimization runs**, where you can compare prompts, failure modes, and dataset coverage before promoting the change.
  </Step>
</Steps>

## Start fast

- Follow the [Quickstart](/agent_optimization/getting_started/quickstart) to run your first optimization locally.
- Prefer notebooks? Launch the [Quickstart notebook](/agent_optimization/getting_started/quickstart_notebook).
- Need scenario-specific guidance? Explore the [Cookbooks](/agent_optimization/cookbooks/optimizer_introduction_cookbook).

## Optimization Algorithms

The optimizer implements both proprietary and open-source optimization algorithms. Each one has it's
strengths and weaknesses, we recommend first trying out either GEPA or HRPO (Hierarchical Reflective Prompt Optimizer)
as a first step:

| **Algorithm**                                                                               | **Description**                                                                                                                                                                                                      |
| ------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| [MetaPrompt Optimization](/agent_optimization/algorithms/metaprompt_optimizer)              | Uses an LLM ("reasoning model") to critique and iteratively refine an initial instruction prompt. Good for general prompt wording, clarity, and structural improvements. **Supports [MCP tool calling optimization](/agent_optimization/algorithms/tool_optimization).** |
| [HRPO (Hierarchical Reflective Prompt Optimizer)](/agent_optimization/algorithms/hierarchical_adaptive_optimizer) | Uses hierarchical root cause analysis to systematically improve prompts by analyzing failures in batches, synthesizing findings, and addressing identified failure modes. Best for complex prompts requiring systematic refinement based on understanding why they fail. |
| [Few-shot Bayesian Optimization](/agent_optimization/algorithms/fewshot_bayesian_optimizer) | Specifically for chat models, this optimizer uses Bayesian optimization (Optuna) to find the optimal number and combination of few-shot examples (demonstrations) to accompany a system prompt.                      |
| [Evolutionary Optimization](/agent_optimization/algorithms/evolutionary_optimizer)          | Employs genetic algorithms to evolve a population of prompts. Can discover novel prompt structures and supports multi-objective optimization (e.g., score vs. length). Can use LLMs for advanced mutation/crossover. |
| [GEPA Optimization](/agent_optimization/algorithms/gepa_optimizer)                   | Wraps the external GEPA package to optimize a single system prompt for single-turn tasks using a reflection model. Requires `pip install gepa`.                                         |
| [Parameter Optimization](/agent_optimization/algorithms/parameter_optimizer)                   | Optimizes LLM call parameters (temperature, top_p, etc.) using Bayesian optimization. Uses Optuna for efficient parameter search with global and local search phases. Best for tuning model behavior without changing the prompt.                                         |

<Tip>
  Want to see numbers? Check the new [optimizer benchmarks](/agent_optimization/algorithms/benchmarks) page for the latest performance table and instructions for running the benchmark suite yourself.
</Tip>

## Next Steps

1. Explore specific [Optimizers](/agent_optimization/overview#optimization-algorithms) for algorithm details.
2. Refer to the [FAQ](/agent_optimization/faq) for common questions and troubleshooting.
3. Refer to the [API Reference](/agent_optimization/api-reference) for detailed configuration options.

<Info>
  ðŸš€ Want to see Opik Agent Optimizer in action? Check out our [Example Projects & Cookbooks](/agent_optimization/cookbooks/optimizer_introduction_cookbook) for runnable Colab notebooks covering real-world optimization workflows, including HotPotQA and synthetic data generation.
</Info>
