This guide helps you integrate the Opik platform with your existing LLM application. The goal of this guide is to help you log your first LLM calls and chains to the Opik platform.

<Frame>
  <img src="/img/home/traces_page_for_quickstart.png" />
</Frame>

## ⚡ Quick Start with AI Assistants

Get Opik integrated instantly by copying this prompt to Cursor, Claude, or any AI coding assistant:

```text
# OPIK Agentic Onboarding

## Goals

You must help me:

1. Integrate the Opik client with my existing LLM application
2. Set up tracing for my LLM calls and chains

## Rules

Before you begin, you must understand and strictly adhere to these core principles:

1. Code Preservation & Integration Guidelines:

   - Existing business logic must remain untouched and unmodified
   - Only add Opik-specific code (decorators, imports, handlers, env vars)
   - Integration must be non-invasive and backwards compatible

2. Process Requirements:

   - Follow the workflow steps sequentially without deviation
   - Validate completion of each step before proceeding
   - Request explicit approval for any workflow modifications

3. Documentation & Resources:

   - Reference official Opik documentation at https://www.comet.com/docs/opik/quickstart.md
   - Follow Opik best practices and recommended patterns
   - Maintain detailed integration notes and configuration details

4. Testing & Validation:
   - Verify Opik integration without impacting existing functionality
   - Validate tracing works correctly for all LLM interactions
   - Ensure proper error handling and logging

## Integration Workflow

### Step 1: Language and Compatibility Check

First, analyze the codebase to identify:

1. Primary programming language and frameworks
2. Existing LLM integrations and patterns

Compatibility Requirements:

- Supported Languages: Python, JavaScript/TypeScript

If the codebase uses unsupported languages:

- Stop immediately
- Inform me that the codebase is unsupported for AI integration

Only proceed to Step 2 if:

- Language is Python or JavaScript/TypeScript

### Step 2: Codebase Discovery & Entrypoint Confirmation

After verifying language compatibility, perform a full codebase scan with the following objectives:

- LLM Touchpoints: Locate all files and functions that invoke or interface with LLMs or can be a candidates for tracing.
- Entrypoint Detection: Identify the primary application entry point(s) (e.g., main script, API route, CLI handler). If ambiguous, pause and request clarification on which component(s) are most important to trace before proceeding.
  ⚠️ Do not proceed to Step 3 without explicit confirmation if the entrypoint is unclear.
- Return the LLM Touchpoints to me

### Step 3: Discover Available Integrations

After I confirm the LLM Touchpoints and entry point, find the list of supported integrations at https://www.comet.com/docs/opik/tracing/integrations/overview.md

### Step 4: Deep Analysis Confirmed files for LLM Frameworks & SDKs

Using the files confirmed in Step 2, perform targeted inspection to detect specific LLM-related technologies in use, such as:
SDKs: openai, anthropic, huggingface, etc.
Frameworks: LangChain, LlamaIndex, Haystack, etc.

### Step 5: Pre-Implementation Development Plan (Approval Required)

Do not write or modify code yet. You must propose me a step-by-step plan including:

- Opik packages to install
- Files to be modified
- Code snippets for insertion, clearly scoped and annotated
- Where to place Opik API keys, with placeholder comments (Visit https://comet.com/opik/your-workspace-name/get-started to copy your API key)
  Wait for approval before proceeding!

### Step 6: Execute the Integration Plan

After approval:

- Run the package installation command via terminal (pip install opik, npm install opik, etc.).
- Apply code modifications exactly as described in Step 5.
- Keep all additions minimal and non-invasive.
  Upon completion, review the changes made and confirm installation success.

### Step 7: Request User Review and Wait

Notify me that all integration steps are complete.
"Please run the application and verify if Opik is capturing traces as expected. Let me know if you need adjustments."

### Step 8: Debugging Loop (If Needed)

If issues are reported:

1. Parse the error or unexpected behavior from feedback.
2. Re-query the Opik docs using https://www.comet.com/docs/opik/quickstart.md if needed.
3. Propose a minimal fix and await approval.
4. Apply and revalidate.
```

---

## Set up

Getting started is as simple as creating an [account on Comet](https://www.comet.com/signup?from=llm) or [self-hosting the platform](/self-host/overview).

Once your account is created, you can start logging traces by installing the Opik Python SDK:

<Tabs>
  <Tab value="Python" title="Python">

```bash
pip install opik
```

</Tab>

<Tab value="JS / TS" title="JS / TS">

```bash
npm install opik
```

</Tab>
</Tabs>

and configuring the SDK with:

<Tabs>
<Tab value="Python" title="Python">

If you are using the Python SDK, we recommend running the `opik configure` command
from the command line. This will prompt you to set up your API key and Opik instance URL (if applicable) to ensure proper routing and authentication.

```bash
opik configure
```

You can learn more about configuring the Python SDK [here](/tracing/sdk_configuration).

</Tab>
<Tab value="JS / TS" title="JS / TS">

If you are using the Javascript SDK you will need to set the required
parameters when initializing the client:

```js
import { Opik } from "opik";

// Create a new Opik client with your configuration
const client = new Opik({
  apiKey: "<your-api-key>",
  apiUrl: "https://www.comet.com/opik/api", // Replace with http://localhost:5173/api if you are self-hosting
  projectName: "default",
  workspaceName: "<your-workspace-name>", // Typically the same as your username
});
```

</Tab>

</Tabs>

## Adding Opik observability to your codebase

### Logging LLM calls

The first step in integrating Opik with your codebase is to track your LLM calls. If you are using OpenAI, OpenRouter, or any LLM provider that is supported by LiteLLM, then you
can use one of our [integrations](/tracing/integrations/overview):

<Tabs>
    <Tab value="OpenAI (Python)" title="OpenAI (Python)">

```python
from opik.integrations.openai import track_openai
from openai import OpenAI

# Wrap your OpenAI client
openai_client = OpenAI()
openai_client = track_openai(openai_client)
```

All OpenAI calls made using the `openai_client` will now be logged to Opik.

</Tab>
<Tab value="OpenRouter (Python)" title="OpenRouter (Python)">

```python {pytest_codeblocks_skip=true}
from opik.integrations.openai import track_openai
from openai import OpenAI

# Initialize OpenRouter client
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="YOUR_OPENROUTER_API_KEY"
)
client = track_openai(client)

# Optional headers for OpenRouter leaderboard
headers = {
    "HTTP-Referer": "YOUR_SITE_URL",  # Optional
    "X-Title": "YOUR_SITE_NAME"  # Optional
}

response = client.chat.completions.create(
    model="openai/gpt-4",  # You can use any model available on OpenRouter
    extra_headers=headers,
    messages=[{"role": "user", "content": "Hello!"}]
)
```

All OpenRouter calls made using the `client` will now be logged to Opik.

</Tab>
<Tab value="LiteLLM (Python)" title="LiteLLM (Python)">

```python {pytest_codeblocks_skip=true}
from litellm.integrations.opik.opik import OpikLogger
import litellm

# Wrap your LiteLLM client
opik_logger = OpikLogger()
litellm.callbacks = [opik_logger]
```

All LiteLLM calls made using the `litellm` client will now be logged to Opik.

</Tab>

<Tab value="Decorator (Python)" title="Decorator (Python)">

If you are using an LLM provider that Opik does not have an integration for, you can still log the LLM calls by using the `@track` decorator:

```python {pytest_codeblocks_skip=true}
from opik import track
import anthropic

@track
def call_llm(client, messages):
    return client.messages.create(messages=messages)

client = anthropic.Anthropic()

call_llm(client, [{"role": "user", "content": "Why is tracking and evaluation of LLMs important?"}])
```

The `@track` decorator will automatically log the input and output of the decorated function allowing you to track the user
messages and the LLM responses in Opik. If you want to log more than just the input and output, you can use the `update_current_span` function
as described in the [Traces / Logging Additional Data section](/tracing/log_traces#logging-additional-data).

</Tab>

<Tab value="JS / TS" title="JS / TS">

You can use the Opik client to log your LLM calls to Opik:

```js
import { Opik } from "opik";

// Create a new Opik client with your configuration
const client = new Opik({
  apiKey: "<your-api-key>",
  host: "https://www.comet.com/opik/api",
  projectName: "<your-project-name>",
  workspaceName: "<your-workspace-name>",
});

// Log a trace with an LLM span
const trace = client.trace({
  name: `Trace`,
  input: {
    prompt: `Hello!`,
  },
  output: {
    response: `Hello, world!`,
  },
});

const span = trace.span({
  name: `Span`,
  type: "llm",
  input: {
    prompt: `Hello, world!`,
  },
  output: {
    response: `Hello, world!`,
  },
});

// Flush the client to send all traces and spans
await client.flush();
```

</Tab>
</Tabs>

### Logging chains

It is common for LLM applications to use chains rather than just calling the LLM once. This is achieved by either using a framework
like [LangChain](/tracing/integrations/langchain), [LangGraph](/tracing/integrations/langgraph) or [LLamaIndex](/tracing/integrations/llama_index),
or by writing custom python code.

Opik makes it easy for you to log your chains no matter how you implement them:

<Tabs>
    <Tab value="Custom Python Code" title="Custom Python Code">

If you are not using any frameworks to build your chains, you can use the `@track` decorator to log your chains. When a
function is decorated with `@track`, the input and output of the function will be logged to Opik. This works well even for very
nested chains:

```python
from opik import track
from opik.integrations.openai import track_openai
from openai import OpenAI

# Wrap your OpenAI client
openai_client = OpenAI()
openai_client = track_openai(openai_client)

# Create your chain
@track
def llm_chain(input_text):
    context = retrieve_context(input_text)
    response = generate_response(input_text, context)

    return response

@track
def retrieve_context(input_text):
    # For the purpose of this example, we are just returning a hardcoded list of strings
    context =[
        "What specific information are you looking for?",
        "How can I assist you with your interests today?",
        "Are there any topics you'd like to explore or learn more about?",
    ]
    return context

@track
def generate_response(input_text, context):
    full_prompt = (
        f" If the user asks a question that is not specific, use the context to provide a relevant response.\n"
        f"Context: {', '.join(context)}\n"
        f"User: {input_text}\n"
        f"AI:"
    )

    response = openai_client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": full_prompt}]
    )
    return response.choices[0].message.content

llm_chain("Hello, how are you?")
```

While this code sample assumes that you are using OpenAI, the same principle applies if you are using any other LLM provider.

<Info>
  Your chains will now be logged to Opik and can be viewed in the Opik UI. To
  learn more about how you can customize the logged data, see the [Log
  Traces](/tracing/log_traces) guide.
</Info>

</Tab>
<Tab value="LangChain" title="LangChain">

If you are using LangChain to build your chains, you can use the `OpikTracer` to log your chains. The `OpikTracer` is a LangChain callback that will
log every step of the chain to Opik:

```python {pytest_codeblocks_skip=true}
from langchain_openai import OpenAI
from langchain.prompts import PromptTemplate
from opik.integrations.langchain import OpikTracer

# Initialize the tracer
opik_tracer = OpikTracer()

# Create the LLM Chain using LangChain
llm = OpenAI(temperature=0)

prompt_template = PromptTemplate(
    input_variables=["input"],
    template="Translate the following text to French: {input}"
)

# Use pipe operator to create LLM chain
llm_chain = prompt_template | llm

# Generate the translations
llm_chain.invoke({"input": "Hello, how are you?"}, callbacks=[opik_tracer])
```

<Info>
  Your chains will now be logged to Opik and can be viewed in the Opik UI. To
  learn more about how you can customize the logged data, see the [Log
  Traces](/tracing/log_traces) guide.
</Info>

</Tab>

<Tab value="LLamaIndex" title="LLamaIndex">

If you are using LLamaIndex you can set `opik` as a global callback to log all LLM calls:

```python {pytest_codeblocks_skip=true}
from llama_index.core import global_handler, set_global_handler

set_global_handler("opik")
opik_callback_handler = global_handler
```

Your LlamaIndex calls from that point forward will be logged to Opik. You can learn more about the LlamaIndex integration in the [LLamaIndex integration docs](/tracing/integrations/llama_index).

<Info>
  Your chains will now be logged to Opik and can be viewed in the Opik UI. To
  learn more about how you can customize the logged data, see the [Log
  Traces](/tracing/log_traces) guide.
</Info>

</Tab>

<Tab value="AI Vercel SDK (JS / TS)" title="AI Vercel SDK (JS / TS)">

If you are using the [Vercel AI SDK](https://sdk.vercel.ai/), you can integrate Opik using the `OpikExporter` which works through OpenTelemetry:

```bash
npm install opik ai @ai-sdk/openai @opentelemetry/sdk-node @opentelemetry/auto-instrumentations-node
```

Set up your environment variables:

```bash
export OPIK_API_KEY="<your-api-key>"
export OPIK_URL_OVERRIDE="https://www.comet.com/opik/api" # Cloud version
export OPIK_PROJECT_NAME="<your-project-name>"
export OPIK_WORKSPACE="<your-workspace>"
export OPENAI_API_KEY="<your-openai-api-key>"
```

Initialize the OpikExporter with your AI SDK:

```ts
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";
import { NodeSDK } from "@opentelemetry/sdk-node";
import { getNodeAutoInstrumentations } from "@opentelemetry/auto-instrumentations-node";
import { OpikExporter } from "opik/vercel";

// Set up OpenTelemetry with Opik
const sdk = new NodeSDK({
  traceExporter: new OpikExporter(),
  instrumentations: [getNodeAutoInstrumentations()],
});
sdk.start();

// Your AI SDK calls with telemetry enabled
const result = await generateText({
  model: openai("gpt-4o"),
  prompt: "What is love?",
  experimental_telemetry: { isEnabled: true },
});

console.log(result.text);
```

All AI SDK calls with `experimental_telemetry: { isEnabled: true }` will now be logged to Opik. You can learn more about the Vercel AI SDK integration in the [Vercel AI SDK integration docs](/tracing/integrations/vercel-ai-sdk).

</Tab>

</Tabs>

## Next steps

Now that you have logged your first LLM calls and chains to Opik, why not check out:

1. [Opik's evaluation metrics](/evaluation/metrics/overview): Opik provides a suite of evaluation metrics (Hallucination, Answer Relevance, Context Recall, etc.) that you can use to score your LLM responses.
2. [Opik Experiments](/evaluation/concepts): Opik allows you to automated the evaluation process of your LLM application so that you no longer need to manually review every LLM response.

## How can I diagnose issues with Opik?

If you are experiencing any problems using Opik, such as receiving 400 or 500 errors from the backend, or being unable to connect at all, we recommend running the following command in your terminal:

```bash
opik healthcheck
```

This command will analyze your configuration and backend connectivity, providing useful insights into potential issues.

<Frame>
  <img src="../img/healthcheck.png" />
</Frame>

Reviewing these sections can help pinpoint the source of the problem and suggest possible resolutions.
