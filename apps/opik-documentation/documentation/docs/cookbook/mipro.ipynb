{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Agent Optimization with MIPRO\n",
    "\n",
    "MIPRO (Multiprompt Instruction PRoposal Optimizer) is an optimizer you can use to create the best prompts from your task. You can read the original paper [here](https://arxiv.org/abs/2406.11695).\n",
    "\n",
    "This optimizer was popularized by the [DSPy library](https://dspy.ai/) that provided the first and most commonly used implementation.\n",
    "\n",
    "In the notebook below, we will re-implement the algorithm from scratch to get a better understanding of how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization strategy\n",
    "\n",
    "The MIPRO algorithm was developed to optimize the multi-step LLM applications when you don't have labels for each step but rather global label for the entire task. This makes it a great algorithm to automatically optimize agents !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the agent\n",
    "\n",
    "To test the optimization algorithm, we are going to start by creating a very simple agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (1.66.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (0.8.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py312_llm_eval/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "%pip install openai\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sentiment analysis tool\n",
    "sentiment_prompt = \"Analyze the sentiment of this text and classify it as POSITIVE, NEGATIVE. Don't return any other words.\"\n",
    "\n",
    "\n",
    "class SentimentAnalysisTool:\n",
    "    def __init__(self, name: str = \"sentiment\", prompt: str = \"\"):\n",
    "        self.name = name\n",
    "        self.openai_client = OpenAI()\n",
    "        self.sentiment_prompt = prompt\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        completion = self.openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.sentiment_prompt},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ],\n",
    "            model=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt = \"Summarize the text provided below.\"\n",
    "\n",
    "\n",
    "class SummarizeTool:\n",
    "    def __init__(self, name: str = \"summarization\", prompt: str = \"\"):\n",
    "        self.name = name\n",
    "        self.openai_client = OpenAI()\n",
    "        self.summarization_prompt = prompt\n",
    "\n",
    "    def __call__(self, text: str) -> str:\n",
    "        completion = self.openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.summarization_prompt},\n",
    "                {\"role\": \"user\", \"content\": text},\n",
    "            ],\n",
    "            model=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "orchestrator_prompt = \"\"\"\n",
    "You are an LLM agent that can both summarize and calculate.\n",
    "\n",
    "You have access to the tools:\n",
    "- sentiment: Return a sentiment analysis score\n",
    "- summarization: Summarize some text\n",
    "\n",
    "To call a tool, return the text:\n",
    "Action: <tool_name>: <tool_parameter>\n",
    "\n",
    "Return STOP when finished\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Orchestrator:\n",
    "    def __init__(self, name: str = \"orchestror\", prompt: str = \"\"):\n",
    "        self.name = name\n",
    "        self.openai_client = OpenAI()\n",
    "        self.orchestrator_prompt = prompt\n",
    "\n",
    "    def __call__(self, messages: list) -> str:\n",
    "        completion = self.openai_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": self.orchestrator_prompt},\n",
    "            ]\n",
    "            + messages,\n",
    "            model=\"gpt-4o\",\n",
    "        )\n",
    "\n",
    "        return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Questions chatbot ------\n",
      "Summarize the following customer reviews and analyze their overall sentiment:\n",
      "\n",
      "The laptop is fantastic! The battery life lasts all day, and the display is beautiful. However, the keyboard feels a bit flimsy.\n",
      "\n",
      "\n",
      "Response: sentiment: POSITIVE\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import re\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(\n",
    "        self, client: OpenAI, orchestrator: Orchestrator, tools: list = []\n",
    "    ) -> None:\n",
    "        self.client = client\n",
    "        self.tools = tools\n",
    "        self.orchestrator = orchestrator\n",
    "\n",
    "    def call_agent(self, message: str = \"\", max_iterations: int = 10) -> str:\n",
    "        i = 0\n",
    "        module_calls = []\n",
    "        messages = [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "        next_prompt = message\n",
    "        while i < max_iterations:\n",
    "            i += 1\n",
    "            orchestrator_response = self.orchestrator(messages)\n",
    "\n",
    "            module_calls.append(\n",
    "                {\n",
    "                    \"module_name\": \"orchestrator\",\n",
    "                    \"input\": messages,\n",
    "                    \"output\": orchestrator_response,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if \"STOP\" in orchestrator_response:\n",
    "                break\n",
    "            elif \"Action\" in orchestrator_response:\n",
    "                action = re.findall(\n",
    "                    r\"Action: ([a-z_]+): (.+)\", orchestrator_response, re.IGNORECASE\n",
    "                )\n",
    "                chosen_tool = action[0][0]\n",
    "                arg = action[0][1]\n",
    "\n",
    "                tool_names = [x.name for x in self.tools]\n",
    "                if chosen_tool in tool_names:\n",
    "                    tool = [x for x in self.tools if x.name == chosen_tool][0]\n",
    "\n",
    "                    result_tool = tool(arg)\n",
    "                    next_prompt = f\"{chosen_tool}: {result_tool}\"\n",
    "\n",
    "                    messages += [{\"role\": \"assistant\", \"content\": next_prompt}]\n",
    "\n",
    "                    module_calls.append(\n",
    "                        {\n",
    "                            \"module_name\": chosen_tool,\n",
    "                            \"input\": arg,\n",
    "                            \"output\": result_tool,\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find tool {chosen_tool}\")\n",
    "\n",
    "        return next_prompt, module_calls\n",
    "\n",
    "\n",
    "agent = Agent(\n",
    "    client=OpenAI(),\n",
    "    orchestrator=Orchestrator(prompt=orchestrator_prompt),\n",
    "    tools=[\n",
    "        SentimentAnalysisTool(prompt=sentiment_prompt),\n",
    "        SummarizeTool(prompt=summarize_prompt),\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(\"----- Questions chatbot ------\")\n",
    "example = \"\"\"Summarize the following customer reviews and analyze their overall sentiment:\n",
    "\n",
    "The laptop is fantastic! The battery life lasts all day, and the display is beautiful. However, the keyboard feels a bit flimsy.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "print(example + \"\\n\")\n",
    "res, module_calls = agent.call_agent(example)\n",
    "print(\"Response: \" + res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MIPRO algorithm relies on the following:\n",
    "1. A dataset: This is a set of labelled data with positive and negative samples\n",
    "2. A metric: A metric to optimize over\n",
    "3. A task: The agent to optimize\n",
    "\n",
    "For the purposes of this investigation, we will define these as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    {\n",
    "        \"input\": \"\"\"Summarize the following customer reviews and analyze their overall sentiment:\n",
    "        I just love waiting in long lines at the DMV... said no one ever.\"\"\",\n",
    "        \"expected_output\": \"Result: NEGATIVE\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Summarize the following customer reviews and analyze their overall sentiment:\n",
    "        The food was absolutely terrible, but at least the waiter was friendly.\"\"\",\n",
    "        \"expected_output\": \"Result: NEGATIVE\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Summarize the following customer reviews and analyze their overall sentiment:\n",
    "        This movie was so bad that itâ€™s actually hilarious. I had a great time watching it!\"\"\",\n",
    "        \"expected_output\": \"Result: POSITIVE\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Summarize the following customer reviews and analyze their overall sentiment:\n",
    "        Wow, I can't believe how amazing this service is! It only took them two hours to get my order wrong.\"\"\",\n",
    "        \"expected_output\": \"Result: NEGATIVE\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"\"\"Summarize the following customer reviews and analyze their overall sentiment:\n",
    "        The software crashes every time I open it. But hey, at least the icon looks nice!\"\"\",\n",
    "        \"expected_output\": \"Result: NEGATIVE\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def equals_metric(output, expected_output):\n",
    "    if expected_output.lower() == output.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment: NEGATIVE\n",
      "0\n",
      "---\n",
      "sentiment: NEGATIVE\n",
      "0\n",
      "---\n",
      "sentiment: POSITIVE\n",
      "0\n",
      "---\n",
      "sentiment: NEGATIVE\n",
      "0\n",
      "---\n",
      "sentiment: NEGATIVE\n",
      "0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for i in dataset:\n",
    "    res, module_calls = agent.call_agent(i[\"input\"])\n",
    "    print(res)\n",
    "    print(equals_metric(res, i[\"expected_output\"]))\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MIPRO algorithm\n",
    "\n",
    "The MIPRO algorithm has 3 phases:\n",
    "1. Initialize\n",
    "\n",
    "### Initialization\n",
    "\n",
    "For the initialization step, we will start by bootstraping a set of N few-shot examples for each module. In the agent defined above, we have three different modules:\n",
    "\n",
    "1. Orchestrator\n",
    "2. Sentiment analysis tool\n",
    "3. Summarization tool\n",
    "\n",
    "For this we are going to use the Bootstrap Demonstration and Grounding.\n",
    "\n",
    "#### Bootstrap demonstrations\n",
    "\n",
    "The concept of bootstrap demonstrations is remarkably simple. The idea is that if the agent as a whole returns the correct output, then we can expect that the input / output pair for each module is correct. This allows us to create a dataset of \"labelled\" data for each module without needing to specify a dataset for each module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_demonstrations(agent, dataset, metric):\n",
    "    positive_samples = []\n",
    "    for item in dataset:\n",
    "        output, module_calls = agent.call_agent(item[\"input\"])\n",
    "\n",
    "        score = metric(output=output, expected_output=item[\"expected_output\"])\n",
    "\n",
    "        if score >= 1:\n",
    "            positive_samples += module_calls\n",
    "\n",
    "    return positive_samples\n",
    "\n",
    "\n",
    "positive_samples = bootstrap_demonstrations(agent, dataset, equals_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'module_name': 'orchestrator',\n",
       "  'input': 'Summarize the following customer reviews and analyze their overall sentiment:\\n        I just love waiting in long lines at the DMV... said no one ever.',\n",
       "  'output': 'Action: summarization: I just love waiting in long lines at the DMV... said no one ever.'},\n",
       " {'module_name': 'summarization',\n",
       "  'input': 'I just love waiting in long lines at the DMV... said no one ever.',\n",
       "  'output': 'This statement humorously expresses the common frustration people feel about waiting in long lines at the DMV. Nobody actually enjoys the experience.'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: This statement humorously expresses the common frustration people feel about waiting in long lines at the DMV. Nobody actually enjoys the experience.',\n",
       "  'output': 'Action: sentiment: This statement humorously expresses the common frustration people feel about waiting in long lines at the DMV. Nobody actually enjoys the experience.'},\n",
       " {'module_name': 'sentiment',\n",
       "  'input': 'This statement humorously expresses the common frustration people feel about waiting in long lines at the DMV. Nobody actually enjoys the experience.',\n",
       "  'output': 'NEGATIVE'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': \"It appears you're referring to a sentiment analysis result with a negative sentiment. If you need further assistance or a summary on a particular piece of text, please provide the content or details.\"},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'It looks like you are seeking to analyze sentiment. Could you provide the text or data you would like me to analyze for sentiment?'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'Action: sentiment: NEGATIVE'},\n",
       " {'module_name': 'sentiment', 'input': 'NEGATIVE', 'output': 'NEGATIVE'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'Could you please provide the text or context for which you want a sentiment analysis? Then I can determine why the result might be negative or provide you with more information.'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'It seems like there is a sentiment analysis result that shows a negative sentiment. If you have any text you would like me to analyze or if you need a summary of some content, let me know how I can assist further!'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'It seems like you are referring to an analysis or a sentiment result that is negative. Would you like me to perform a sentiment analysis or summarize some text for more context on this result?'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': \"It seems you want to analyze something for a negative sentiment. Do you have specific text or content you'd like me to analyze?\"},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'It seems like you are looking for a sentiment analysis that resulted in a negative score. Would you like more context or a summary of this?'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Summarize the following customer reviews and analyze their overall sentiment:\\n        The software crashes every time I open it. But hey, at least the icon looks nice!',\n",
       "  'output': 'Action: summarization: The software repeatedly crashes upon opening, with only a positive remark about the appearance of its icon.\\n\\nAction: sentiment: The software crashes every time I open it. But hey, at least the icon looks nice!'},\n",
       " {'module_name': 'summarization',\n",
       "  'input': 'The software repeatedly crashes upon opening, with only a positive remark about the appearance of its icon.',\n",
       "  'output': 'The software consistently crashes when opened, with the only positive feedback being the appearance of its icon.'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: The software consistently crashes when opened, with the only positive feedback being the appearance of its icon.',\n",
       "  'output': 'Action: sentiment: The software consistently crashes when opened, with the only positive feedback being the appearance of its icon.'},\n",
       " {'module_name': 'sentiment',\n",
       "  'input': 'The software consistently crashes when opened, with the only positive feedback being the appearance of its icon.',\n",
       "  'output': 'NEGATIVE'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': \"It seems like you're referring to a sentiment result. If you have a specific text or content that resulted in this sentiment score, I can help analyze or summarize it further. Please provide the text or let me know how I can assist you.\"},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'Thank you for sharing the result. If you would like me to perform a task, such as conducting a sentiment analysis, summarizing a text, or need help with something specific, please let me know!'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'It seems you are referring to the sentiment result of a particular text as \"NEGATIVE\". If you have any specific text you\\'d like me to analyze or if there\\'s anything else you\\'d like to know, please let me know!'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'This text seems to indicate a sentiment conclusion without providing context or reason. To provide more clarity, I can analyze the sentiment of a specific text or summarize information. Please provide text for analysis or summarization.'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'You provided the result \"NEGATIVE.\" Would you like me to perform a sentiment analysis on a particular text to confirm this result, or would you like a summary of a document? Please provide the text or specify your request.'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'You\\'ve shared a result labeled as \"NEGATIVE.\" If you want me to perform an action or need further information related to this result, please let me know how I can assist you!'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: NEGATIVE',\n",
       "  'output': 'Action: sentiment: NEGATIVE'},\n",
       " {'module_name': 'sentiment', 'input': 'NEGATIVE', 'output': 'POSITIVE'},\n",
       " {'module_name': 'orchestrator',\n",
       "  'input': 'Result: POSITIVE',\n",
       "  'output': 'Action: sentiment: POSITIVE'},\n",
       " {'module_name': 'sentiment', 'input': 'POSITIVE', 'output': 'NEGATIVE'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grounding\n",
    "\n",
    "In order to propose a better prompt template for each module, we need the LLM to create some instruction set based on what each module does specifically. While the authors created a zero-shot LLM program for this, we are going to take a simpler approach. We are going to craft a prompt that will take the code of each module as well as the agent as a whole and return a set of instructions on how to improve each module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312_llm_eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
