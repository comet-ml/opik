---
sidebar_label: Evaluate a prompt
description: Step by step guide on how to evaluate LLM prompts
pytest_codeblocks_execute_previous: true
---

# Evaluate a prompt

You can evaluate a prompt by running the `evaluate_prompt` function. This function takes:

1. A dataset: A list of samples to evaluate the prompt on
2. A prompt: List of messages that wil be evaluated
3. A model: The model to use for evaluation
4. Scoring metrics: A list of metrics to evaluate the output on
