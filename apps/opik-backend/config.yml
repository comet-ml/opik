---
# https://www.dropwizard.io/en/stable/manual/configuration.html#logging
logging:
  #  Default: INFO
  #  Description: Logback logging level
  level: ${GENERAL_LOG_LEVEL:-INFO}
  #  Default: com.comet: INFO
  #  Description: Individual logger configuration
  loggers:
    com.comet: ${OPIK_LOG_LEVEL:-INFO}
    # Default: INFO
    # Description: Set to DEBUG to see complete assembled LLM streaming responses (application-level logging)
    com.comet.opik.infrastructure.llm: ${LLM_PROXY_REQUEST_LOG_LEVEL:-INFO}

# Description: state database configuration
# https://www.dropwizard.io/en/stable/manual/configuration.html#database
database:
  #  Default: jdbc:mysql://localhost:3306/opik?createDatabaseIfNotExist=true&rewriteBatchedStatements=true
  #  Description: The URL of the server
  url: ${STATE_DB_PROTOCOL:-jdbc:mysql://}${STATE_DB_URL:-localhost:3306/opik?createDatabaseIfNotExist=true&rewriteBatchedStatements=true}
  #  Default: opik
  #  Description: The username used to connect to the server
  user: ${STATE_DB_USER:-opik}
  #  Default: opik
  #  Description: The password used to connect to the server
  password: ${STATE_DB_PASS:-opik}
  #  Default: com.mysql.cj.jdbc.Driver
  #  Description: The fully qualified class name of the JDBC driver class.
  #   Only required if there were no JDBC drivers registered in META-INF/services/java.sql.Driver.
  driverClass: ${STATE_DB_DRIVER_CLASS:-com.mysql.cj.jdbc.Driver}
  #  Default: Empty
  #  Description: Any additional JDBC driver parameters
  properties:
    wrapperPlugins: ${STATE_DB_PLUGINS:-''}
    useSSL: ${MYSQL_USE_SSL:-''}
    requireSSL: ${MYSQL_REQUIRE_SSL:-''}
    verifyServerCertificate: ${MYSQL_VERIFY_SSL:-''}
    trustCertificateKeyStoreUrl: ${MYSQL_TRUSTSTORE_URL:-''}
    trustCertificateKeyStorePassword: ${MYSQL_TRUSTSTORE_PASSWORD:-''}
    trustCertificateKeyStoreType: ${MYSQL_TRUSTSTORE_TYPE:-''}
    clientCertificateKeyStoreUrl: ${MYSQL_CLIENTSTORE_URL:-''}
    clientCertificateKeyStorePassword: ${MYSQL_CLIENTSTORE_PASSWORD:-''}
    clientCertificateKeyStoreType: ${MYSQL_CLIENTSTORE_TYPE:-''}

# Description: analytics database configuration for migrations connection
databaseAnalyticsMigrations:
  #  Default: jdbc:clickhouse://localhost:8123/opik
  #  Description: The URL of the server
  url: ${ANALYTICS_DB_MIGRATIONS_URL:-jdbc:clickhouse://localhost:8123/opik}
  #  Default: opik
  #  Description: The username used to connect to the server
  user: ${ANALYTICS_DB_MIGRATIONS_USER:-opik}
  #  Default: opik
  #  Description: The password used to connect to the server
  password: ${ANALYTICS_DB_MIGRATIONS_PASS:-opik}
  #  Default: ru.yandex.clickhouse.ClickHouseDriver
  #  Description: The fully qualified class name of the JDBC driver class.
  #   Community support only. Requires an old driver for migrations to work
  driverClass: ru.yandex.clickhouse.ClickHouseDriver

# Description: analytics database configuration for service connection
databaseAnalytics:
  #  Default: http
  #  Description: The protocol used to connect to the server
  protocol: ${ANALYTICS_DB_PROTOCOL:-HTTP}
  #  Default: localhost
  #  Description: The host used to connect to the server
  host: ${ANALYTICS_DB_HOST:-localhost}
  #  Default: 8123
  #  Description: The port used to connect to the server
  port: ${ANALYTICS_DB_PORT:-8123}
  #  Default: opik
  #  Description: The username used to connect to the server
  username: ${ANALYTICS_DB_USERNAME:-opik}
  #  Default: opik
  #  Description: The password used to connect to the server
  password: ${ANALYTICS_DB_PASS:-opik}
  #  Default: opik
  #  Description: The database name
  databaseName: ${ANALYTICS_DB_DATABASE_NAME:-opik}
  #  Default:
  #   - health_check_interval=2000
  #   - compress=1
  #   - auto_discovery=true
  #   - failover=3
  #   - custom_http_params=max_query_size=100000000
  #  Description: query parameters that will be added to the connection string
  queryParameters: ${ANALYTICS_DB_QUERY_PARAMETERS:-health_check_interval=2000&compress=1&auto_discovery=true&failover=3&custom_http_params=max_query_size=100000000,async_insert_busy_timeout_max_ms=250,async_insert_busy_timeout_min_ms=100,async_insert=1,wait_for_async_insert=1,async_insert_use_adaptive_busy_timeout=1,async_insert_deduplicate=1}

# https://www.dropwizard.io/en/stable/manual/configuration.html#health
health:
  # Default: ["/health-check"]
  # Description: URLs to expose the appâ€™s health check on.
  healthCheckUrlPaths: ["/health-check"]
  # Options around a particular health check which is registered in an Application
  # https://www.dropwizard.io/en/stable/manual/configuration.html#man-configuration-health-checks
  healthChecks:
    - name: deadlocks
      critical: true
      type: alive
    - name: db
      critical: true
      type: ready
    - name: redis
      critical: true
      type: ready
    - name: clickhouse
      critical: true
      type: ready
    - name: mysql
      critical: true
      type: ready

# Configuration for distributed locking using redis
distributedLock:
  # Default: 500
  # Description: Lease time in milliseconds
  lockTimeoutMS: ${DISTRIBUTED_LOCK_TIME_OUT:-500}
  # Default: 5
  # Description: This value has to be considerably higher than the lockTimeoutMS value, as it has to guarantee that the
  # last thread to join the queue to acquire the lock will have enough time to execute the action. Then, the lock will
  # be deleted from redis after the specified period of time.
  # This is needed as Redisson by default doesn't delete the lock from redis after the lease time expires, it just
  # releases the lock. The expiration time will be reset every time the lock is acquired.
  ttlInSeconds: ${DISTRIBUTED_LOCK_TTL_IN_SEC:-5}

# Redis configuration
redis:
  # Default: redis://:opik@localhost:6379/0
  # Description: single node redis's URL
  singleNodeUrl: ${REDIS_URL:-redis://:opik@localhost:6379/0}
  # AWS IAM authentication configuration (optional)
  # When enabled, takes precedence over singleNodeUrl authentication
  awsIamAuth:
    # Default: false
    # Description: Enable AWS IAM authentication for ElasticCache Redis
    enabled: ${OPIK_REDIS_AWS_IAM_AUTH_ENABLED:-false}
    # Description: AWS User ID for IAM authentication
    awsUserId: ${OPIK_REDIS_AWS_USER_ID:-''}
    # Default: us-east-1
    # Description: AWS region where ElastiCache is deployed
    awsRegion: ${AWS_REGION:-us-east-1}
    # Default: opik-redis
    # Description: ElasticCache resource name (replication group/cluster/serverless name)
    awsResourceName: ${OPIK_REDIS_AWS_RESOURCE_NAME:-''}
    # Default: 13m
    # Description: How often to refresh the AWS authentication token
    tokenCacheRefreshAfter: ${OPIK_REDIS_AWS_TOKEN_CACHE_REFRESH_AFTER:-13m}
    # Default: 14m
    # Description: How long the AWS authentication token will be cached locally
    tokenCacheExpireAfter: ${OPIK_REDIS_AWS_TOKEN_CACHE_EXPIRE_AFTER:-14m}
    # Default: 15m
    # Description: How long the AWS authentication token is valid
    tokenExpiryDuration: ${OPIK_REDIS_AWS_TOKEN_EXPIRY_DURATION:-15m}

openTelemetry:
  # Default: 3h
  # Description: how long it takes to expire non-used keys
  ttl: ${OTEL_TTL_INTERVAL:-3h}

# Authentication configuration. This is not enabled by default for open source installations.
authentication:
  # Default: false
  # Description: Whether or not to enable authentication
  enabled: ${AUTH_ENABLED:-false}
  # Default: 5
  # Description: API key resolution cache TTL (seconds). Setting this value to 0 means no caching.
  apiKeyResolutionCacheTTLInSec: ${AUTH_API_KEY_RESOLUTION_CACHE_TTL_IN_SEC:-5}
  # Default: http://react-svc:8080
  # Description: Configures base url for React service, used for user management and authentication
  reactService:
    url: ${REACT_SERVICE_URL:-http://react-svc:8080}

# https://www.dropwizard.io/en/stable/manual/configuration.html#servers
server:
  # Default: false
  # Description: Whether to enable virtual threads for Jetty's thread pool.
  enableVirtualThreads: ${ENABLE_VIRTUAL_THREADS:-false}
  # https://www.dropwizard.io/en/stable/manual/configuration.html#gzip
  gzip:
    # Default: true
    # Description: If true, all requests with gzip in the Accept-Encoding header will have their response entities
    # compressed and requests with gzip in the Content-Encoding header will have their request entities decompressed.
    enabled: true
  # HTTP connector configuration for handling large request headers
  # Default: 16KB (16000 bytes) - matches ALB/Nginx request line limit
  # Description: Maximum size of request headers to prevent 431 errors from long URLs with many filters
  applicationConnectors:
    - type: http
      # Default: 8080
      # Description: The port on which the application HTTP connector listens
      port: ${SERVER_APPLICATION_PORT:-8080}
      maxRequestHeaderSize: ${SERVER_MAX_REQUEST_HEADER_SIZE:-16KB}
      # Default: 1m
      # Description: The maximum idle time for a connection, after which it will be closed.
      idleTimeout: ${SERVER_IDLE_TIMEOUT:-1m}
  adminConnectors:
    - type: http
      # Default: 8081
      # Description: The port on which the admin HTTP connector listens
      port: ${SERVER_ADMIN_PORT:-8081}

# Jackson JSON processing configuration
# Controls limits for JSON deserialization to prevent memory exhaustion
jacksonConfig:
  # Default: 104857600 (100MB)
  # Description: Maximum size for individual string values during JSON deserialization.
  #   A JSON with 3 fields up to this size will work, but a single larger string will fail.
  #   This configuration is used by both HTTP layer and internal JSON processing for consistency,
  #   preventing memory exhaustion from extremely large base64-encoded attachments before they can be stripped
  maxStringLength: ${JACKSON_MAX_STRING_LENGTH:-104857600}

# Configuration for batch operations
batchOperations:
  datasets:
    # Default: 5000
    # Description: The maximal number of ids to be used for IN clause. Find requests with a larger number of ids will
    # involve the use of temp tables for querying
    maxExperimentInClauseSize: ${BATCH_OPERATIONS_MAX_EXPERIMENT_IN_CLAUSE_SIZE:-5000}
    # Default: 1000
    # Description: The batch size for processing CSV files. Dataset items are saved to the database in batches of this size.
    csvBatchSize: ${BATCH_OPERATIONS_CSV_BATCH_SIZE:-1000}

# Configuration for experiment aggregates population
experimentAggregates:
  # Default: 1000
  # Description: Number of experiment items processed per batch when populating experiment_item_aggregates table. Higher values improve performance but increase memory usage. Recommended range: 100-5000.
  batchSize: ${EXPERIMENT_AGGREGATES_BATCH_SIZE:-1000}

# Configuration for rate limit. This is not enabled by default for open source installations.
# If enabled, rate limit is applied to creation and update of various entities including traces, spans, projects,
# prompts, feedback definitions, experiments, datasets and dataset items
rateLimit:
  # Default: false
  # Description: Whether or not rate limit is enabled
  enabled: ${RATE_LIMIT_ENABLED:-false}
  # This uses as a fallback rate limit configuration in case an entity specific configuration doesn't exist
  generalLimit:
    # Default: 10000
    # Description: how many events are allowed in the specified time bucket
    limit: ${RATE_LIMIT_GENERAL_EVENTS_LIMIT:-10000}
    # Default: 60
    # Description: Time bucket size in seconds
    durationInSeconds: ${RATE_LIMIT_GENERAL_EVENTS_DURATION_IN_SEC:-60}
    # Description: Header name to use for rate limiting
    headerName: User
    # Description: User facing bucket name
    userFacingBucketName: general_events
    # Description: Rate limit error message
    errorMessage: "You have exceeded the general rate limit for this user. Please try again later."

  workspaceLimit:
    # Default: 5000
    # Description: how many events per workspace are allowed in the specified time bucket
    limit: ${RATE_LIMIT_WORKSPACE_EVENTS_LIMIT:-5000}
    # Default: 60
    # Description: Time bucket size in seconds
    durationInSeconds: ${RATE_LIMIT_WORKSPACE_EVENTS_DURATION_IN_SEC:-60}
    # Description: Header name to use for rate limiting
    headerName: Workspace
    # Description: User facing bucket name
    userFacingBucketName: workspace_events
    # Description: Rate limit error message
    errorMessage: "You have exceeded the rate limit for this user in this workspace. Please try again later."

  customLimits:
    getSpanById:
      # Default: 250
      # Description: how many events are allowed in the specified time bucket
      limit: ${RATE_LIMIT_GET_SPANS_BY_ID_EVENTS_PER_WORKSPACE_LIMIT:-250}
      # Default: 60
      # Description: Time bucket size in seconds
      durationInSeconds: ${RATE_LIMIT_WORKSPACE_EVENTS_DURATION_IN_SEC:-60}
      # Description: Header name to use for rate limiting
      headerName: Get-Span-Id
      # Description: User facing bucket name
      userFacingBucketName: get_span_by_id
      # Description: Rate limit error message
      errorMessage: "You have exceeded the rate limit for this operation. Please try again later."
    singleTracingOps:
      # Default: 800
      # Description: how many events are allowed in the specified time bucket
      limit: ${RATE_LIMIT_SINGLE_TRACING_OPS_EVENTS_PER_WORKSPACE_LIMIT:-800}
      # Default: 60
      # Description: Time bucket size in seconds
      durationInSeconds: ${RATE_LIMIT_WORKSPACE_EVENTS_DURATION_IN_SEC:-60}
      # Description: Header name to use for rate limiting
      headerName: Single-Tracing-Ops
      # Description: User facing bucket name
      userFacingBucketName: single_tracing_ops
      # Description: Rate limit error message
      errorMessage: "You have exceeded the rate limit for single tracing operations. Please try again later."

# Configuration for usage limit. This is not enabled by default for open source installations.
# In order to support that, the remote authentication server must contain a `quotas` object in its authentication
# response.
usageLimit:
  # Description: The error to be displayed when submitting entities to a workspace where the usage limit is exceeded
  errorMessage: ${USAGE_LIMIT_ERROR_MESSAGE:-You have exceeded the usage limit for this operation.}

# Configuration for anonymous usage reporting
usageReport:
  # Default: true
  # Description: Whether or not to send anonymous usage reports
  enabled: ${OPIK_USAGE_REPORT_ENABLED:-true}
  # Default: https://stats.comet.com/notify/event/
  # Description: URL to send the anonymous usage reports to
  url: ${OPIK_USAGE_REPORT_URL:-https://stats.comet.com/notify/event/}
  # Description: Configuration for anonymous ID used to identify the installation
  # Default: empty
  anonymousId: ${OPIK_ANONYMOUS_ID:-}

# Configuration for application metadata
metadata:
  # Default: latest
  # Description: The application version
  version: ${OPIK_VERSION:-latest}

# CORS related configuration
cors:
  # Default: false
  # Description: Whether or not to allow cross site scripting
  enabled: ${CORS:-false}

# Encryption related configuration
encryption:
  # Default: GiTHubiLoVeYouAA
  # Description: Encryption key to use when storing sensitive information
  key: ${OPIK_ENCRYPTION_KEY:-'GiTHubiLoVeYouAA'}

# Configuration for Online Scoring
onlineScoring:
  # Default: 500 ms
  # Description: How often Online Scoring will check Redis for new messages (in milliseconds)
  # This is the global default value, can be overridden per stream
  poolingInterval: ${REDIS_SCORING_CONSUMER_POOL_INTERVAL:-500ms}
  # Default: 5s
  # Description: Timeout for blocking read operations on Redis streams (long polling duration)
  # This is the global default value, can be overridden per stream
  longPollingDuration: ${REDIS_SCORING_LONG_POLLING_DURATION:-5s}
  # Default: online_scoring
  # Description: A consumer group name so multiple instances can share the stream load
  consumerGroupName: ${REDIS_SCORING_CONSUMER_GROUP_NAME:-'online_scoring'}
  # Default: 10
  # Description: Maximum number of messages returned within a Redis Stream get
  # This is the global default value, can be overridden per stream
  consumerBatchSize: ${REDIS_SCORING_CONSUMER_BATCH_SIZE:-10}
  # Default: 10
  # Description: Claim pending messages every N polling intervals (global default)
  claimIntervalRatio: ${REDIS_SCORING_CLAIM_INTERVAL_RATIO:-10}
  # Default: 10m
  # Description: Time before message considered orphaned and eligible for claiming (global default)
  pendingMessageDuration: ${REDIS_SCORING_PENDING_MESSAGE_DURATION:-10m}
  # Default: 3
  # Description: Maximum number of retry attempts for failed messages (1-10, global default)
  maxRetries: ${REDIS_SCORING_MAX_RETRIES:-3}
  ## scorer: options from AutomationRuleEvaluatorType
  ## streamName: the name of the stream in redis
  ## codec: 'json' when there are non-java consumers, 'java' for java consumers only
  ## poolingInterval: (optional, but highly recommended) per stream pooling interval, overrides global value
  ## longPollingDuration: (optional, but highly recommended) per stream long polling duration, overrides global value
  ## consumerBatchSize: (optional, but highly recommended) per stream batch size, overrides global value
  ## claimIntervalRatio: (optional) per stream claim interval ratio, overrides global value
  ## pendingMessageDuration: (optional) per stream pending message duration, overrides global value
  ## maxRetries: (optional) per stream max retries, overrides global value
  streams:
    - scorer: llm_as_judge
      streamName: stream_scoring_llm_as_judge
      codec: java
      # Optional per stream configuration (falls back to global if not specified)
      poolingInterval: ${REDIS_SCORING_LLM_AS_JUDGE_CONSUMER_POOL_INTERVAL:-}
      longPollingDuration: ${REDIS_SCORING_LLM_AS_JUDGE_LONG_POLLING_DURATION:-}
      consumerBatchSize: ${REDIS_SCORING_LLM_AS_JUDGE_CONSUMER_BATCH_SIZE:-}
      claimIntervalRatio: ${REDIS_SCORING_LLM_AS_JUDGE_CLAIM_INTERVAL_RATIO:-}
      pendingMessageDuration: ${REDIS_SCORING_LLM_AS_JUDGE_PENDING_MESSAGE_DURATION:-}
      maxRetries: ${REDIS_SCORING_LLM_AS_JUDGE_MAX_RETRIES:-}
    - scorer: user_defined_metric_python
      streamName: stream_scoring_user_defined_metric_python
      codec: java
      # Optional per stream configuration (falls back to global if not specified)
      poolingInterval: ${REDIS_SCORING_USER_DEFINED_METRIC_PYTHON_CONSUMER_POOL_INTERVAL:-}
      longPollingDuration: ${REDIS_SCORING_USER_DEFINED_METRIC_PYTHON_LONG_POLLING_DURATION:-}
      consumerBatchSize: ${REDIS_SCORING_USER_DEFINED_METRIC_PYTHON_CONSUMER_BATCH_SIZE:-}
      claimIntervalRatio: ${REDIS_SCORING_USER_DEFINED_METRIC_PYTHON_CLAIM_INTERVAL_RATIO:-}
      pendingMessageDuration: ${REDIS_SCORING_USER_DEFINED_METRIC_PYTHON_PENDING_MESSAGE_DURATION:-}
      maxRetries: ${REDIS_SCORING_USER_DEFINED_METRIC_PYTHON_MAX_RETRIES:-}
    - scorer: trace_thread_llm_as_judge
      streamName: stream_scoring_trace_thread_llm_as_judge
      codec: java
      # Optional per stream configuration (falls back to global if not specified)
      poolingInterval: ${REDIS_SCORING_TRACE_THREAD_LLM_AS_JUDGE_CONSUMER_POOL_INTERVAL:-}
      longPollingDuration: ${REDIS_SCORING_TRACE_THREAD_LLM_AS_JUDGE_LONG_POLLING_DURATION:-}
      consumerBatchSize: ${REDIS_SCORING_TRACE_THREAD_LLM_AS_JUDGE_CONSUMER_BATCH_SIZE:-}
      claimIntervalRatio: ${REDIS_SCORING_TRACE_THREAD_LLM_AS_JUDGE_CLAIM_INTERVAL_RATIO:-}
      pendingMessageDuration: ${REDIS_SCORING_TRACE_THREAD_LLM_AS_JUDGE_PENDING_MESSAGE_DURATION:-}
      maxRetries: ${REDIS_SCORING_TRACE_THREAD_LLM_AS_JUDGE_MAX_RETRIES:-}
    - scorer: trace_thread_user_defined_metric_python
      streamName: stream_scoring_trace_thread_user_defined_metric_python
      codec: java
      # Optional per stream configuration (falls back to global if not specified)
      poolingInterval: ${REDIS_SCORING_TRACE_THREAD_USER_DEFINED_METRIC_PYTHON_CONSUMER_POOL_INTERVAL:-}
      longPollingDuration: ${REDIS_SCORING_TRACE_THREAD_USER_DEFINED_METRIC_PYTHON_LONG_POLLING_DURATION:-}
      consumerBatchSize: ${REDIS_SCORING_TRACE_THREAD_USER_DEFINED_METRIC_PYTHON_CONSUMER_BATCH_SIZE:-}
      claimIntervalRatio: ${REDIS_SCORING_TRACE_THREAD_USER_DEFINED_METRIC_PYTHON_CLAIM_INTERVAL_RATIO:-}
      pendingMessageDuration: ${REDIS_SCORING_TRACE_THREAD_USER_DEFINED_METRIC_PYTHON_PENDING_MESSAGE_DURATION:-}
      maxRetries: ${REDIS_SCORING_TRACE_THREAD_USER_DEFINED_METRIC_PYTHON_MAX_RETRIES:-}
    - scorer: span_llm_as_judge
      streamName: stream_scoring_span_llm_as_judge
      codec: java
      # Optional per stream configuration (falls back to global if not specified)
      poolingInterval: ${REDIS_SCORING_SPAN_LLM_AS_JUDGE_CONSUMER_POOL_INTERVAL:-}
      longPollingDuration: ${REDIS_SCORING_SPAN_LLM_AS_JUDGE_LONG_POLLING_DURATION:-}
      consumerBatchSize: ${REDIS_SCORING_SPAN_LLM_AS_JUDGE_CONSUMER_BATCH_SIZE:-}
      claimIntervalRatio: ${REDIS_SCORING_SPAN_LLM_AS_JUDGE_CLAIM_INTERVAL_RATIO:-}
      pendingMessageDuration: ${REDIS_SCORING_SPAN_LLM_AS_JUDGE_PENDING_MESSAGE_DURATION:-}
      maxRetries: ${REDIS_SCORING_SPAN_LLM_AS_JUDGE_MAX_RETRIES:-}
    - scorer: span_user_defined_metric_python
      streamName: stream_scoring_span_user_defined_metric_python
      codec: java
      # Optional per stream configuration (falls back to global if not specified)
      poolingInterval: ${REDIS_SCORING_SPAN_USER_DEFINED_METRIC_PYTHON_CONSUMER_POOL_INTERVAL:-}
      longPollingDuration: ${REDIS_SCORING_SPAN_USER_DEFINED_METRIC_PYTHON_LONG_POLLING_DURATION:-}
      consumerBatchSize: ${REDIS_SCORING_SPAN_USER_DEFINED_METRIC_PYTHON_CONSUMER_BATCH_SIZE:-}
      claimIntervalRatio: ${REDIS_SCORING_SPAN_USER_DEFINED_METRIC_PYTHON_CLAIM_INTERVAL_RATIO:-}
      pendingMessageDuration: ${REDIS_SCORING_SPAN_USER_DEFINED_METRIC_PYTHON_PENDING_MESSAGE_DURATION:-}
      maxRetries: ${REDIS_SCORING_SPAN_USER_DEFINED_METRIC_PYTHON_MAX_RETRIES:-}

# Configuration for Dataset Export
datasetExport:
  # Default: false
  # Description: Whether dataset export feature is enabled
  enabled: ${DATASET_EXPORT_ENABLED:-false}
  # Default: dataset-export
  # Description: Redis stream name for dataset export jobs
  streamName: ${DATASET_EXPORT_STREAM_NAME:-'dataset-export'}
  # Default: dataset-export-consumers
  # Description: Consumer group name for dataset export job processing
  consumerGroupName: ${DATASET_EXPORT_CONSUMER_GROUP_NAME:-'dataset-export-consumers'}
  # Default: 10
  # Description: Maximum number of export jobs to process in a single batch
  consumerBatchSize: ${DATASET_EXPORT_CONSUMER_BATCH_SIZE:-10}
  # Default: 1s
  # Description: How often to poll Redis for new export jobs
  poolingInterval: ${DATASET_EXPORT_POOLING_INTERVAL:-1s}
  # Default: 5s
  # Description: Timeout for blocking read operations on Redis streams (long polling)
  longPollingDuration: ${DATASET_EXPORT_LONG_POLLING_DURATION:-5s}
  # Default: 3
  # Description: Maximum number of retry attempts for failed export jobs (1-10)
  maxRetries: ${DATASET_EXPORT_MAX_RETRIES:-3}
  # Default: 2
  # Description: Claim pending messages every N polling intervals
  claimIntervalRatio: ${DATASET_EXPORT_CLAIM_INTERVAL_RATIO:-2}
  # Default: 5m
  # Description: Time before export job message considered orphaned and eligible for claiming
  pendingMessageDuration: ${DATASET_EXPORT_PENDING_MESSAGE_DURATION:-5m}
  # Default: 24h
  # Description: Default time-to-live for exported CSV files before automatic cleanup
  defaultTtl: ${DATASET_EXPORT_DEFAULT_TTL:-24h}
  # Default: 5MB (5242880 bytes - S3 minimum for non-final parts)
  # Description: Minimum part size for S3 multipart upload (in bytes). Must be >= 5242880 bytes.
  #              Values below 5242880 will be clamped to 5242880 at runtime.
  minPartSize: ${DATASET_EXPORT_MIN_PART_SIZE:-5242880}
  # Default: 10MB (10 * 1024 * 1024 = 10485760 bytes)
  # Description: Maximum part size for S3 multipart upload (in bytes). Must be >= minPartSize.
  #              Values below minPartSize will be clamped to minPartSize at runtime.
  maxPartSize: ${DATASET_EXPORT_MAX_PART_SIZE:-10485760}
  # Default: 100
  # Description: Number of items to fetch per batch when streaming dataset items
  itemBatchSize: ${DATASET_EXPORT_ITEM_BATCH_SIZE:-100}
  # Default: 5m
  # Description: Timeout for cleanup job execution (how long the cleanup can run)
  cleanupTimeout: ${DATASET_EXPORT_CLEANUP_TIMEOUT:-5m}
  # Default: 1s
  # Description: How long to wait for acquiring the distributed lock for cleanup
  cleanupLockWaitTime: ${DATASET_EXPORT_CLEANUP_LOCK_WAIT_TIME:-1s}
  # Default: 100
  # Description: Batch size for cleanup job operations (number of jobs to process per batch). Must be between 10 and 1000.
  cleanupBatchSize: ${DATASET_EXPORT_CLEANUP_BATCH_SIZE:-100}

# Configuration for Optimization Studio logs synchronization
optimizationLogs:
  # Default: true
  # Description: Whether log synchronization from Redis to S3 is enabled
  enabled: ${OPTIMIZATION_LOGS_ENABLED:-true}
  # Default: 30s
  # Description: Interval between log sync runs
  syncInterval: ${OPTIMIZATION_LOGS_SYNC_INTERVAL:-30s}
  # Default: 30s
  # Description: Lock timeout for per-optimization distributed lock during sync
  lockTimeout: ${OPTIMIZATION_LOGS_LOCK_TIMEOUT:-30s}
  # Default: 5
  # Description: Maximum number of concurrent S3 uploads during sync
  syncConcurrency: ${OPTIMIZATION_LOGS_SYNC_CONCURRENCY:-5}

# LLM providers client configuration
llmProviderClient:
  # Default: 3
  # Description: Max amount of attempts to reach the LLM provider before giving up
  maxAttempts: ${LLM_PROVIDER_CLIENT_MAX_ATTEMPTS:-3}
  # Default: 500
  # Description: Number of milliseconds to wait between retry attempts
  delayMillis: ${LLM_PROVIDER_CLIENT_DELAY_MILLIS:-500}
  # Default: 0.2
  # Description: Max amount of jitter to add to the delay. The value is a percentage in the range [0-1]
  jitterScale: ${LLM_PROVIDER_CLIENT_JITTER_SCALE:-0.2}
  # Default: 1.5
  # Description: Backoff exponent to be used in retry attempts
  backoffExp: ${LLM_PROVIDER_CLIENT_BACKOFF_EXP:-1.5}
  # Default: 60s
  # Description: Call timeout for LLM providers
  callTimeout: ${LLM_PROVIDER_CLIENT_CALL_TIMEOUT:-60s}
  # Default: 60s
  # Description: Connect timeout for LLM providers
  connectTimeout: ${LLM_PROVIDER_CLIENT_CONNECT_TIMEOUT:-60s}
  # Default: 60s
  # Description: Read timeout for LLM providers
  readTimeout: ${LLM_PROVIDER_CLIENT_READ_TIMEOUT:-60s}
  # Default: 60s
  # Description: Write timeout for LLM providers
  writeTimeout: ${LLM_PROVIDER_CLIENT_WRITE_TIMEOUT:-60s}
  # Default: false
  # Description: Whether or not to log requests (HTTP-level logging via langchain4j - can be verbose and may contain credentials)
  logRequests: ${LLM_PROVIDER_CLIENT_LOG_REQUESTS:-false}
  # Default: false
  # Description: Whether or not to log responses (HTTP-level logging via langchain4j - can be verbose)
  logResponses: ${LLM_PROVIDER_CLIENT_LOG_RESPONSES:-false}
  # Configuration for OpenAI client
  openAiClient:
    # Default:
    # Description: OpenAI API URL
    url: ${LLM_PROVIDER_OPENAI_URL:-}
  # Configuration for Anthropic client
  anthropicClient:
    # Default: https://api.anthropic.com/v1/
    # Description: Anthropic API URL
    url: ${LLM_PROVIDER_ANTHROPIC_URL:-https://api.anthropic.com/v1/}
    # Default: 2023-06-01
    # Description: Anthropic API version https://docs.anthropic.com/en/api/versioning
    version: ${LLM_PROVIDER_ANTHROPIC_VERSION:-'2023-06-01'}
  # Default: https://openrouter.ai/api/v1
  # Description: OpenRouter API URL
  openRouterUrl: ${LLM_PROVIDER_OPENROUTER_URL:-https://openrouter.ai/api/v1}

  vertexAIClient:
    # Default: https://www.googleapis.com/auth/cloud-platform
    # Description: Vertex AI SCOPE API URL
    scope: ${LLM_PROVIDER_VERTEXAI_SCOPE:-https://www.googleapis.com/auth/cloud-platform}

# Configuration for cache manager
cacheManager:
  # Default: true
  # Description: Whether or not cache manager is enabled
  enabled: ${CACHE_MANAGER_ENABLED:-true}
  # Default: PT1S
  # Description: Time to live for cache entries in using the formats accepted are based on the ISO-8601: https://docs.oracle.com/javase/8/docs/api/java/time/Duration.html#parse-java.lang.CharSequence-
  defaultDuration: ${CACHE_MANAGER_DEFAULT_DURATION:-PT1S}
  caches:
    # Default: {}
    # Description: Dynamically created caches with their respective time to live in seconds
    automationRules: ${CACHE_MANAGER_AUTOMATION_RULES_DURATION:-PT1S}
    workspace_metadata: ${CACHE_MANAGER_WORKSPACE_METADATA_DURATION:-PT1H}
    experiment_metadata: ${CACHE_MANAGER_EXPERIMENT_METADATA_DURATION:-PT1H}

# Configuration for clickhouse log appender
clickHouseLogAppender:
  # Default: 1000
  # Description: Number of log messages to be batched before sending to ClickHouse
  batchSize: ${CLICKHOUSE_LOG_APPENDER_BATCH_SIZE:-1000}

  # Default: PT0.500S or 500ms
  # Description: Time interval after which the log messages are sent to ClickHouse if the batch size is not reached
  flushIntervalDuration: ${CLICKHOUSE_LOG_APPENDER_FLUSH_INTERVAL_DURATION:-PT0.500S}

workspaceSettings:
  # Default: 50
  # Description: Maximum size of a workspace in GB to allow dynamic sorting.
  # Set to -1 to disable the limit (allow sorting for all workspace sizes).
  # This value is calculated via estimated size based on spans table
  maxSizeToAllowSorting: ${WORKSPACE_SETTINGS_MAX_SIZE_IN_GB_TO_ALLOW_SORTING:-50}
  # Default: 100
  # Description: Maximum size of a project in GB to allow dynamic sorting.
  # Set to -1 to disable the limit (allow sorting for all project sizes).
  # This value is calculated via estimated size based on spans table.
  maxProjectSizeToAllowSorting: ${WORKSPACE_SETTINGS_MAX_PROJECT_SIZE_IN_GB_TO_ALLOW_SORTING:-100}
  # Default: 1000000
  # Description: Maximum number of experiment items in a workspace to allow dynamic sorting.
  # Set to -1 to disable the limit (allow sorting for all experiment item counts).
  # This count-based limit is used because experiment_items has fixed-size columns (UUIDs only)
  # and query performance scales with row count, not data size.
  maxExperimentItemsToAllowSorting: ${WORKSPACE_SETTINGS_MAX_EXPERIMENT_ITEMS_TO_ALLOW_SORTING:-1000000}

# Configuration AWS S3
s3Config:
  s3Region: ${S3_REGION:-'us-east-1'}
  # Description: AWS S3 bucket for Opik
  s3BucketName: ${S3_BUCKET:-'public'}
  # Description: AWS S3 presign url timeout
  preSignUrlTimeoutSec: ${S3_PRESIGN_URL_TIMEOUT:-3600}
  # Description: Url for local MinIO or Aws
  s3Url: ${S3_URL:-http://localhost:9001}
  # Description: Whether installation uses MinIO or AWS S3
  isMinIO: ${IS_MINIO:-true}

# Attachments configuration
attachmentsConfig:
  # Description: Minimum size (bytes) for base64 strings to be considered attachments
  # Default: 256000 (256KB)
  stripMinSize: ${ATTACHMENTS_STRIP_MIN_SIZE:-256000}

# Python evaluator configuration
pythonEvaluator:
  # Default: http://localhost:8000
  # Description: URL of the Python evaluator service, generally the opik-python-backend service
  url: ${PYTHON_EVALUATOR_URL:-http://localhost:8000}
  # Default: 4
  # Description: Maximum number of retry attempts to reach the Python evaluator before giving up
  maxRetryAttempts: ${PYTHON_EVALUATOR_MAX_RETRY_ATTEMPTS:-4}
  # Default: 1s
  # Description: Max delay between retry attempts, will be increased exponentially based on backoffExp
  maxRetryDelay: ${PYTHON_EVALUATOR_MAX_RETRY_DELAY:-1s}
  # Default: 500ms
  # Description: Min delay between retry attempts
  minRetryDelay: ${PYTHON_EVALUATOR_MIN_RETRY_DELAY:-500ms}

serviceToggles:
  # Default: true
  # Description: Whether or not Python evaluator is enabled
  pythonEvaluatorEnabled: ${TOGGLE_PYTHON_EVALUATOR_ENABLED:-"true"}
  # Default: false
  # Description: Whether or not Guardrails feature is enabled
  guardrailsEnabled: ${TOGGLE_GUARDRAILS_ENABLED:-"false"}
  # Default: false
  # Description: Whether or not to enable the trace thread Python evaluator
  traceThreadPythonEvaluatorEnabled: ${TOGGLE_TRACE_THREAD_PYTHON_EVALUATOR_ENABLED:-"true"}
  # Default: false
  # Description: Whether or not to enable the span LLM as Judge evaluator
  spanLlmAsJudgeEnabled: ${TOGGLE_SPAN_LLM_AS_JUDGE_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not to enable the span user-defined metric Python evaluator
  spanUserDefinedMetricPythonEnabled: ${TOGGLE_SPAN_USER_DEFINED_METRIC_PYTHON_ENABLED:-"true"}
  # Default: false
  # Description: Whether or not OpikAI feature is enabled
  opikAIEnabled: ${TOGGLE_OPIK_AI_ENABLED:-"false"}
  # Default: false
  # Description: Whether or not Alert feature is enabled
  alertsEnabled: ${TOGGLE_ALERTS_ENABLED:-"true"}
  # Default: false
  # Description: Whether or not Welcome Wizard is enabled
  welcomeWizardEnabled: ${TOGGLE_WELCOME_WIZARD_ENABLED:-"false"}
  # Default: false
  # Description: Whether or not CSV upload feature is enabled for dataset items
  csvUploadEnabled: ${TOGGLE_CSV_UPLOAD_ENABLED:-"false"}
  # Default: true
  # Description: Whether or not export/download functionality is enabled
  exportEnabled: ${TOGGLE_EXPORT_ENABLED:-"true"}
  # Default: false
  # Description: Whether or not dataset versioning feature is enabled
  datasetVersioningEnabled: ${TOGGLE_DATASET_VERSIONING_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not dataset export to CSV feature is enabled
  datasetExportEnabled: ${DATASET_EXPORT_ENABLED:-"false"}
  # Default: false
  # Description: Whether or not Optimization Studio feature is enabled
  optimizationStudioEnabled: ${TOGGLE_OPTIMIZATION_STUDIO_ENABLED:-"false"}
  # Default: true
  # Description: Whether or not OpenAI provider is enabled
  openaiProviderEnabled: ${TOGGLE_OPENAI_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not Anthropic provider is enabled
  anthropicProviderEnabled: ${TOGGLE_ANTHROPIC_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not Gemini provider is enabled
  geminiProviderEnabled: ${TOGGLE_GEMINI_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not OpenRouter provider is enabled
  openrouterProviderEnabled: ${TOGGLE_OPENROUTER_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not Vertex AI provider is enabled
  vertexaiProviderEnabled: ${TOGGLE_VERTEXAI_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not Bedrock provider is enabled
  bedrockProviderEnabled: ${TOGGLE_BEDROCK_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not Custom LLM provider is enabled
  customllmProviderEnabled: ${TOGGLE_CUSTOMLLM_PROVIDER_ENABLED:-"true"}
  # Default: true
  # Description: Whether or not Ollama provider is enabled
  ollamaProviderEnabled: ${TOGGLE_OLLAMA_PROVIDER_ENABLED:-"true"}
  # Default: false
  # Description: Whether or not Collaborators tab feature is enabled
  collaboratorsTabEnabled: ${TOGGLE_COLLABORATORS_TAB_ENABLED:-"false"}

# Trace Thread configuration
traceThreadConfig:
  # Default: true
  # Description: Whether the trace thread closing job is enabled
  enabled: ${OPIK_TRACE_THREAD_CLOSING_JOB_ENABLED:-true}
  # Default: 15min
  # Description: Inactive thread timeout. If a thread is inactive for this amount of time, it will be marked as inactive
  timeoutToMarkThreadAsInactive: ${OPIK_TRACE_THREAD_TIMEOUT_TO_MARK_AS_INACTIVE:-15m}
  # Default: trace_thread_closing
  # Description: A consumer group name so multiple instances can share the stream load
  consumerGroupName: ${OPIK_REDIS_TRACE_THREAD_CONSUMER_GROUP_NAME:-'trace_thread_closing'}
  # Default: 100
  # Description: Maximum number of messages returned within a Redis Stream get
  consumerBatchSize: ${OPIK_REDIS_TRACE_THREAD_CONSUMER_BATCH_SIZE:-100}
  # Default: 500ms
  # Description: How often the trace thread consumer will check Redis for new messages (in milliseconds)
  poolingInterval: ${OPIK_REDIS_TRACE_THREAD_POOLING_INTERVAL:-500ms}
  # Default: 5s
  # Description: Timeout for blocking read operations on Redis streams (long polling duration)
  longPollingDuration: ${OPIK_REDIS_TRACE_THREAD_LONG_POLLING_DURATION:-5s}
  # Default: trace_thread_closing_stream
  # Description: The name of the Redis Stream used for trace thread closing
  streamName: ${OPIK_REDIS_TRACE_THREAD_STREAM_NAME:-'trace_thread_closing_stream'}
  # Default: 3s
  # Description: The interval at which the close trace thread job will run to inactive threads
  closeTraceThreadJobInterval: ${OPIK_CLOSE_TRACE_THREAD_JOB_INTERVAL:-3s}
  # Default: 4s
  # Description: The time to wait for the close trace threads job to finish before releasing the lock
  closeTraceThreadJobLockTime: ${OPIK_CLOSE_TRACE_THREAD_JOB_LOCK_TIME:-4s}
  # Default: 300ms
  # Description: The time to wait for the close trace threads job to acquire the lock before giving up
  closeTraceThreadJobLockWaitTime: ${OPIK_CLOSE_TRACE_THREAD_JOB_LOCK_WAIT_TIME:-300ms}
  # Default: 2000
  # Description: The maximum number of item to process in a single close trace thread run
  closeTraceThreadMaxItemPerRun: ${OPIK_CLOSE_TRACE_THREAD_MAX_ITEM_PER_RUN:-2000}
  # Default: 10
  # Description: Claim pending messages every N polling intervals
  claimIntervalRatio: ${OPIK_REDIS_TRACE_THREAD_CLAIM_INTERVAL_RATIO:-10}
  # Default: 10m
  # Description: Time before message considered orphaned and eligible for claiming
  pendingMessageDuration: ${OPIK_REDIS_TRACE_THREAD_PENDING_MESSAGE_DURATION:-10m}
  # Default: 3
  # Description: Maximum number of retry attempts for failed messages (1-10)
  maxRetries: ${OPIK_REDIS_TRACE_THREAD_MAX_RETRIES:-3}

# Job timeout configuration
jobTimeout:
  # Default: 30s
  # Description: Timeout for the daily usage report job execution
  dailyUsageReportJobTimeout: ${OPIK_DAILY_USAGE_REPORT_JOB_TIMEOUT:-30}
  # Default: 30s
  # Description: Timeout for the trace threads closing job execution
  traceThreadsClosingJobTimeout: ${OPIK_TRACE_THREADS_CLOSING_JOB_TIMEOUT:-30}

# Response formatting configuration
responseFormatting:
  # Default: 10001
  # Description: Maximum size of input/output/metadata fields in responses (FE limit + 1)
  truncationSize: ${OPIK_RESPONSE_TRUNCATION_CHAR_LIMIT:-10001}

# Queue configuration for Python RQ workers
queues:
  # Default: true
  # Description: Whether queue functionality is enabled
  enabled: ${OPIK_QUEUES_ENABLED:-true}
  # Default: 14 days
  # Description: Default TTL for jobs in queues (how long job data is kept in Redis)
  defaultJobTtl: ${OPIK_QUEUES_DEFAULT_JOB_TTL:-14 days}
  # Per-queue specific configurations
  queues:
    # Optimizer cloud queue configuration
    opik:optimizer-cloud:
      # Job TTL for optimizer jobs
      jobTTl: ${OPIK_OPTIMIZER_QUEUE_JOB_TTL:-1 day}

# Webhook configuration
webhook:
  # Default: false
  # Description: Whether or not webhook functionality is enabled
  enabled: ${WEBHOOK_ENABLED:-true}
  # Default: webhook-events
  # Description: Redis stream name for webhook events
  streamName: ${WEBHOOK_STREAM_NAME:-webhook-events}
  # Default: webhook-consumers
  # Description: Consumer group name for webhook event processing
  consumerGroupName: ${WEBHOOK_CONSUMER_GROUP_NAME:-webhook-consumers}
  # Default: 10
  # Description: Maximum number of messages to consume in a single batch (1-100)
  consumerBatchSize: ${WEBHOOK_CONSUMER_BATCH_SIZE:-10}
  # Default: 1s
  # Description: How often webhook consumer will check Redis for new messages
  poolingInterval: ${WEBHOOK_POOLING_INTERVAL:-1s}
  # Default: 5s
  # Description: Timeout for blocking read operations on Redis streams (long polling duration)
  longPollingDuration: ${WEBHOOK_LONG_POLLING_DURATION:-5s}
  # Default: 3
  # Description: Maximum number of retry attempts for failed webhooks (1-10)
  maxRetries: ${WEBHOOK_MAX_RETRIES:-3}
  # Default: 500ms
  # Description: Initial delay between retry attempts
  initialRetryDelay: ${WEBHOOK_INITIAL_RETRY_DELAY:-500ms}
  # Default: 30s
  # Description: Maximum delay between retry attempts
  maxRetryDelay: ${WEBHOOK_MAX_RETRY_DELAY:-30s}
  # Default: 10s
  # Description: HTTP request timeout for webhook calls
  requestTimeout: ${WEBHOOK_REQUEST_TIMEOUT:-10s}
  # Default: 5s
  # Description: HTTP connection timeout for webhook calls
  connectionTimeout: ${WEBHOOK_CONNECTION_TIMEOUT:-5s}
  # Default: 10
  # Description: Claim pending messages every N polling intervals
  claimIntervalRatio: ${WEBHOOK_CLAIM_INTERVAL_RATIO:-10}
  # Default: 10m
  # Description: Time before message considered orphaned and eligible for claiming
  pendingMessageDuration: ${WEBHOOK_PENDING_MESSAGE_DURATION:-10m}
  debouncing:
    # Default: true
    # Description: Whether or not webhook event debouncing is enabled
    enabled: ${WEBHOOK_DEBOUNCING_ENABLED:-true}
    # Default: 60s
    # Description: Period for events aggregation before sending to webhook endpoint
    windowSize: ${WEBHOOK_DEBOUNCING_WINDOW_SIZE:-60s}
    # Default: 3m
    # Description: Time-to-live for debouncing buckets in Redis
    bucketTtl: ${WEBHOOK_DEBOUNCING_BUCKET_TTL:-3m}
    # Default: 4s
    # Description: Timeout for alert job execution
    alertJobTimeout: ${WEBHOOK_DEBOUNCING_ALERT_JOB_TIMEOUT:-4s}
    # Default: 100ms
    # Description: Maximum time to wait for acquiring alert job lock
    alertJobLockWaitTimeout: ${WEBHOOK_DEBOUNCING_ALERT_JOB_LOCK_WAIT_TIMEOUT:-100ms}
  metrics:
    # Default: 300s
    # Description: Initial delay before the first metrics alert job execution
    initialDelay: ${WEBHOOK_METRICS_INITIAL_DELAY:-300s}
    # Default: 300s
    # Description: Fixed delay between metrics alert job executions (waits this duration after completion)
    fixedDelay: ${WEBHOOK_METRICS_FIXED_DELAY:-300s}
    # Default: 60s
    # Description: Timeout for metrics alert job execution
    metricsAlertJobTimeout: ${WEBHOOK_METRICS_ALERT_JOB_TIMEOUT:-60s}
    # Default: 1s
    # Description: Maximum time to wait for acquiring metrics alert job lock
    metricsAlertJobLockWaitTimeout: ${WEBHOOK_METRICS_ALERT_JOB_LOCK_WAIT_TIMEOUT:-1s}

# Free Model configuration (admin-configured server-side LLM provider)
# Note: The model name "opik-free-model" is hardcoded and cannot be changed.
# This ensures backward compatibility with automation rules that store the model name.
freeModel:
  # Description: Whether to enable the free model provider
  # Default: false
  enabled: ${OPIK_FREE_MODEL_ENABLED:-false}
  # Description: Actual model name sent to the AI server (the underlying LLM)
  # Default: empty
  actualModel: ${OPIK_FREE_MODEL_ACTUAL_MODEL:-}
  # Description: Provider name stored in spans for cost tracking (e.g., "openai")
  # Default: empty
  spanProvider: ${OPIK_FREE_MODEL_SPAN_PROVIDER:-}
  # Description: Base URL for OpenAI-compatible endpoint (e.g., "https://api.openai.com/v1")
  # Default: empty
  baseUrl: ${OPIK_FREE_MODEL_BASE_URL:-}
  # Description: API key for the endpoint (optional for auth-less endpoints)
  # Default: empty
  apiKey: ${OPIK_FREE_MODEL_API_KEY:-}

# Dataset Versioning Migration configuration
# Includes both lazy migration (on-demand) and items_total migration (startup)
datasetVersioningMigration:
  # Default: false
  # Description: Whether to enable lazy migration (migrate datasets on first access)
  lazyEnabled: ${DATASET_VERSIONING_MIGRATION_LAZY_ENABLED:-"false"}
  # Default: true
  # Description: Whether to run the items_total migration on startup
  # Set to true to enable the migration, false to disable after completion
  # This migration calculates and updates the items_total field for dataset versions
  # that were created by the Liquibase migration (where dataset_id = dataset_version_id)
  itemsTotalEnabled: ${DATASET_VERSION_ITEMS_TOTAL_MIGRATION_ENABLED:-"true"}
  # Default: 100
  # Description: Number of dataset versions to process in each batch for items_total migration
  # Smaller batches reduce memory usage but increase total migration time
  itemsTotalDatasetsBatchSize: ${DATASET_VERSION_ITEMS_TOTAL_MIGRATION_BATCH_SIZE:-100}
  # Default: 3600 (1 hour)
  # Description: Lock timeout in seconds for the items_total migration process
  # Should be long enough to complete the migration
  itemsTotalLockTimeoutSeconds: ${DATASET_VERSION_ITEMS_TOTAL_MIGRATION_LOCK_TIMEOUT_SECONDS:-3600}
  # Default: 30 seconds
  # Description: Delay in seconds before starting the items_total migration job after application startup
  # This allows the service to stabilize before running the migration
  itemsTotalStartupDelaySeconds: ${DATASET_VERSION_ITEMS_TOTAL_MIGRATION_STARTUP_DELAY_SECONDS:-30}
  # Default: 3600s (1 hour)
  # Description: Job execution timeout in seconds for the items_total migration
  # Should be long enough to complete the entire migration process
  itemsTotalJobTimeoutSeconds: ${DATASET_VERSION_ITEMS_TOTAL_MIGRATION_JOB_TIMEOUT_SECONDS:-3600}
