{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LangGraph Agent with @agent_config\n",
    "\n",
    "A simple agent with nodes A, B, C where routing is controlled by config."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "from dataclasses import dataclass, field\nfrom typing import TypedDict\n\nfrom langgraph.graph import StateGraph, END\nfrom opik.integrations.langchain import track_langgraph, OpikTracer\n\nfrom opik_config import Prompt, agent_config\nfrom opik_config.registration import flush_registrations\n\n\n@agent_config\n@dataclass\nclass AgentConfig:\n    use_path_b: bool = True\n    prompt_a: Prompt = field(default_factory=lambda: Prompt(name=\"SystemPromptA\", prompt=\"You are node A. Analyze the input.\"))\n    prompt_b: Prompt = field(default_factory=lambda: Prompt(name=\"AIMessageB\", prompt=\"You are node B. Summarize concisely.\"))\n    model: str = \"gpt-4\"\n\n\nconfig = AgentConfig()\n\n# Force immediate registration with backend (normally happens in background)\nflush_registrations()\nprint(\"Config registered with backend\")\n\n\nclass State(TypedDict):\n    input: str\n    output: str\n\n\ndef node_a(state: State) -> State:\n    prompt = config.prompt_a\n    print(f\"[Node A] Using prompt: {prompt}\")\n    return {\"input\": state[\"input\"], \"output\": f\"A processed: {state['input']}\"}\n\n\ndef node_b(state: State) -> State:\n    prompt = config.prompt_b\n    print(f\"[Node B] Using prompt: {prompt}\")\n    return {\"input\": state[\"input\"], \"output\": f\"B summarized: {state['output']}\"}\n\n\ndef node_c(state: State) -> State:\n    print(f\"CCCCC\")\n    print(config.model)\n    return {\"input\": state[\"input\"], \"output\": f\"C expanded: {state['output']}\"}\n\n\ndef route_from_a(state: State) -> str:\n    if config.use_path_b:\n        print(\"[Router] Taking path B\")\n        return \"node_b\"\n    else:\n        print(\"[Router] Taking path C\")\n        return \"node_c\"\n\n# Build the graph\ngraph = StateGraph(State)\ngraph.add_node(\"node_a\", node_a)\ngraph.add_node(\"node_b\", node_b)\ngraph.add_node(\"node_c\", node_c)\n\ngraph.set_entry_point(\"node_a\")\ngraph.add_conditional_edges(\"node_a\", route_from_a, {\"node_b\": \"node_b\", \"node_c\": \"node_c\"})\ngraph.add_edge(\"node_b\", END)\ngraph.add_edge(\"node_c\", END)\n\ntracer = OpikTracer(project_name=\"config-demo\")\nagent = track_langgraph(graph.compile(), tracer)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Run with default config (path B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"Default Project\" project at http://localhost:5174/api/v1/session/redirect/projects/?trace_id=019c0b62-47fd-7b8b-86f2-10897181db16&path=aHR0cDovL2xvY2FsaG9zdDo1MTc0L2FwaS8=.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Default config (use_path_b=True) ===\n",
      "[Node A] Using prompt: You are node A. Analyze the input.\n",
      "[Router] Taking path B\n",
      "[Node B] Using prompt: You are node B. Summarize concisely.\n",
      "Result: B summarized: A processed: Hello world\n"
     ]
    }
   ],
   "source": [
    "def run_agent(user_input: str) -> str:\n",
    "    result = agent.invoke({\"input\": user_input, \"output\": \"\"})\n",
    "    return result[\"output\"]\n",
    "\n",
    "\n",
    "print(\"=== Default config (use_path_b=True) ===\")\n",
    "output = run_agent(\"Hello world\")\n",
    "print(f\"Result: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7b58dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node A] Using prompt: You are node A. Analyze the input.\n",
      "[Router] Taking path C\n",
      "CCCCC\n",
      "gpt-4\n",
      "Result: C expanded: A processed: Hello world\n"
     ]
    }
   ],
   "source": [
    "from opik_config import experiment_context\n",
    "\n",
    "EXPERIMENT_ID = \"8d826426-8e81-444d-b3df-ae0dfebb064d\"\n",
    "\n",
    "experiment_id = str(EXPERIMENT_ID)\n",
    "with experiment_context(experiment_id):\n",
    "    output = run_agent(\"Hello world\")\n",
    "    print(f\"Result: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Set up experiment overrides\n",
    "\n",
    "Start the config backend first:\n",
    "```bash\n",
    "uv run python -m opik_config\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport uuid\n\nCONFIG_URL = \"http://localhost:5050\"\n\nexperiment_id = str(uuid.uuid4())\nprint(f\"Experiment ID: {experiment_id}\")\n\n# Create the experiment (mask) first\nrequests.post(f\"{CONFIG_URL}/v1/config/masks\", json={\n    \"project_id\": \"default\",\n    \"env\": \"prod\",\n    \"mask_id\": experiment_id,\n    \"is_ab\": False,\n})\n\n# Override: take path C instead of B\nrequests.post(f\"{CONFIG_URL}/v1/config/masks/override\", json={\n    \"project_id\": \"default\",\n    \"env\": \"prod\",\n    \"mask_id\": experiment_id,\n    \"variant\": \"default\",\n    \"key\": \"AgentConfig.use_path_b\",\n    \"value\": False,\n})\n\n# Override: change model\nrequests.post(f\"{CONFIG_URL}/v1/config/masks/override\", json={\n    \"project_id\": \"default\",\n    \"env\": \"prod\",\n    \"mask_id\": experiment_id,\n    \"variant\": \"default\",\n    \"key\": \"AgentConfig.model\",\n    \"value\": \"gpt-3.5-turbo\",\n})\n\nprint(\"Experiment created with overrides: use_path_b=False, model=gpt-3.5-turbo\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c4384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Run with experiment context (path C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opik_config import experiment_context\n",
    "\n",
    "print(\"=== With experiment (use_path_b=False, new prompt_c) ===\")\n",
    "with experiment_context(experiment_id):\n",
    "    output = run_agent(\"Hello world\")\n",
    "print(f\"Result: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Run again without experiment (back to path B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Without experiment context (defaults) ===\")\n",
    "output = run_agent(\"Hello again\")\n",
    "print(f\"Result: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}