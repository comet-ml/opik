# Default values for opik public helm chart
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

nameOverride: "opik"
fullnameOverride: ""

standalone: true
basicAuth: false

registry: &registry ghcr.io/comet-ml/opik

localFEAddress: "host.minikube.internal:5174"
localFE: false

demoDataJob: true

component:
  backend:
    enabled: true
    serviceAccount:
      create: true
    metrics:
      enabled: false
    image:
      repository: opik-backend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: true
    run_migration: true
    waitForClickhouse:
      clickhouse:
        host: clickhouse-opik-clickhouse
        port: 8123
        protocol: http
      image:
        registry: docker.io
        repository: curlimages/curl
        tag: 8.12.1
    livenessProbe:
      path: /health-check?name=all&type=alive
      port:  8080
    readinessProbe:
      path: /health-check?name=all&type=ready
      port:  8080
      initialDelaySeconds: 20

    env:
      STATE_DB_PROTOCOL: "jdbc:mysql://"
      STATE_DB_URL: "opik-mysql:3306/opik?rewriteBatchedStatements=true"
      STATE_DB_DATABASE_NAME: "opik"
      STATE_DB_USER: opik
      ANALYTICS_DB_MIGRATIONS_URL: "jdbc:clickhouse://clickhouse-opik-clickhouse:8123"
      ANALYTICS_DB_MIGRATIONS_USER: "opik"
      ANALYTICS_DB_PROTOCOL: "HTTP"
      ANALYTICS_DB_HOST: "clickhouse-opik-clickhouse"
      ANALYTICS_DB_PORT: "8123"
      ANALYTICS_DB_USERNAME: "opik"
      ANALYTICS_DB_DATABASE_NAME: "opik"
      JAVA_OPTS: "-Dliquibase.propertySubstitutionEnabled=true -XX:+UseG1GC -XX:MaxRAMPercentage=80.0 -XX:MinRAMPercentage=75"
      REDIS_URL: redis://:wFSuJX9nDBdCa25sKZG7bh@opik-redis-master:6379/
      ANALYTICS_DB_MIGRATIONS_PASS: opik
      ANALYTICS_DB_PASS: opik
      STATE_DB_PASS: opik
      OPIK_OTEL_SDK_ENABLED: false
      OTEL_VERSION: 2.12.0
      OTEL_PROPAGATORS: "tracecontext,baggage,b3"
      OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: true
      OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION: BASE2_EXPONENTIAL_BUCKET_HISTOGRAM
      OTEL_EXPERIMENTAL_RESOURCE_DISABLED_KEYS: process.command_args
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: delta
      PYTHON_EVALUATOR_URL: http://opik-python-backend:8000
      AWS_ACCESS_KEY_ID: THAAIOSFODNN7EXAMPLE # rootUser for minio
      AWS_SECRET_ACCESS_KEY: LESlrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY # rootPassword for minio
      S3_URL: "http://opik-minio:9000"
      TOGGLE_PYTHON_EVALUATOR_ENABLED: true
    envFrom:
      - configMapRef:
          name: opik-backend

    service:
      type: ClusterIP
      ports:
      - name: http
        port:  8080
        protocol: TCP
        targetPort:  8080
      - name: backend
        port:  3003
        protocol: TCP
        targetPort:  3003

    ingress:
      enabled: false

  python-backend:
    enabled: true
    serviceAccount:
      create: true
    metrics:
      enabled: false
    image:
      repository: opik-python-backend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: true
    env:
      PYTHON_CODE_EXECUTOR_IMAGE_REGISTRY: *registry
      PYTHON_CODE_EXECUTOR_IMAGE_NAME: "opik-sandbox-executor-python"
      PYTHON_CODE_EXECUTOR_IMAGE_TAG: "latest"
      PYTHON_CODE_EXECUTOR_STRATEGY: "process"
      PYTHON_CODE_EXECUTOR_PARALLEL_NUM: "5"
      PYTHON_CODE_EXECUTOR_EXEC_TIMEOUT_IN_SECS: "3"
      OPIK_REVERSE_PROXY_URL: "http://opik-frontend:5173/api"
      OTEL_SERVICE_NAME: "opik-python-backend"
      OTEL_METRIC_EXPORT_INTERVAL: "60000"      
      OTEL_PROPAGATORS: "tracecontext,baggage"
      OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: true
      OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION: BASE2_EXPONENTIAL_BUCKET_HISTOGRAM
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
    envFrom:
      - configMapRef:
          name: opik-python-backend

    securityContext:
      privileged: true
    networkPolicy:
      enabled: true
      engineEgress:
        ipBlock: 0.0.0.0/0
        except:
          - 10.0.0.0/8
          - 100.64.0.0/10
          - 172.16.0.0/12
          - 192.0.0.0/24
          - 198.18.0.0/15
          - 192.168.0.0/16

    service:
      type: ClusterIP
      ports:
      - name: http
        port:  8000
        protocol: TCP
        targetPort:  8000

    ingress:
      enabled: false

  frontend:
    enabled: true
    serviceAccount:
      create: true
    metrics:
      enabled: false
    image:
      repository: opik-frontend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: false
    volumeMounts:
    - mountPath: /etc/nginx/conf.d/
      name: opik-frontend-nginx
    volumes:
    - name: opik-frontend-nginx
      configMap:
        name: opik-frontend-nginx
        items:
        - key: default.conf
          path: default.conf
    service:
      type: ClusterIP
      ports:
      - name: http
        port:  5173
        protocol: TCP
        targetPort:  5173
    ingress:
      enabled: false
    # throttled locations
    throttling: {}
    # map variables for use with throttling and other places
    maps: []
    # use internal aws resolvr on vpc
    awsResolver: false
    logFormat: logger-json
    logFormats:
      "logger-json": >-
        escape=json '{
        "body_bytes_sent": $body_bytes_sent,
        "http_referer": "$http_referer",
        "http_user_agent": "$http_user_agent",
        "remote_addr": "$remote_addr",
        "remote_user": "$remote_user",
        "request": "$request",
        "status": $status,
        "time_local": "$time_local",
        "x_forwarded_for": "$http_x_forwarded_for"
        }'
    upstreamConfig: {}
      # proxy_read_timeout: 60
      # proxy_connect_timeout: 60
      # proxy_send_timeout: 60
      # proxy_buffering: 'on'
      # proxy_request_buffering: 'on'

mysql:
  enabled: true
  fullnameOverride: opik-mysql
  auth:
    rootPassword: "opik"
  initdbScripts:
    createdb.sql: |-
      CREATE DATABASE IF NOT EXISTS opik DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
      CREATE USER IF NOT EXISTS 'opik'@'%' IDENTIFIED BY 'opik';
      GRANT ALL ON `opik`.* TO 'opik'@'%';
      FLUSH PRIVILEGES;

minio:
  enabled: true
  mode: standalone
  disableWebUI: true
  persistence:
    enabled: true
    size: 50Gi
  auth:
    rootUser: "THAAIOSFODNN7EXAMPLE"
    rootPassword: "LESlrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  provisioning:
    enabled: true
    extraCommands:
      - mc alias set s3 http://opik-minio:9000 THAAIOSFODNN7EXAMPLE LESlrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY --api S3v4
      - mc mb --ignore-existing s3/public
      - mc anonymous set download s3/public/
  fullnameOverride: opik-minio


redis:
  enabled: true
  ssl: false
  fullnameOverride: opik-redis
  architecture: standalone
  metrics:
    enabled: false
  auth:
    password: "wFSuJX9nDBdCa25sKZG7bh"
  image:
    repository: redis/redis-stack-server
    tag: 7.2.0-v10
  master:
    resources:
      requests:
        cpu: 15m
        memory: 105M
      limits:
        # cpu: 15m
        memory: 1Gi
    args:
      - -c
      - /opt/bitnami/scripts/merged-start-scripts/start-master.sh
    configuration: maxmemory 105M
    extraVolumes:
      - name: merged-start-scripts
        configMap:
          name: bitnami-redis-stack-server-merged
          defaultMode: 493
    extraVolumeMounts:
      - name: merged-start-scripts
        mountPath: /opt/bitnami/scripts/merged-start-scripts
  extraDeploy:
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: bitnami-redis-stack-server-merged
      data:
        start-master.sh: "#!/usr/bin/dumb-init /bin/bash\n### docker entrypoint script,\
          \ for starting redis stack\nBASEDIR=/opt/redis-stack\ncd ${BASEDIR}\nCMD=${BASEDIR}/bin/redis-server\n\
          if [ -z \"${REDISEARCH_ARGS}\" ]; then\nREDISEARCH_ARGS=\"MAXSEARCHRESULTS\
          \ 10000 MAXAGGREGATERESULTS 10000\"\nfi\n[[ -f $REDIS_PASSWORD_FILE ]] &&\
          \ export REDIS_PASSWORD=\"$(< \"${REDIS_PASSWORD_FILE}\")\"\nif [[ -f /opt/bitnami/redis/mounted-etc/master.conf\
          \ ]];then\n    cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n\
          fi\nif [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then\n    cp /opt/bitnami/redis/mounted-etc/redis.conf\
          \ /opt/bitnami/redis/etc/redis.conf\nfi\n${CMD} \\\n--port \"${REDIS_PORT}\"\
          \ \\\n--requirepass \"${REDIS_PASSWORD}\" \\\n--masterauth \"${REDIS_PASSWORD}\"\
          \ \\\n--include \"/opt/bitnami/redis/etc/redis.conf\" \\\n--include \"/opt/bitnami/redis/etc/master.conf\"\
          \ \\\n--loadmodule ${BASEDIR}/lib/redisearch.so ${REDISEARCH_ARGS} \\\n\
          --loadmodule ${BASEDIR}/lib/redistimeseries.so ${REDISTIMESERIES_ARGS} \\\
          \n--loadmodule ${BASEDIR}/lib/rejson.so ${REDISJSON_ARGS} \\\n--loadmodule\
          \ ${BASEDIR}/lib/redisbloom.so ${REDISBLOOM_ARGS}\n"

altinity-clickhouse-operator:
  metrics:
    enabled: false
  serviceMonitor:
    enabled: false

clickhouse:
  enabled: true
  shardsCount: 1
  replicasCount: 1
  image: altinity/clickhouse-server:24.3.5.47.altinitystable
  storage: 50Gi
  logsLevel: information
  service:
    serviceTemplate: clickhouse-cluster-svc-template
  adminUser:
    useSecret:
      enabled: false
    username: opik
    password: opik
  zookeeper:
      host: opik-zookeeper
  backup: 
    enabled: false
    successfulJobsHistoryLimit: 1
    # bucketURL: ""
    # secretName: ""
    # schedule: ""
    # baseBackup: ""
  monitoring:
    enabled: false
    username: opikmon # readonly user
    password: opikmon
    # resources:
    #   requests:
    #     cpu: 300m
    #     memory: 1Gi
    #   limits:
    #     memory: 1Gi

zookeeper:
    enabled: true
    fullnameOverride: opik-zookeeper
    replicaCount: 1
    # podAntiAffinityPreset: hard
    podDisruptionBudget:
      enabled: true
    persistence:
      enabled: true
      ## The amount of PV storage allocated to each Zookeeper pod in the statefulset
      size: "50Gi"
    ## Environmental variables to set in Zookeeper
    ##
    env:
      ## The JVM heap size to allocate to Zookeeper
      ZK_HEAP_SIZE: "512M"
    ## Configure Zookeeper headless
    headless:
      publishNotReadyAddresses: true

    # resources:
    #   requests:
    #     cpu: 300m
    #     memory: 1500Mi
    #   limits:
    #     memory: 2Gi
