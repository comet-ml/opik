# Default values for opik public helm chart
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  argocd: false
  useHelmHooks: true
  security:
    allowInsecureImages: true

nameOverride: "opik"
fullnameOverride: ""

standalone: true
basicAuth: false

registry: &registry ghcr.io/comet-ml/opik

localFEAddress: "host.minikube.internal:5174"
localFE: false

serviceAccount:
  create: false
  name: ""
  annotations: {}

demoDataJob:
  enabled: true

chartMigration:
  # Enable this for the first upgrade that includes a migration from Bitnami charts and images
  # After successful migration, set this back to false
  enabled: true
  image: "alpine/kubectl:1.35.0"
  # Service account name (must have permissions to delete StatefulSets and Deployments)
  serviceAccountName: ""
  # Node selector for the migration job pod (falls back to global nodeSelector if not specified)
  nodeSelector: {}
  # Tolerations for the migration job pod (falls back to global tolerations if not specified)
  tolerations: []


nodeSelector: {}
tolerations: []
affinity: {}

caCerts:
  # -- (list) Additional Certificate Authority Certificates to trust.
  # Each list entry has two keys: `name` and `content`. `name` should be a
  # unique identifier. content Should be PEM formated Public Certificate
  # contents.
  additionalCACerts:

  # -- Certificates are Public Keys + Metadata, but some organizations may
  # still prefer storing these in Secrets.
  additionalCACertsInSecret: false

  # -- If there is an existing ConfigMap containing the additional CA
  # Certificates you can provide its name here instead of creatinga a new one.
  # If `additionalCACertsInSecret` is `true` we will look for this name in
  # Secrets.
  existingAdditionalCACertsRef:

  overwriteJavaCATrustStore:
    # -- If enabled we will not inject additional CA Certificates to the Java
    # TrustStore, but instead will mount a given volume containing a Java Trust
    # Store to replace the default one in the containers.
    enabled: false
    # -- A Kubernetes Volume definition which will contain a valid Java Trust
    # Store. See
    # https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/volume/
    # for valid parameters.
    volume: {}
    # -- The sub-path of the valid Java Trust Store to mount from the volume.
    subPath: cacerts


component:
  backend:
    enabled: true
    serviceAccount:
      create: true
      name: opik-backend
    metrics:
      enabled: false
    image:
      repository: opik-backend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: true
    run_migration: true
    waitForClickhouse:
      clickhouse:
        host: clickhouse-opik-clickhouse
        port: 8123
        protocol: http
      image:
        registry: docker.io
        repository: curlimages/curl
        tag: 8.12.1
    waitForMysql:
      enabled: false
      mysql:
        host: opik-mysql
        port: 3306
      image:
        registry: docker.io
        repository: busybox
        tag: 1.36
    resources:
      requests:
        ephemeral-storage: 10Gi
    livenessProbe:
      httpGet:
        path: /health-check?name=all&type=alive
        port: 8080
        httpHeaders:
          - name: Accept
            value: application/json
    readinessProbe:
      httpGet:
        path: /health-check?name=all&type=ready
        port: 8080
        httpHeaders:
          - name: Accept
            value: application/json
      initialDelaySeconds: 20
    podDisruptionBudget:
      enabled: false
    env:
      STATE_DB_PROTOCOL: "jdbc:mysql://"
      STATE_DB_URL: "opik-mysql:3306/opik?rewriteBatchedStatements=true"
      STATE_DB_DATABASE_NAME: "opik"
      STATE_DB_USER: opik
      ANALYTICS_DB_MIGRATIONS_URL: "jdbc:clickhouse://clickhouse-opik-clickhouse:8123"
      ANALYTICS_DB_MIGRATIONS_USER: "opik"
      ANALYTICS_DB_PROTOCOL: "HTTP"
      ANALYTICS_DB_HOST: "clickhouse-opik-clickhouse"
      ANALYTICS_DB_PORT: "8123"
      ANALYTICS_DB_USERNAME: "opik"
      ANALYTICS_DB_DATABASE_NAME: "opik"
      JAVA_OPTS: "-Dliquibase.propertySubstitutionEnabled=true -XX:+UseG1GC -XX:MaxRAMPercentage=80.0 -XX:MinRAMPercentage=75"
      REDIS_URL: redis://:wFSuJX9nDBdCa25sKZG7bh@opik-redis-master:6379/
      ANALYTICS_DB_MIGRATIONS_PASS: opik
      ANALYTICS_DB_PASS: opik
      STATE_DB_PASS: opik
      OPIK_OTEL_SDK_ENABLED: false
      OTEL_VERSION: 2.12.0
      OTEL_PROPAGATORS: "tracecontext,baggage,b3"
      OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: true
      OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION: BASE2_EXPONENTIAL_BUCKET_HISTOGRAM
      OTEL_EXPERIMENTAL_RESOURCE_DISABLED_KEYS: process.command_args
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: delta
      PYTHON_EVALUATOR_URL: http://opik-python-backend:8000
    envFrom:
      - configMapRef:
          name: opik-backend
    service:
      type: ClusterIP
      ports:
      - name: http
        port:  8080
        protocol: TCP
        targetPort:  8080
      - name: swagger
        port:  3003
        protocol: TCP
        targetPort:  3003
    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
      hosts: []
      tls:
        enabled: false
        hosts: []
        secretName: ""
    # @ignored
    usesJavaTrustStore: true

  python-backend:
    enabled: true
    serviceAccount:
      create: true
      name: opik-python-backend
    metrics:
      enabled: false
    image:
      repository: opik-python-backend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: true
    podDisruptionBudget:
      enabled: false
    env:
      PYTHON_CODE_EXECUTOR_IMAGE_REGISTRY: *registry
      PYTHON_CODE_EXECUTOR_IMAGE_NAME: "opik-sandbox-executor-python"
      PYTHON_CODE_EXECUTOR_IMAGE_TAG: "latest"
      PYTHON_CODE_EXECUTOR_STRATEGY: "process"
      PYTHON_CODE_EXECUTOR_PARALLEL_NUM: "5"
      PYTHON_CODE_EXECUTOR_EXEC_TIMEOUT_IN_SECS: "3"
      PYTHON_CODE_EXECUTOR_ALLOW_NETWORK: "false"
      OPIK_REVERSE_PROXY_URL: "http://opik-frontend:5173/api"
      OTEL_SERVICE_NAME: "opik-python-backend"
      OTEL_METRIC_EXPORT_INTERVAL: "60000"
      OTEL_PROPAGATORS: "tracecontext,baggage"
      OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: true
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
      # Opik SDK URL for optimizer to reach Java backend
      OPIK_URL_OVERRIDE: "http://opik-backend:8080"
      # Redis URL for Optimization Studio log streaming
      REDIS_URL: redis://:wFSuJX9nDBdCa25sKZG7bh@opik-redis-master:6379/
      
      # ============================================================
      # Optimization Studio Configuration (optional)
      # ============================================================
      # Maximum number of concurrent optimization jobs (default: 5)
      # Each job runs in a separate worker thread with isolated subprocess
      # OPTSTUDIO_MAX_CONCURRENT_JOBS: "5"
      # Number of dataset items to sample for optimization (default: 20)
      # Limits memory usage for large datasets
      # OPTSTUDIO_DATASET_SAMPLES: "20"
      # Terminal width for Rich log output formatting (default: 80)
      # OPTSTUDIO_LOG_TERM_WIDTH: "80"
      # Execution timeout for optimization jobs in seconds (default: 7200 = 2 hours)
      # OPTSTUDIO_EXECUTION_TIMEOUT: "7200"
      # TTL for optimization logs in Redis in seconds (default: 86400 = 24 hours)
      # OPTSTUDIO_LOG_REDIS_TTL: "86400"
      # Default max output tokens for optimizer LLM calls (default: 8192)
      # Prevents truncation of structured JSON outputs from LLM
      # OPTSTUDIO_LLM_MAX_TOKENS: "8192"
      # TTL for failed RQ jobs in Redis in seconds (default: 86400 = 1 day)
      # RQ_WORKER_TTL_FAILURE: "86400"
    
    # Optional: Reference external Kubernetes secrets for sensitive configuration
    # Use this to inject LLM API keys (OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.)
    # from external secret managers (AWS Secrets Manager, GCP Secret Manager, Azure Key Vault)
    # 
    # Customers can create secrets using:
    # - ExternalSecret Operator (recommended for production)
    # - Kubernetes Secrets CSI Driver
    # - Manual kubectl create secret
    # 
    # Each secretRef will be added as an envFrom entry, making all key-value pairs
    # from the secret available as environment variables in the python-backend pod.
    # 
    # Example configuration:
    # secretRefs:
    #   - name: "opik-llm-api-keys"       # Contains OPENAI_API_KEY, ANTHROPIC_API_KEY, etc.
    #   - name: "opik-aws-credentials"    # Contains AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
    # 
    # For detailed setup instructions, see:
    # apps/opik-python-backend/docs/LLM_API_KEYS_CONFIGURATION.md
    secretRefs: []
    
    envFrom:
      - configMapRef:
          name: opik-python-backend

    securityContext:
      privileged: true
    networkPolicy:
      enabled: true
      engineEgress:
        ipBlock: 0.0.0.0/0
        except:
          - 10.0.0.0/8
          - 100.64.0.0/10
          - 172.16.0.0/12
          - 192.0.0.0/24
          - 198.18.0.0/15
          - 192.168.0.0/16

    service:
      type: ClusterIP
      ports:
      - name: http
        port:  8000
        protocol: TCP
        targetPort:  8000

    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
      hosts: []
      tls:
        enabled: false
        hosts: []
        secretName: ""

  frontend:
    enabled: true
    serviceAccount:
      create: true
      name: opik-frontend
    metrics:
      enabled: false
    image:
      repository: opik-frontend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: false
    resources:
      requests:
        ephemeral-storage: 10Gi
    service:
      type: ClusterIP
      ports:
      - name: http
        port:  5173
        protocol: TCP
        targetPort:  5173
    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
      hosts: []
        # - host: opik.example.com
        #   paths:
        #     - path: /
        #       port: 5173
        #       pathType: Prefix
      tls:
        enabled: false
        hosts: []
        secretName: ""
    # throttled locations
    throttling: {}
    # map variables for use with throttling and other places
    maps: []
    # use internal aws resolvr on vpc
    awsResolver: false
    # Enable HSTS header (only enable when running behind HTTPS termination)
    hstsEnabled: false
    upstreamConfig: {}
      # proxy_read_timeout: 60
      # proxy_connect_timeout: 60
      # proxy_send_timeout: 60
      # proxy_buffering: 'on'
      # proxy_request_buffering: 'on'
    podDisruptionBudget:
      enabled: false

mysql:
  enabled: true
  fullnameOverride: opik-mysql
  auth:
    rootPassword: "opik"
  initdbScripts:
    createdb.sql: |-
      CREATE DATABASE IF NOT EXISTS opik DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
      CREATE USER IF NOT EXISTS 'opik'@'%' IDENTIFIED BY 'opik';
      GRANT ALL ON `opik`.* TO 'opik'@'%';
      FLUSH PRIVILEGES;
  primary:
    # Use Bitnami MySQL data directory path for compatibility
    dataDir: /bitnami/mysql/data
    persistence:
      enabled: true
      size: 20Gi

minio:
  enabled: true
  image:
    registry: docker.io
    repository: minio/minio
    tag: "RELEASE.2025-03-12T18-04-18Z"
    imagePullPolicy: IfNotPresent
  replicaCount: 1
  config:
    browserEnabled: false
  persistence:
    enabled: true
    size: 50Gi
  auth:
    rootUser: "THAAIOSFODNN7EXAMPLE"
    rootPassword: "LESlrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  defaultBuckets: "public:download"
  fullnameOverride: opik-minio


redis:
  enabled: true
  # Use bitnami redis fullnameOverride for compatibility
  fullnameOverride: opik-redis-master
  architecture: standalone
  metrics:
    enabled: false
  auth:
    enabled: true
    password: "wFSuJX9nDBdCa25sKZG7bh"
  image:
    registry: docker.io
    repository: redis/redis-stack-server
    tag: "7.2.0-v10"
    pullPolicy: IfNotPresent
  resources:
    requests:
      cpu: 15m
      memory: 105M
    limits:
      memory: 1Gi
  config:
    content: |
      # Redis Stack configuration
      dir /data
      maxmemory 105M
      maxmemory-policy allkeys-lru
      # Redis Stack modules are automatically loaded by the redis-stack-server image
  persistence:
    enabled: true
    size: 8Gi

altinity-clickhouse-operator:
  enabled: true
  metrics:
    enabled: false
  serviceMonitor:
    enabled: false
    interval: '' # workaround for https://github.com/Altinity/clickhouse-operator/issues/1759

clickhouse:
  enabled: true
  shardsCount: 1
  replicasCount: 1
  image: altinity/clickhouse-server:25.3.6.10034.altinitystable
  storage: 50Gi
  logsLevel: information
  # Custom health checks with longer timeouts for major version upgrades
  livenessProbe:
    httpGet:
      path: /ping
      port: 8123
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 10
  readinessProbe:
    httpGet:
      path: /ping
      port: 8123
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30
  configuration:
    files:
      conf.d/memory.xml: |
        <yandex>
          <max_server_memory_usage_to_ram_ratio>0.85</max_server_memory_usage_to_ram_ratio>
        </yandex>
      conf.d/profiles.xml: |
        <clickhouse>
          <profiles>
            <default>
                <max_bytes_ratio_before_external_sort>0.2</max_bytes_ratio_before_external_sort>
                <max_bytes_ratio_before_external_group_by>0.2</max_bytes_ratio_before_external_group_by>
            </default>
          </profiles>
        </clickhouse>
      # Remove or add TTL to system tables that are not needed
      conf.d/system_tables.xml: |
        <clickhouse>
          <opentelemetry_span_log remove="1"/>
          <asynchronous_metric_log remove="1"/>
          <processors_profile_log remove="1"/>
          <text_log remove="1"/>
          <trace_log remove="1"/>
          <blob_storage_log remove="1"/>
          <error_log>
              <engine>
                  ENGINE MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>error_log</table>
          </error_log>
          <latency_log>
              <engine>
                  ENGINE = MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>latency_log</table>
          </latency_log>
          <metric_log>
              <engine>
                  ENGINE = MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>metric_log</table>
          </metric_log>
          <query_metric_log>
              <engine>
                  ENGINE = MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>query_metric_log</table>
          </query_metric_log>
        </clickhouse>
  service:
    serviceTemplate: clickhouse-cluster-svc-template
  templates:
    podTemplate: clickhouse-cluster-pod-template
    replicaServiceTemplate: clickhouse-replica-svc-template
    serviceTemplate: clickhouse-cluster-svc-template
    volumeClaimTemplate: storage-vc-template
  extraServiceTemplates: []
  extraPodTemplates: []
  extraVolumeClaimTemplates: []
  adminUser:
    useSecret:
      enabled: false
    username: opik
    password: opik
  serviceAccount:
    annotations: {}
    create: false
    name: ""
  zookeeper:
      host: opik-zookeeper
  backup:
    enabled: false
    successfulJobsHistoryLimit: 1
    serviceAccount:
      annotations: {}
      create: false
      name: ""
    schedule: "0 0 * * *" # daily at midnight
    extraEnv: {}
    command:
      - /bin/bash
      - '-cx'
      - |-
        export backupname=backup$(date +'%Y%m%d%H%M')
        echo "BACKUP ALL EXCEPT DATABASE system TO S3('${CLICKHOUSE_BACKUP_BUCKET}/${backupname}/', '$ACCESS_KEY', '$SECRET_KEY');" > /tmp/backQuery.sql
        clickhouse-client -h clickhouse-opik-clickhouse --send_timeout 600000 --receive_timeout 600000 --port 9000 --queries-file=/tmp/backQuery.sql
  backupServer:
    enabled: false
    image: altinity/clickhouse-backup:2.6.23
    port: 7171
    monitoring:
      enabled: false
      additionalLabels: {}
      annotations: {}
      service:
        type: ClusterIP
        ports:
          - name: ch-backup-rest
            port: 80
            targetPort: 7171
      serviceMonitor:
        enabled: false
        additionalLabels: {}
        annotations: {}
        namespace: ""
        portName: ""
        interval: "60s"
        scrapeTimeout: "30s"
        relabelings: []
        metricRelabelings: []
        podTargetLabels: []
        honorLabels: false
    env:
      LOG_LEVEL: info
      ALLOW_EMPTY_BACKUPS: true
      API_LISTEN: 0.0.0.0:7171
      API_CREATE_INTEGRATION_TABLES: true
  monitoring:
    enabled: false
    username: opikmon # readonly user
    password: opikmon
    useSecret:
      enabled: false
    # resources:
    #   requests:
    #     cpu: 300m
    #     memory: 1Gi
    #   limits:
    #     memory: 1Gi
    additionalLabels: {}
    annotations: {}
    port: 9363
    service:
      type: ClusterIP
      ports:
        - name: prometheus-metrics
          port: 80
          targetPort: 9363
    serviceMonitor:
      enabled: false
      additionalLabels: {}
      annotations: {}
      namespace: ""
      portName: ""
      interval: "60s"
      scrapeTimeout: "30s"
      relabelings: []
      metricRelabelings: []
      podTargetLabels: []
      honorLabels: false

zookeeper:
  enabled: true
  fullnameOverride: opik-zookeeper
  # Server ID offset for Zookeeper cluster. Default is 1 for compatibility with
  # previous Bitnami-based installations. Set to 0 only for fresh installs.
  serverIdOffset: 1
  image:
    registry: docker.io
    repository: zookeeper
    tag: "3.9.4"
    imagePullPolicy: IfNotPresent
  replicaCount: 1
  podDisruptionBudget:
    enabled: true
  persistence:
    # Use bitnami zookeeper data directory path for compatibility
    mountPath: /bitnami/zookeeper
    dataDir: /bitnami/zookeeper/data
    enabled: true
    size: "50Gi"
  headless:
    publishNotReadyAddresses: true
  extraEnvVars:
    - name: ZK_HEAP_SIZE
      value: "512M"
    - name: ZOO_DATA_DIR
      value: "/bitnami/zookeeper/data"

    # resources:
    #   requests:
    #     cpu: 300m
    #     memory: 1500Mi
    #   limits:
    #     memory: 2Gi
