# Default values for opik public helm chart
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  argocd: false
  useHelmHooks: true
  security:
    allowInsecureImages: true

nameOverride: "opik"
fullnameOverride: ""

standalone: true
basicAuth: false

registry: &registry ghcr.io/comet-ml/opik

localFEAddress: "host.minikube.internal:5174"
localFE: false

serviceAccount:
  create: false
  name: ""
  annotations: {}

demoDataJob: 
  enabled: true

nodeSelector: {}
tolerations: []
affinity: {}

component:
  backend:
    enabled: true
    serviceAccount:
      create: true
      name: opik-backend
    metrics:
      enabled: false
    image:
      repository: opik-backend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: true
    run_migration: true
    waitForClickhouse:
      clickhouse:
        host: clickhouse-opik-clickhouse
        port: 8123
        protocol: http
      image:
        registry: docker.io
        repository: curlimages/curl
        tag: 8.12.1
    waitForMysql:
      enabled: true
      mysql:
        host: opik-mysql
        port: 3306
      image:
        registry: docker.io
        repository: busybox
        tag: 1.36
    livenessProbe:
      path: /health-check?name=all&type=alive
      port:  8080
    readinessProbe:
      path: /health-check?name=all&type=ready
      port:  8080
      initialDelaySeconds: 20
    env:
      STATE_DB_PROTOCOL: "jdbc:mysql://"
      STATE_DB_URL: "opik-mysql:3306/opik?rewriteBatchedStatements=true"
      STATE_DB_DATABASE_NAME: "opik"
      STATE_DB_USER: opik
      ANALYTICS_DB_MIGRATIONS_URL: "jdbc:clickhouse://clickhouse-opik-clickhouse:8123"
      ANALYTICS_DB_MIGRATIONS_USER: "opik"
      ANALYTICS_DB_PROTOCOL: "HTTP"
      ANALYTICS_DB_HOST: "clickhouse-opik-clickhouse"
      ANALYTICS_DB_PORT: "8123"
      ANALYTICS_DB_USERNAME: "opik"
      ANALYTICS_DB_DATABASE_NAME: "opik"
      JAVA_OPTS: "-Dliquibase.propertySubstitutionEnabled=true -XX:+UseG1GC -XX:MaxRAMPercentage=80.0 -XX:MinRAMPercentage=75"
      REDIS_URL: redis://:wFSuJX9nDBdCa25sKZG7bh@opik-redis-master:6379/
      ANALYTICS_DB_MIGRATIONS_PASS: opik
      ANALYTICS_DB_PASS: opik
      STATE_DB_PASS: opik
      OPIK_OTEL_SDK_ENABLED: false
      OTEL_VERSION: 2.12.0
      OTEL_PROPAGATORS: "tracecontext,baggage,b3"
      OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: true
      OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION: BASE2_EXPONENTIAL_BUCKET_HISTOGRAM
      OTEL_EXPERIMENTAL_RESOURCE_DISABLED_KEYS: process.command_args
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: delta
      PYTHON_EVALUATOR_URL: http://opik-python-backend:8000
    envFrom:
      - configMapRef:
          name: opik-backend
    service:
      type: ClusterIP
      ports:
      - name: http
        port:  8080
        protocol: TCP
        targetPort:  8080
      - name: swagger
        port:  3003
        protocol: TCP
        targetPort:  3003
    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
      hosts: []
      tls:
        enabled: false
        hosts: []
        secretName: ""

  python-backend:
    enabled: true
    serviceAccount:
      create: true
      name: opik-python-backend
    metrics:
      enabled: false
    image:
      repository: opik-python-backend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: true
    env:
      PYTHON_CODE_EXECUTOR_IMAGE_REGISTRY: *registry
      PYTHON_CODE_EXECUTOR_IMAGE_NAME: "opik-sandbox-executor-python"
      PYTHON_CODE_EXECUTOR_IMAGE_TAG: "latest"
      PYTHON_CODE_EXECUTOR_STRATEGY: "process"
      PYTHON_CODE_EXECUTOR_PARALLEL_NUM: "5"
      PYTHON_CODE_EXECUTOR_EXEC_TIMEOUT_IN_SECS: "3"
      PYTHON_CODE_EXECUTOR_ALLOW_NETWORK: "false"
      OPIK_REVERSE_PROXY_URL: "http://opik-frontend:5173/api"
      OTEL_SERVICE_NAME: "opik-python-backend"
      OTEL_METRIC_EXPORT_INTERVAL: "60000"
      OTEL_PROPAGATORS: "tracecontext,baggage"
      OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: true
      OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: cumulative
    envFrom:
      - configMapRef:
          name: opik-python-backend

    securityContext:
      privileged: true
    networkPolicy:
      enabled: true
      engineEgress:
        ipBlock: 0.0.0.0/0
        except:
          - 10.0.0.0/8
          - 100.64.0.0/10
          - 172.16.0.0/12
          - 192.0.0.0/24
          - 198.18.0.0/15
          - 192.168.0.0/16

    service:
      type: ClusterIP
      ports:
      - name: http
        port:  8000
        protocol: TCP
        targetPort:  8000

    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
      hosts: []
      tls:
        enabled: false
        hosts: []
        secretName: ""

  frontend:
    enabled: true
    serviceAccount:
      create: true
      name: opik-frontend
    metrics:
      enabled: false
    image:
      repository: opik-frontend
      pullPolicy: IfNotPresent
      tag: latest
    replicaCount: 1
    autoscaling:
      enabled: false
    backendConfigMap:
      enabled: false
    volumeMounts:
    - mountPath: /etc/nginx/conf.d/
      name: opik-frontend-nginx
    volumes:
    - name: opik-frontend-nginx
      configMap:
        name: opik-frontend-nginx
        items:
        - key: default.conf
          path: default.conf
    service:
      type: ClusterIP
      ports:
      - name: http
        port:  5173
        protocol: TCP
        targetPort:  5173
    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
      hosts: []
        # - host: opik.example.com
        #   paths:
        #     - path: /
        #       port: 5173
        #       pathType: Prefix
      tls:
        enabled: false
        hosts: []
        secretName: ""
    logFormats:
      logger-json: |-
        escape=json '{'
                '  "body_bytes_sent": $body_bytes_sent'
                ', "comet_workspace": "$http_comet_workspace"'
                ', "host": "$host"'
                ', "http_referer": "$http_referer"'
                ', "http_user_agent": "$http_user_agent"'
                ', "limit_req_status": "$limit_req_status"'
                ', "method": "$request_method"'
                ', "remote_addr": "$remote_addr"'
                ', "remote_user": "$remote_user"'
                ', "request_length": $request_length'
                ', "request_time": $request_time'
                ', "request": "$request"'
                ', "response": $status'
                ', "resp_body_size": $body_bytes_sent'
                ', "source": "nginx"'
                ', "status": $status'
                ', "time_local": "$time_local"'
                ', "time": $msec'
                ', "uri": "$request_uri"'
                ', "user_agent": "$http_user_agent"'
                ', "x_forwarded_for": "$http_x_forwarded_for"'
                ', "x_sdk_version": "$http_x_opik_debug_sdk_version"'
                ', "upstream_connect_time": "$upstream_connect_time", "upstream_header_time": "$upstream_header_time", "upstream_response_time": "$upstream_response_time"'
                ', "upstream_addr": "$upstream_addr", "upstream_status": "$upstream_status", "host": "$host"'
            '}'
    # throttled locations
    throttling: {}
    # map variables for use with throttling and other places
    maps: []
    # use internal aws resolvr on vpc
    awsResolver: false
    logFormat: logger-json
    logFormats:
      "logger-json": >-
        escape=json '{
        "body_bytes_sent": $body_bytes_sent,
        "http_referer": "$http_referer",
        "http_user_agent": "$http_user_agent",
        "remote_addr": "$remote_addr",
        "remote_user": "$remote_user",
        "request": "$request",
        "status": $status,
        "time_local": "$time_local",
        "x_forwarded_for": "$http_x_forwarded_for"
        }'
    upstreamConfig: {}
      # proxy_read_timeout: 60
      # proxy_connect_timeout: 60
      # proxy_send_timeout: 60
      # proxy_buffering: 'on'
      # proxy_request_buffering: 'on'

mysql:
  enabled: true
  image:
    repository: bitnamilegacy/mysql
  fullnameOverride: opik-mysql
  auth:
    rootPassword: "opik"
  initdbScripts:
    createdb.sql: |-
      CREATE DATABASE IF NOT EXISTS opik DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
      CREATE USER IF NOT EXISTS 'opik'@'%' IDENTIFIED BY 'opik';
      GRANT ALL ON `opik`.* TO 'opik'@'%';
      FLUSH PRIVILEGES;

minio:
  enabled: true
  image:
    repository: bitnamilegacy/minio
  volumePermissions:
    image:
      repository: bitnamilegacy/os-shell
  mode: standalone
  disableWebUI: true
  persistence:
    enabled: true
    size: 50Gi
  auth:
    rootUser: "THAAIOSFODNN7EXAMPLE"
    rootPassword: "LESlrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  provisioning:
    enabled: true
    extraCommands:
      - mc alias set s3 http://opik-minio:9000 THAAIOSFODNN7EXAMPLE LESlrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY --api S3v4
      - mc mb --ignore-existing s3/public
      - mc anonymous set download s3/public/
  fullnameOverride: opik-minio


redis:
  enabled: true
  ssl: false
  fullnameOverride: opik-redis
  architecture: standalone
  metrics:
    enabled: false
  auth:
    password: "wFSuJX9nDBdCa25sKZG7bh"
  image:
    repository: redis/redis-stack-server
    tag: 7.2.0-v10
  master:
    resources:
      requests:
        cpu: 15m
        memory: 105M
      limits:
        # cpu: 15m
        memory: 1Gi
    args:
      - -c
      - /opt/bitnami/scripts/merged-start-scripts/start-master.sh
    configuration: maxmemory 105M
    extraVolumes:
      - name: merged-start-scripts
        configMap:
          name: bitnami-redis-stack-server-merged
          defaultMode: 493
    extraVolumeMounts:
      - name: merged-start-scripts
        mountPath: /opt/bitnami/scripts/merged-start-scripts
  extraDeploy:
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: bitnami-redis-stack-server-merged
      data:
        start-master.sh: "#!/usr/bin/dumb-init /bin/bash\n### docker entrypoint script,\
          \ for starting redis stack\nBASEDIR=/opt/redis-stack\ncd ${BASEDIR}\nCMD=${BASEDIR}/bin/redis-server\n\
          if [ -z \"${REDISEARCH_ARGS}\" ]; then\nREDISEARCH_ARGS=\"MAXSEARCHRESULTS\
          \ 10000 MAXAGGREGATERESULTS 10000\"\nfi\n[[ -f $REDIS_PASSWORD_FILE ]] &&\
          \ export REDIS_PASSWORD=\"$(< \"${REDIS_PASSWORD_FILE}\")\"\nif [[ -f /opt/bitnami/redis/mounted-etc/master.conf\
          \ ]];then\n    cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf\n\
          fi\nif [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then\n    cp /opt/bitnami/redis/mounted-etc/redis.conf\
          \ /opt/bitnami/redis/etc/redis.conf\nfi\n${CMD} \\\n--port \"${REDIS_PORT}\"\
          \ \\\n--requirepass \"${REDIS_PASSWORD}\" \\\n--masterauth \"${REDIS_PASSWORD}\"\
          \ \\\n--include \"/opt/bitnami/redis/etc/redis.conf\" \\\n--include \"/opt/bitnami/redis/etc/master.conf\"\
          \ \\\n--loadmodule ${BASEDIR}/lib/redisearch.so ${REDISEARCH_ARGS} \\\n\
          --loadmodule ${BASEDIR}/lib/redistimeseries.so ${REDISTIMESERIES_ARGS} \\\
          \n--loadmodule ${BASEDIR}/lib/rejson.so ${REDISJSON_ARGS} \\\n--loadmodule\
          \ ${BASEDIR}/lib/redisbloom.so ${REDISBLOOM_ARGS}\n"

altinity-clickhouse-operator:
  metrics:
    enabled: false
  serviceMonitor:
    enabled: false
    interval: '' # workaround for https://github.com/Altinity/clickhouse-operator/issues/1759

clickhouse:
  enabled: true
  shardsCount: 1
  replicasCount: 1
  image: altinity/clickhouse-server:25.3.6.10034.altinitystable
  storage: 50Gi
  logsLevel: information
  # Custom health checks with longer timeouts for major version upgrades
  livenessProbe:
    httpGet:
      path: /ping
      port: 8123
    initialDelaySeconds: 60
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 10
  readinessProbe:
    httpGet:
      path: /ping
      port: 8123
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 30
  configuration:
    files:
      conf.d/memory.xml: |
        <yandex>
          <max_server_memory_usage_to_ram_ratio>0.85</max_server_memory_usage_to_ram_ratio>
        </yandex>
      conf.d/profiles.xml: |
        <clickhouse>
          <profiles>
            <default>
                <max_bytes_ratio_before_external_sort>0.2</max_bytes_ratio_before_external_sort>
                <max_bytes_ratio_before_external_group_by>0.2</max_bytes_ratio_before_external_group_by>
            </default>
          </profiles>
        </clickhouse>
      # Remove or add TTL to system tables that are not needed
      conf.d/system_tables.xml: |
        <clickhouse>
          <opentelemetry_span_log remove="1"/>
          <asynchronous_metric_log remove="1"/>
          <processors_profile_log remove="1"/>
          <text_log remove="1"/>
          <trace_log remove="1"/>
          <blob_storage_log remove="1"/>
          <error_log>
              <engine>
                  ENGINE MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>error_log</table>
          </error_log>
          <latency_log>
              <engine>
                  ENGINE = MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>latency_log</table>
          </latency_log>
          <metric_log>
              <engine>
                  ENGINE = MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>metric_log</table>
          </metric_log>
          <query_metric_log>
              <engine>
                  ENGINE = MergeTree
                  PARTITION BY toYYYYMM(event_date)
                  ORDER BY (event_date, event_time)
                  TTL event_date + toIntervalDay(30)
                  SETTINGS index_granularity = 8192
              </engine>
              <database>system</database>
              <table>query_metric_log</table>
          </query_metric_log>
        </clickhouse>
  service:
    serviceTemplate: clickhouse-cluster-svc-template
  templates:
    podTemplate: clickhouse-cluster-pod-template
    replicaServiceTemplate: clickhouse-replica-svc-template
    serviceTemplate: clickhouse-cluster-svc-template
    volumeClaimTemplate: storage-vc-template
  extraServiceTemplates: []
  extraPodTemplates: []
  extraVolumeClaimTemplates: []
  adminUser:
    useSecret:
      enabled: false
    username: opik
    password: opik
  serviceAccount:
    annotations: {}
    create: false
    name: ""
  zookeeper:
      host: opik-zookeeper
  backup: 
    enabled: false
    successfulJobsHistoryLimit: 1
    serviceAccount:
      annotations: {}
      create: false
      name: ""
    schedule: "0 0 * * *" # daily at midnight
    extraEnv: {}
    command:
      - /bin/bash
      - '-cx'
      - |-
        export backupname=backup$(date +'%Y%m%d%H%M')
        echo "BACKUP ALL EXCEPT DATABASE system TO S3('${CLICKHOUSE_BACKUP_BUCKET}/${backupname}/', '$ACCESS_KEY', '$SECRET_KEY');" > /tmp/backQuery.sql
        clickhouse-client -h clickhouse-opik-clickhouse --send_timeout 600000 --receive_timeout 600000 --port 9000 --queries-file=/tmp/backQuery.sql
  backupServer:
    enabled: false
    image: altinity/clickhouse-backup:2.6.23
    port: 7171
    monitoring:
      enabled: false
      additionalLabels: {}
      annotations: {}
      service:
        type: ClusterIP
        ports:
          - name: ch-backup-rest
            port: 80
            targetPort: 7171
      serviceMonitor:
        enabled: false
        additionalLabels: {}
        annotations: {}
        namespace: ""
        portName: ""
        interval: "60s"
        scrapeTimeout: "30s"
        relabelings: []
        metricRelabelings: []
        podTargetLabels: []
        honorLabels: false
    env:
      LOG_LEVEL: info
      ALLOW_EMPTY_BACKUPS: true
      API_LISTEN: 0.0.0.0:7171
      API_CREATE_INTEGRATION_TABLES: true
  monitoring:
    enabled: false
    username: opikmon # readonly user
    password: opikmon
    useSecret:
      enabled: false
    # resources:
    #   requests:
    #     cpu: 300m
    #     memory: 1Gi
    #   limits:
    #     memory: 1Gi
    additionalLabels: {}
    annotations: {}
    port: 9363
    service:
      type: ClusterIP
      ports:
        - name: prometheus-metrics
          port: 80
          targetPort: 9363
    serviceMonitor:
      enabled: false
      additionalLabels: {}
      annotations: {}
      namespace: ""
      portName: ""
      interval: "60s"
      scrapeTimeout: "30s"
      relabelings: []
      metricRelabelings: []
      podTargetLabels: []
      honorLabels: false

zookeeper:
    enabled: true
    fullnameOverride: opik-zookeeper
    image:
      repository: bitnamilegacy/zookeeper
      tag: 3.9.3-debian-12-r16
    replicaCount: 1
    # podAntiAffinityPreset: hard
    podDisruptionBudget:
      enabled: true
    persistence:
      enabled: true
      ## The amount of PV storage allocated to each Zookeeper pod in the statefulset
      size: "50Gi"
    ## Environmental variables to set in Zookeeper
    ##
    env:
      ## The JVM heap size to allocate to Zookeeper
      ZK_HEAP_SIZE: "512M"
    ## Configure Zookeeper headless
    headless:
      publishNotReadyAddresses: true

    # resources:
    #   requests:
    #     cpu: 300m
    #     memory: 1500Mi
    #   limits:
    #     memory: 2Gi
